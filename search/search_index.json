{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Goals, Statement of Approach, Learning Objectives","text":""},{"location":"#1-goal","title":"1. Goal","text":"<p>The general aim of this advanced training is to make the technical sellers PoT/PoC ready for the portfolio. At the end of this training, the technical sellers should be able to take a customer problem, use their knowledge of the portfolio, then plan, define and execute the PoC/PoT using the components in the portfolio.</p>"},{"location":"#2-statement-of-approach","title":"2. Statement of Approach","text":"<p>Attendees are expected to grow deep skills across the CP4BA portfolio including RPA and Process Mining.\u00a0 Each attendee will be placed in teams with a mix of capability specific skills and are expected to select labs that will build their capability skills where they need the most depth.</p>"},{"location":"#3-learning-objectives","title":"3. Learning Objectives","text":"<p>After this event, tech sellers should be able to do a PoC targeting a customer use case.   </p> <ul> <li>Individually, install CP4BA starter pattern (advanced attendees to use production pattern)</li> <li>As a team:<ul> <li>Deploy seven prebuilt artifacts (ADS, Workflow, etc...) on a cluster</li> <li>Integrate and test deployed artifacts</li> <li>Rebuild selected artifacts (ADS, Workflow, etc...), likely one capability per team member</li> <li>Learn how to effectively demonstrate the client onboarding demo that spans across the Cloud Pak for Business Automation (CP4BA)</li> </ul> </li> </ul> <p>Active participation will be critical to the success of this course. Specifically, there will be tremendous value learning from your peers not only on the capabilities but by discussing experiences executing a client PoC.</p>"},{"location":"index-tbd/","title":"Index tbd","text":""},{"location":"agenda/agenda-daily/","title":"Agenda daily","text":""},{"location":"agenda/agenda-daily/#day-1","title":"Day 1","text":"Expand to view Day Time Session / Activity Day 1 1:00 \u2013 1:30 Opening Remarks \u2013 Sarah McAndrew, Carlos Camilion 1:30 \u2013 2:15 Automation Roadmap and Strategy - Bill Lobig 2:15 \u2013 2:30 Support and Feedback process 2:30 \u2013 2:45 Break 2:45 \u2013 3:45 Segment Strategy - (Segment VP\u2019s/Execs: Matt Warta, Rakesh Ranjan, Eileen Lowry, John Greene) 4:00 \u20135:15 Team Building Evening Team Dinner <p>Top of page | Top of section</p>"},{"location":"agenda/agenda-daily/#day-2","title":"Day 2","text":"Expand to view Day Time Session / Activity Day 2 8:00 - 8:30 Introduction 8:30 -10:30 Deployment - Daffy &amp; Cluster Build 10:30 - 11:00 Break 11:00 - 12:30 Client Onboarding Demonstration - Art of the Possible 12:30 - 1:30 Lunch 1:30 - 5:00 Deploy and Integrate <p>Top of page | Top of section</p>"},{"location":"agenda/agenda-daily/#day-3","title":"Day 3","text":"Expand to view Day Time Session / Activity Day 3 9:00 - 9:30 Client Onboarding - Review Demo Checklist 9:30 - 12:00 Client Onboarding - Rebuild - Developing &amp; Delivering 12:00 - 1:00 Lunch 1:00 - 1:30 Review Demo Outline 1:30 - 5:00 Continue Rebuild &amp; Begin Customizations <p>Top of page | Top of section</p>"},{"location":"agenda/agenda-daily/#day-4","title":"Day 4","text":"Expand to view Day Time Session / Activity Day 4 9:00 - 10:00 Prep for demo 10:00 - 12:00 Deliver Customized Client Onboarding Demo 12:00 - 1:00 Lunch 1:00 - 4:00 Advanced Topics &amp; Best Practices Day 4 4:00 PM Attendees required to stay until 4:00 PM <p>Top of page | Top of section</p>"},{"location":"agenda/agenda/","title":"Event Schedule","text":""},{"location":"agenda/agenda/#2022","title":"2022","text":"Expand to view"},{"location":"agenda/agenda/#americas","title":"Americas","text":"Expand to view <p>This event will be 4 days long with the following agenda:</p> Day Time Session / Activity Day 1 1:00 \u2013 1:30 Opening Remarks \u2013 Sarah McAndrew, Carlos Camilion 1:30 \u2013 2:15 Automation Roadmap and Strategy - Bill Lobig 2:15 \u2013 2:30 Support and Feedback process 2:30 \u2013 2:45 Break 2:45 \u2013 3:45 BA Strategy - Matt Warta Other Segments: Rakesh Ranjan, Eileen Lowry, John Greene 4:00 \u2013 5:15 Team Building Evening Team Dinner Day 2 8:00 - 8:30 Introduction 8:30 - 10:30 Deployment - Daffy &amp; Cluster Build [Individual] 10:30 - 11:00 Break 11:00 - 12:30 Client Onboarding Demonstration - Art of the Possible - Swapnil Agrawal 12:30 - 1:30 Lunch 1:30 - 5:00 Client Onboarding - Deploy and Integrate [Team] Day 3 8:00 - 8:30 Briefing on activities for the day  Review Stand and Deliver Guidelines 8:30 - 12:30 Client Onboarding - Rebuild portion of solution [Team/Individual] 12:00 - 12:30 Prior to lunch, customization use case scenarios will be posted to the slack channel 12:30 - 1:30 Lunch 1:30 - 5:00 Continue Rebuild &amp; Complete Customizations [Team/Individual] Day 4 8:00 - 8:30 Briefing on activities for the day 8:30 - 9:45 Deliver Customized Client Onboarding Demo - Round 1  Groups of 3 teams, demo 15 mins, feedback 5 mins, break between teams 5 mins [Team] 9:45 - 10:00 Break 10:00 - 12:30 Deliver Customized Client Onboarding Demo - Round 2  Top 5 teams, demo 15 mins to room, feedback 5 min, break between teams 6 min [Team] 12:30 - 1:30 Lunch 1:30 - 2:15 Automation Document Processing (ADP) Update - Dan Ouimet (remote) 2:15 - 2:30 BREAK 2:30 - 3:00 Process Discovery\u00a0with Automation Compass - Jared Michalec (Salient Process) 3:00 - 3:30 Robotic Process Automation (RPA) &amp; Workflow Process Services (WfPS) - Zach Silverstein 3:30 - 4:00 Competitive (Pega) - Greg Violette Day 4 4:00 PM Attendees required to stay until 4:00 PM <p>For more info on SKO Tech Academy, please refer to the links below:     - SKO Tech Academy (Seismic)     - SKO Tech Academy - Automation (Seismic)     - SKO Tech Academy - Automation - Detailed Agenda </p> <p>Top of page | Top of section</p> <p> </p>"},{"location":"agenda/agenda/#emea","title":"EMEA","text":"Expand to view <p>Location: IBM Briefing Center - Auditorium Room, H002-Heathcode</p> <p>This event will be 4 days long with the following agenda:</p> Day Time Session / Activity Day 1 1:00 \u2013 1:30 Opening Remarks \u2013 Sarah McAndrew, Carmen Raileanu 1:30 \u2013 2:15 Automation Roadmap and Strategy - Harley Davis 2:15 \u2013 3:00 Hybrid Cloud and Automation - Dennis Lauwers 3:00 \u2013 3:15 Break 3:15 \u2013 4:15 BA Strategy - Harley Davis (H002-Heathcode) 4:15 \u2013 5:30 Team Building 5:30 \u2013 6:00 Break 6:00 - Welcome Dinner Day 2 9:30 - 10:00 Introduction 10:00 - 11:30 Deployment - Daffy &amp; Cluster Build [Individual] 11:30 - 12:30 Client Onboarding Demonstration - Art of the Possible - Olaf Hahnl 12:30 - 1:30 Lunch 1:30 - 5:00 Client Onboarding - Deploy and Integrate [Team] Day 3 9:00 - 9:30 Briefing on activities for the day  Review Stand and Deliver Guidelines 9:30 - 12:00 Client Onboarding - Rebuild portion of solution [Team/Individual] 11:30 - 12:00 Prior to lunch, customization use case scenarios will be posted to the slack channel 12:00 - 1:00 Lunch 1:00 - 5:00 Continue Rebuild &amp; Complete Customizations [Team/Individual] Day 4 9:00 - 9:30 Briefing on activities for the day  Prepare for Stand and Deliver 9:30 - 10:30 Deliver Customized Client Onboarding Demo - Round 1 10:30 - 10:45 Break 10:45 - 11:45 Deliver Customized Client Onboarding Demo - Round 2 11:45 - 12:30 Managing Users and Access Rights in CP4BA - Fadi Sandakly 12:30 - 1:30 Lunch 1:30 - 2:15 IBM Process Mining: The Evolution, Success and Lessons Learned since the myInvenio Acquisition (Lee Bonham) 2:15 - 3:00 Automation Document Processing (ADP) Update - Dan Ouimet (remote) 3:00 - 3:15 Break 3:15 - 4:00 Best Practices and Experiences from PoCs - Dan Crow 4:00 - 4:45 Competing with Pega - Greg Violette 4:45 - 5:00 Closing Day 4 5:00 PM Attendees required to stay until all sessions are complete <p>For more info on SKO Tech Academy, please refer to the links below:     - SKO Tech Academy (Seismic)     - SKO Tech Academy - Automation (Seismic)     - SKO Tech Academy - Automation - Detailed Agenda     - Hursley Box Share (IBM Only) </p> <p>Top of page | Top of section</p> <p> </p>"},{"location":"agenda/agenda/#apac-tokyo","title":"APAC - Tokyo","text":"Expand to view <p>This event will be 4 days long with the following agenda:</p> Day Time Session / Activity Day 1 1:00 \u2013 1:30 Opening Remarks \u2013 Sarah McAndrew, Shintaroh Yokotani 1:30 \u2013 2:15 Automation Roadmap and Strategy 2:15 \u2013 3:00 Hybrid Cloud and Automation 3:00 \u2013 3:15 Break 3:15 \u2013 4:15 BA Strategy 4:15 \u2013 5:30 Team Building 5:30 \u2013 6:00 Break 6:00 - Welcome Dinner Day 2 9:00 - 9:30 Introduction 9:30 - 11:00 Deployment - Daffy &amp; Cluster Build [Individual] 11:00 - 12:00 Client Onboarding Demonstration - Art of the Possible - Swapnil Agrawal 12:00 - 1:00 Lunch 1:00 - 5:00 Client Onboarding - Deploy and Integrate [Team] Day 3 9:00 - 9:30 Briefing on activities for the day  Review Stand and Deliver Guidelines 9:30 - 12:00 Client Onboarding - Rebuild portion of solution [Team/Individual] 11:30 - 12:00 Prior to lunch, customization use case scenarios will be posted to the slack channel 12:00 - 1:00 Lunch 1:00 - 5:00 Continue Rebuild &amp; Complete Customizations [Team/Individual] Day 4 9:00 - 9:30 Briefing on activities for the day  Prepare for Stand and Deliver 9:30 - 11:00 Deliver Customized Client Onboarding Demo - Team Demos (4) [15 min demo, 5 min review] 11:00 - 11:10 Break 11:10 - 11:55 Competing with Pega - Greg Violette 11:55 - 12:55 Lunch 12:55 - 1:00 SKO Tech Academy - Business Automation - Pictures 1:00 - 1:15 SKO Tech Academy - Automation - Pictures 1:15 - 1:35 Demo 1 - Inheritance of Expert - Koichi Nakamura [15 min demo, 5 min review] 1:35 - 1:55 Demo 2 - Branch Office Innovation - Hirofumi Nishimura [15 min demo, 5 min review] 1:55 - 2:15 Demo 3 - Legacy Modernization - Nobuo Hamasaki [15 min demo, 5 min review] 2:15 - 2:35 Demo 4 - RPA - Shintaro Hase [15 min demo, 5 min review] 2:35 - 2:55 Demo 5 - Insurance Claims Analysis - Hideo Saito [15 min demo, 5 min review] 2:55 - 3:00 Closing Day 4 3:00 PM Attendees required to stay until all sessions are complete <p>For more info on SKO Tech Academy, please refer to the links below:     - SKO Tech Academy (Seismic)     - SKO Tech Academy - Automation (Seismic)     - SKO Tech Academy - Automation - Detailed Agenda </p> <p>Top of page | Top of section</p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"agenda/process-americas/","title":"Process americas","text":"<p>The process diagram below represents the activities for this event.</p> <p></p> <p>Course Breakdown</p> Day 1 - Monday 6/13Day 2 - Tuesday 6/14Day 3 - Wednesday 6/15Day 4 - Thursday 6/16 <p>Exec Sessions</p> Time Session / Activity 1:00 \u2013 1:30 Opening Remarks \u2013 Sarah McAndrew, Carlos Camilion 1:30 \u2013 2:15 Automation Roadmap and Strategy - Bill Lobig 2:15 \u2013 2:30 Support and Feedback process 2:30 \u2013 2:45 Break 2:45 \u2013 3:45 Segment Strategy - (Segment VP\u2019s/Execs: Matt Warta, Rakesh Ranjan, Eileen Lowry, John Greene) 4:00 \u20135:15 Team Building Evening Team Dinner <ol> <li>Each attendee will set up Daffy and provision a cluster in ROKS.</li> <li>Teams member assignments will be published and also given a pre-provisioned team cluster.</li> <li>Team members will work together on the team cluster and perform the Deploy and Integrate section to deploy artifacts for the Client Onboarding solution.</li> </ol> Time Session / Activity 8:00 - 8:30 Introduction 8:30 - 10:30 Deployment - Daffy &amp; Cluster Build [Individual] 10:30 - 11:00 Break 11:00 - 12:30 Client Onboarding Demonstration - Art of the Possible - Swapnil Agrawal 12:30 - 1:30 Lunch 1:30 - 5:00 Client Onboarding - Deploy and Integrate [Team] <ol> <li>Review Stand and Deliver Guidelines</li> <li>Team members will work together on the team cluster and rebuild portions of the Client Onboarding solution, selecting a capability that is not their deepest expertise and/or is the most needed skill in their market.  Every member of the team should select a different rebuild lab.</li> <li>Customization use case scenarios will be provided prior to lunch.</li> </ol> Time Session / Activity 8:00 - 8:30 Briefing on activities for the day  Review Stand and Deliver Guidelines 8:30 - 12:30 Client Onboarding - Rebuild portion of solution [Team/Individual] 12:00 - 12:30 Prior to lunch, customization use case scenarios will be posted to the slack channel 12:30 - 1:30 Lunch 1:30 - 3:30 Continue Rebuild &amp; Complete Customizations [Team/Individual] 3:30 - 3:45 Break 3:45 - 5:00 Prepare Day 4 demo script [Team] <ol> <li>Team members will prepare and deliver the customized Client Onboarding demo (Round 1 and 2)</li> <li>Advanced Topics: ADP, Automation Compass, RPA &amp; WfPS, Pega</li> </ol> Time Session / Activity 8:00 - 8:30 Briefing on activities for the day 8:30 - 9:45 Deliver Customized Client Onboarding Demo - Round 1  Groups of 3 teams, demo 15 mins, feedback 5 mins, break between teams 5 mins [Team] 9:45 - 10:00 BREAK 10:00 - 12:30 Deliver Customized Client Onboarding Demo - Round 2  Top 5 teams, demo 15 mins to room, feedback 5 min, break between teams 6 min [Team] 12:30 - 1:30 Lunch 1:30 - 2:15 Automation Document Processing (ADP) Update - Dan Ouimet (remote) 2:15 - 2:30 BREAK 2:30 - 3:00 Process Discovery\u00a0with Automation Compass - Jared Michalec (Salient Process) 3:00 - 3:30 Robotic Process Automation (RPA) &amp; Workflow Process Services (WfPS) - Zach Silverstein 3:30 - 4:00 Competitive (Pega) - Greg Violette 4:00 PM Attendees required to stay until 4:00 PM"},{"location":"agenda/process/","title":"Course Breakdown","text":"<p>The process diagram below represents the activities for this event.</p> <p></p> <p>Course Breakdown</p> Day 1 - Monday 6/20Day 2 - Tuesday 6/21Day 3 - Wednesday 6/22Day 4 - Thursday 6/23 <p>Exec Sessions</p> Time Session / Activity 1:00 \u2013 1:30 Opening Remarks \u2013 Sarah McAndrew, Shintaroh Yokotani 1:30 \u2013 2:15 Automation Roadmap and Strategy 2:15 \u2013 3:00 Hybrid Cloud and Automation 3:00 \u2013 3:15 Break 3:15 \u2013 4:15 BA Strategy 4:15 \u2013 5:30 Team Building 5:30 \u2013 6:00 Break 6:00 - Welcome Dinner <ol> <li>Each attendee will set up Daffy and provision a cluster in ROKS.</li> <li>Teams member assignments will be published and also given a pre-provisioned team cluster.</li> <li>Team members will work together on the team cluster and perform the Deploy and Integrate section to deploy artifacts for the Client Onboarding solution.</li> </ol> Time Session / Activity 9:00 - 9:30 Introduction 9:30 - 11:00 Deployment - Daffy &amp; Cluster Build [Individual] 11:00 - 12:00 Client Onboarding Demonstration - Art of the Possible - Swapnil Agrawal 12:00 - 1:00 Lunch 1:00 - 5:00 Client Onboarding - Deploy and Integrate [Team] <ol> <li>Review Stand and Deliver Guidelines</li> <li>Team members will work together on the team cluster and rebuild portions of the Client Onboarding solution, selecting a capability that is not their deepest expertise and/or is the most needed skill in their market.  Every member of the team should select a different rebuild lab.</li> <li>Customization use case scenarios will be provided prior to lunch.</li> </ol> Time Session / Activity 9:00 - 9:30 Briefing on activities for the day  Review Stand and Deliver Guidelines 9:30 - 12:00 Client Onboarding - Rebuild portion of solution [Team/Individual] 11:30 - 12:00 Prior to lunch, customization use case scenarios will be posted to the slack channel 12:00 - 1:00 Lunch 1:00 - 5:00 Continue Rebuild &amp; Complete Customizations [Team/Individual] <ol> <li>Team members will prepare and deliver the customized Client Onboarding demo (Round 1 and 2)</li> <li>Advanced Topics: Competing with Pega,</li> </ol> Time Session / Activity 9:00 - 9:30 Briefing on activities for the day  Prepare for Stand and Deliver 9:30 - 11:00 Deliver Customized Client Onboarding Demo - Team Demos (4) [15 min demo, 5 min review] 11:00 - 11:10 Break 11:10 - 11:55 Competing with Pega - Greg Violette 11:55 - 12:55 Lunch 12:55 - 1:00 SKO Tech Academy - Business Automation - Pictures 1:00 - 1:15 SKO Tech Academy - Automation - Pictures 1:15 - 1:35 Demo 1 - Inheritance of Expert - Koichi Nakamura [15 min demo, 5 min review] 1:35 - 1:55 Demo 2 - Branch Office Innovation - Hirofumi Nishimura [15 min demo, 5 min review] 1:55 - 2:15 Demo 3 - Legacy Modernization - Nobuo Hamasaki [15 min demo, 5 min review] 2:15 - 2:35 Demo 4 - RPA - Shintaro Hase [15 min demo, 5 min review] 2:35 - 2:55 Demo 5 - Insurance Claims Analysis - Hideo Saito [15 min demo, 5 min review] 2:55 - 3:00 Closing 3:00 PM Attendees required to stay until all sessions are complete"},{"location":"agenda/survey/","title":"Survey","text":"<p>At the end of the event, please click the link below to complete the survey: Survey - Automation SKO Technical Academy 2022 - Business Automation</p>"},{"location":"client-onboarding/co-customization/","title":"Customization (Individual)","text":"<p>Customization use case scenarios will be posted to the event slack channel prior to lunch on Day 3.  You may also ask your instructor if you cannot locate them. </p> <p> As reminder, please ensure you review the stand and deliver guidelines:     - Stand &amp; Deliver Guidelines </p>"},{"location":"client-onboarding/co-deploy-integrate/","title":"Deploy and Integrate (Team)","text":"<p>Note</p> <p>These instructions assume that you have Cloud Pak for Business Automation 21.0.3 installed along with Open Prediction Service (OPS).  The CP4BA cluster that has been provisioned for you for Tech Academy meets these requirements.</p> <p>Warning</p> <p>You must use the artifacts downloaded from this site.  Do NOT use the original SWAT artifacts you may have downloaded previously as these are from a Production deployment of CP4BA and there are subtle differences.</p>"},{"location":"client-onboarding/co-deploy-integrate/#instructions","title":"Instructions","text":"<p>There are 7 sections that you will need to import and build the Client Onboarding1 solution into your team cluster.  </p> <p>Below are the recommended tracks so that different members of the team can be the primary driver of the lab. Please designate the primary driver using a team member that may not be the most experienced in each capability. As a recommendation, use the Slack huddle feature so members of the team can clearly see the steps that are being performed.  </p> Track Section(s) Description A 12 Import the ADS ML Model  Import the ADS Project B 45 Import the Workflow Solution  Import objects into FileNet Content Manager C 67 Import the Business Automation Application app  Import the Business Automation Insights data D 39 Setup the RPA Server (optional)  Validate the RPA Integration (optional) <p>Section 8 - Validate Client Onboarding solution should be completed as a team when all the section have been completed.</p> <p>You can access the github repo for this event using the link in the top-right corner. This will provide access to PDF's, pre-built solutions and lab assets. </p>"},{"location":"client-onboarding/co-deploy-integrate/#installation-links","title":"Installation Links","text":"Expand to view <p>Each team will be allocated a pre-configured CP4BA cluster, hosted using an enterprise account on IBM Cloud to which you have been invited.  Log into IBM Cloud and switch to the enterprise account 2326304 - itztsglenablement01 as shown below: </p> <p>Select the burger icon in the top-left corner and select OpenShift, then Clusters </p> <p>To find your cluster enter your allocated lab name in the search box, (here I'm using lab01).  Click on the three dots on the right hand side of the screen adjacent to your cluster and select OpenShift Web Console. </p> <p>Login to the OpenShift Web Console and locate the config map called icp4adeploy-cp4ba-access-info. </p> <p>Select the icp4adeploy-cp4ba-access-info config map then select YAML and then Download </p> <p>You will find all the links and credentials for your deployment. Specify Enterprise LDAP when you log into an application.  </p> <p>Note</p> <p>A deeper understanding of each of the sections below will be covered in the labs in the Rebuild section.</p>"},{"location":"client-onboarding/co-deploy-integrate/#1-import-the-ads-ml-model","title":"1. Import the ADS ML Model","text":"Expand to view <p>1. Open the ADS ML Service (Open Prediction Service) in your browser.  To find the link for this component, open the OpenShift web console and select Networking then Routes. These are all the public links exposed by the OpenShift cluster. To easily find the ADS ML Service public link, select ads-ml-service from the project dropdown near the top of the screen as shown in the diagram below.  From the Location column, click on the link to open the Swagger UI for the ADS ML Service. </p> <p>2. Under <code>manage</code>, expand the <code>POST /models Add Model</code> section </p> <p>3. Click on <code>Try it out</code></p> <p>4. Use the contents of the addModel.json file as the request body. It's easier if you right click on the link and open it in a new tab.</p> <p>5. Click on <code>Execute</code> and scroll down to the actual server repsonse (scroll past the example)</p> <p>6. Copy the ID of the created model from the response body. It is typically <code>1</code> as this ADS ML Service is currently empty. </p> <p>7. We've added the meta-data of the model, now we must add the model binary. Under <code>manage</code>, expand the <code>POST /models/{model_id} Add Binary</code> section </p> <p>8. Click on <code>Try it out</code></p> <p>9. Use the ID copied from the last API call, typically <code>1</code>.</p> <p>10. In the request body, keep <code>pickle</code> selected as the <code>format</code></p> <p>11. Download this pickle file onto your computer and use it as as the selected file </p> <p>12. Click on <code>Execute</code>. The response code is 201. The model named <code>service-payment-default-risk</code> is succesfully deployed. The following instructions validate that this deployment can be safely executed.</p> <p>13. Under <code>run</code>, expand the <code>/predictions</code> section</p> <p>14. Click on <code>Try it out</code></p> <p>15. Use the contents of the runModel.json file as the request body  Note: from your browser, please ensure to select Raw Data to view/copy the request body: </p> <p>16. Update the <code>$PREDICTION-ID$</code> in the json to the <code>ID</code> copied before, typically this is <code>1</code>.</p> <p>17. Click on <code>Execute</code>. The result should be as follows:</p> <pre><code>`{ \"result\": {  \"predictions\": 0,  \"scores\": [   0.9068544064724676,   0.0931455935275324  ]}}`\n</code></pre> <p></p> <p>Tip Keep this browser window open, you'll need the URL in the next step.  </p> <p>Success</p> <p>\u2139\ufe0f \u00a0 You have successfully imported the ADS ML Service, please proceed to the next section</p> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-deploy-integrate/#2-import-the-ads-project","title":"2. Import the ADS Project","text":"Expand to view <p>Warning</p> <p>The ADS Project requires an empty github repo and a git API token, if your team doesn't have git knowledge please contact an instructor. Public github seems to be the most reliable.  Create a Personal Access Token under Developer Settings. If you need additional guidance, please refer to section Item 91 - Creating a git repo for ADS in the Troubleshooting section.  </p> <ol> <li> <p>Download ClientOnboardingDecisions.zip</p> </li> <li> <p>Create an empty GIT repo and get its URL and API Key</p> </li> <li> <p>Log into your CP4BA cluster and Open IBM Business Automation Studio, select LDAP authentication at the login screen.</p> </li> <li> <p>Click to the menu in the upper-left corner and go to <code>Design</code> \u2192 <code>Business Automations</code></p> </li> <li> <p>Click on <code>Create</code> \u2192 <code>Decision Automations</code></p> </li> <li> <p>Provide <code>Client Onboarding</code> as the project name and click <code>Create</code></p> </li> <li> <p>Once the editor loads, click on <code>Import</code> and import the previously downloaded file ClientOnboardingDecisions.zip into the project </p> </li> <li> <p>Click on <code>Connect to Github</code> icon </p> </li> <li> <p>Enter the URL and API key of the GIT repo created previously</p> </li> <li> <p>Click on <code>Connect</code> in the top-right corner</p> </li> <li> <p>Go to the <code>Machine learning providers</code> tab</p> </li> <li> <p>Click on <code>New</code> in the top-right corner</p> </li> <li> <p>In the dialog, select <code>Open Prediction Service</code> as the <code>Type</code></p> </li> <li> <p>Enter <code>OPS</code> as the name</p> </li> <li> <p>Use the ADS ML Service (Open Prediction Service) URL in the <code>URL</code> field     Note: This is the same URL you used for importing the predictive model without the <code>/docs</code> suffix at the end.</p> </li> <li> <p>Click on <code>Test Connection</code> </p> </li> <li> <p>Click on <code>Save</code></p> </li> <li> <p>Go back to the <code>Client Onboarding</code> project </p> </li> <li> <p>Open <code>Client Onboarding Decisions</code></p> </li> <li> <p>Click on <code>Machine learning scoreboard</code></p> </li> <li> <p>Click on <code>Connect</code> and select <code>OPS</code> as the machine learning provider</p> </li> <li> <p>Expand the <code>service-payment-default-risk</code> ML model and select the <code>service-payment-default-risk</code> deployment </p> </li> <li> <p>Click <code>Next</code> twice to <code>Test invocation</code></p> </li> <li> <p>Test the decision by entering the following values:</p> <ul> <li>clientAnnualRevenue: 15708854</li> <li>clientExistenceDuration: 12</li> <li>clientEmployeeNumber: 3</li> <li>clientIndustry: 0</li> </ul> </li> <li> <p>Click on <code>Run</code></p> </li> <li> <p>Verify that the output matches the following: <code>{     \"result\": {         \"predictions\": 1,         \"scores\": [             0.014675209287711932,             0.9853247907122881         ]     } }</code> </p> </li> <li> <p>Click on <code>Next</code></p> </li> <li> <p>Click on <code>Generate from test output</code> then click <code>OK</code>. Verify that <code>predictions</code> and <code>scores</code> are added to the output schema.</p> </li> <li> <p>Click on <code>Apply</code> in the upper-right corner</p> </li> <li> <p>Under <code>Share changes</code> at the top, click on the number of changes </p> </li> <li> <p>Click on <code>Share</code> and then <code>Share</code> again in the popup</p> </li> <li> <p>In the <code>View history</code> tab, click on <code>Version +</code> and create a new version named <code>v21</code></p> </li> <li> <p>In the <code>Deploy</code> tab, expand <code>v21</code>, click on <code>Deploy</code> and wait for deployment to complete </p> </li> <li> <p>Back in the studio, go to <code>Design</code> \u2192 <code>Business Automations</code> \u2192<code>Decisions</code> and click on the <code>Client Onboarding</code> Decision project   </p> </li> <li> <p>Select the three-dot menu for <code>v21</code> and click on <code>Publish</code> and then click on <code>Publish</code> again to make the automation service available without restricting access.  </p> </li> </ol> <p>Success</p> <p>\u2139\ufe0f \u00a0 You have successfully imported the ADS project.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-deploy-integrate/#3-setup-the-rpa-server-optional","title":"3. Setup the RPA Server (optional)","text":"Expand to view <p>Note: You only need to perform these steps if you want to demo the RPA execution. You can choose to skip the execution of the RPA bot when you import the Workflow solution.</p> <ol> <li>Request auto-onboarding onto the SWAT RPA Tenant using this form</li> <li>Reserve an environment from here using your IBMID.    </li> <li>Click Environments on the left panel, and then click computer icon. </li> <li>Click Reserve for now, then click Submit. </li> <li>On the reservation page, make the appropriate selections as below. Once done, click Submit. </li> <li>Once you have reserved an environment, you will receive an email with a link to access the management console for the environment including a password (Desktop Access Information). It also contains a URL to access the IBM RPA Rest Service remotely (Application Service Information), please copy the Application Service Information URL and change HTTP to HTTPS. This will be used in the Workflow solution. </li> <li>Click the Desktop Access link above to open your environment. When you are prompted to enter environment password, enter the desktop password above. Wait a few minutes, your environment will be started as below. </li> <li>Click VM 5 \u2013 RPA to open the RPA client environment.  </li> <li>Start Firefox, click IBM RPA license from the bookmark toolbar to open IBM RPA license manager. </li> <li>You will see the message Not Licensed. Click Activate button to open the License Activation window. </li> <li>Enter your RPA license ID and Password, (ask the instructors for this) then click the Activate button. Once after the license is activated, you should be able to see the number of licenses available for each component. </li> </ol> <p>Success</p> <p>\u2139\ufe0f \u00a0 You have successfully set up the RPA server, please proceed to the next section</p> <p>Once you have setup the RPA server, import the Workflow solution.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-deploy-integrate/#4-import-the-workflow-solution","title":"4. Import the Workflow Solution","text":"Expand to view <ol> <li> <p>\u2139\ufe0f [SKO UPDATE] Download the workflow app. (This version contains minor changes to account for environmental differences). Workflow twx file.</p> </li> <li> <p>Login to IBM Business Automation Studio</p> </li> <li> <p>In the top-left corner, click on the menu icon and go to Business automations. </p> </li> <li> <p>Click on Workflow. </p> </li> <li> <p>Click on the Import button.</p> </li> <li> <p>Click on Browse and select the twx file downloaded in Step 1.</p> </li> <li> <p>Click on OK.</p> </li> <li> <p>Once the import completes, click on tile for the Client Onboarding Workflow project (Don't click on the open button but just the tile).</p> </li> <li> <p>Click on the 3-dot menu next to the Open button on the right and select Open in Process Designer </p> <p>Note</p> <p>The version numbers and dates in the screenshots maybe different from what you see in your system</p> </li> <li> <p>In Process Designer, click on the Environment Variables tab.</p> </li> <li> <p>Fill out credentials for a gmail account in the emailID and emailPassword fields under the Default column. Note that the password here must be an App Password and not your gmail password. This gmail account is used by the solution to send outbound emails. If you don't have a gmail account configured with an app password you should create one now or use the default one provided. </p> </li> <li> <p>If you are showcasing ADP as a part of the scenario, enter the ADP host, username, password, and projectID in their respective fields. If you are not, set adpEnabled to false.</p> <p>Note</p> <p>\u2139\ufe0f \u00a0 For 2022 SKO Tech Academy, we will not be showcasing ADP in this scenario  </p> </li> <li> <p>For the documentUploadPage environment variable, use the URL for Business Automation Navigator for CP4BA and add \"?desktop=CODocumentUpload\". You will add this desktop to the navigator in a later step. The default value is from another environment and illustrates a typical value but you must build yours using your navigator URL.</p> </li> <li> <p>The RPA bot is currently only executed if the user running the scenario matches the rpaBotExecutionUser. You can change this by updating the value of the rpaBotExecutionUser environment variable. In this environment the rpaBotExecution user will be called cp4admin, so only set this value to cp4admin once your option RPA server and configuration is complete otherwise the workflow will encounter errors. It is recommeded that you DO NOT set this value to be cp4admin until you have validated that the end to end COB scenario works without RPA first.</p> </li> <li> <p>If you are executing the RPA bot, update the value for the rpaServer environment variable from the environment you reserved using the previous step.</p> </li> <li> <p>The default target object store name is TARGET. If you have changed this, update the value for the tosName environment variable. Demo environments have the default target object store name of TARGET.</p> </li> <li> <p>In the top-right corner, click on the Finish Editing button. </p> </li> <li> <p>In the top-left corner, click on Business automations to go back to the BA Studio. </p> </li> <li> <p>Click on Open for the Client Onboarding Workflow automation project to open it in the Case Builder. </p> </li> <li> <p>In the top-right corner, click on the Deploy button. The deployment will take a few seconds. Wait until there is a green checkmark next to the button. </p> </li> <li> <p>In the top-left corner, click on Automations to go back to the BA Studio. </p> </li> <li> <p>Click on the tile for the Client Onboarding Workflow automation project.</p> </li> <li> <p>Click on the 3-dot menu for latest version of the project and click on Publish. </p> </li> <li> <p>Close the dialog that shows that the automation services were published successfully.</p> </li> <li> <p>In the top-left corner, click on the menu icon and go to Design \u2192 Business automations.</p> </li> <li> <p>Click on Create \u2192 External.</p> </li> <li> <p>Select Business Automation Workflow under Select the connection type. Note: this is now done automatically.</p> </li> <li> <p>Click on Next. Note: this is now done automatically.</p> </li> <li> <p>In the Connection name field, enter External BAW System.</p> </li> <li> <p>\u2139\ufe0f [SKO UPDATE] In the System URL field, use the Cloudpak Dashboard URL you retrieved from the confing map add /bas to the end of this URL.</p> </li> <li> <p>\u2139\ufe0f [SKO UPDATE] Enter the cp4admin credentials you retrieved from the confing map and click Next.</p> </li> <li> <p>In the Select a process application dropdown, select Client Onboarding.</p> </li> <li> <p>Select the checkbox for New Client Onboarding Request. </p> </li> <li> <p>Click on Next.</p> </li> <li> <p>In the Name field, enter Client_Onboarding_Workflows_External.</p> </li> <li> <p>Click on Publish.</p> </li> </ol> <p>Success</p> <p>\u2139\ufe0f \u00a0 You have successfully published the workflow solution.</p> <p>Go to top of subsection | Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-deploy-integrate/#5-import-objects-into-filenet-content-manager","title":"5. Import objects into FileNet Content Manager","text":"Expand to view <p>Warning</p> <p>You must complete the Import the Workflow Solution prior to this step as the document subclasses are created during the workflow import</p> <ol> <li> <p>Login to ACCE using the URL for \"Content Platform Engine administration\" You can switch to english locale if needed by clicking on the persona icon in upper right corner, and select <code>Change Language and Locale Settings</code>.  </p> </li> <li> <p>Open the target object store (<code>TARGET</code> OR <code>BAWTOS</code>), by clicking on it. In the Tech Academy systems this is <code>TARGET</code> so don't be alarmed by the image below that shows <code>BAWTOS</code>.  </p> </li> <li> <p>Create a new folder named <code>Client Documents</code> under the root folder.  </p> <ol> <li> <p>On the navigation area on the left side, open <code>Browse</code> and click on <code>Root folder</code>. </p> </li> <li> <p>Click on the <code>Actions</code> pulldown menu and click on <code>New Folder</code>. </p> </li> <li> <p>The <code>Define New Folders Dialog</code> opens on the right side. Set the Folder name to <code>Client Documents</code>. Then click on <code>Next &gt;</code> two times, then on <code>Finish</code>. </p> </li> </ol> </li> <li> <p>In the <code>Client Documents</code> folder, create the documents from the table below (see instructions below the table on how to add a document). It is recommended that you open each PDF and image in a new browser tab and save the PDF or image from that tab :  </p> </li> </ol> Document Document Title Document Class Document Properties Banking Information - Automation Elite Inc.pdf Banking Information - Automation Elite Inc Banking Information Client Name: Automation Elite Inc.Account Number: 1179476345 Certificate of Incorporation - Automation Elite Inc.pdf Certificate of Incorporation - Automation Elite Inc Client Document Client Name: Automation Elite Inc. Utility Bill - Automation Elite Inc.pdf Utility Bill - Automation Elite Inc Utility Bill Client Name: Automation Elite Inc.Service Address: 3974 Carson St, Lansing, MI 48911 June Marie - Driver's License.png June Marie - Driver's License Client Document Client Name: Automation Elite Inc. Legacy Consulting - Banking Information.pdf Legacy Consulting - Banking Information Banking Information Client Name: Legacy ConsultingAccount Number: 7250512345 Legacy Consulting - Certificate of Incorporation.pdf Legacy Consulting - Certificate of Incorporation Client Document Client Name: Legacy Consulting"},{"location":"client-onboarding/co-deploy-integrate/#perform-the-steps-below-for-each-document-in-the-table-above","title":"Perform the steps below for each document in the table above","text":"Expand to view (REQUIRED) <p>a. In the navigation area, navigate to \"Browse\", and \"Root Folder\". Click on \"Client Documents\" to bring up the contents of the folder. From the opened \"Client Documents\" folder, invoke \"New Document\" from the Actions pulldown menu.  </p> <p> </p> <p>b. In the New Document wizard, on the first page provide the document title, and select the Document Class from the table. Then click \"Next\".  </p> <p> </p> <p>c. On the \"Document Content Source\" page, click on \"Add\" in the \"Content Element\" section. Use the popup window to upload the document. Click \"Browse\" and locate the document on your harddisk, then click on \"Add Content\" to close the dialog. Then click \"Next\" on the \"New Document\" wizard.  </p> <p> </p> <p>d. On the next page, provide the property values, according to the table above. No further changes are required, so press \"Next\" until the final page, then \"Finish\" and then \"Close\".  </p> <p>Success</p> <p>\u2139\ufe0f \u00a0 You have successfully imported the objects required for the solution into FileNet Content Manager. Next, proceed to Import the Business Automation Application into IBM Business Automation Navigator.</p>"},{"location":"client-onboarding/co-deploy-integrate/#prepare-a-shared-environment-for-labs-optional","title":"Prepare a shared environment for labs (OPTIONAL)","text":"Expand to view (NOT REQUIRED FOR SKO TECH ACADEMY) <p>\u26a0\ufe0f \u00a0 These steps are not required for SKO Tech Academy.  If you are using an environment that will be shared by multiple teams, it is important to disable certain permissions so that the objects defined for the Client Onboarding scenario are immutable.</p> <ol> <li> <p>For the <code>Client Document</code> document class, update the security settings: remove the shared group (eg: cp4bausers) from the Default Instance Security. Update the Security and lower the access level for the cp4bausers group to <code>Modify properties</code>.</p> <ol> <li> <p>On the Navigation area on the left side, open <code>Data Design</code>, <code>Classes</code>, <code>Document</code> and click on the <code>Client Document</code> class to bring up its properties on the right side. </p> </li> <li> <p>Select the <code>Default Instance Security</code> tab. Select the checkbox in front of the line with the shared user group (eg: cp4bausers) group and click on <code>Remove</code>. Then click on <code>Save</code>. </p> </li> <li> <p>Select the <code>Security</code> tab. Select the checkbox in front of the line with the shared user group (eg: cp4bausers) and click on <code>Edit...</code>. In the dialog, set the permission group to <code>Modify properties</code>, then click on <code>Ok</code>. Click on <code>Save</code> again on the <code>Client Document</code> class properties. </p> </li> </ol> </li> <li> <p>Expand the <code>Client Document</code> class on the left and repeat the previous step for the <code>Banking Information</code> and <code>Utility Bill</code> document classes.</p> </li> </ol> <p>Success</p> <p>\u2139\ufe0f \u00a0 You have successfully imported the objects required for the solution into FileNet Content Manager. Next, proceed to Import the Business Automation Application into IBM Business Automation Navigator.</p> <p></p> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-deploy-integrate/#6-import-the-business-automation-application-app","title":"6. Import the Business Automation Application app","text":"Expand to view <p>\u2139\ufe0f [SKO UPDATE] Regenerate the applications from their twx source files before importing them as apps into Navigator.  </p> <ol> <li> <p>Download the the two twx files shown below :</p> <ul> <li>Client_Onboarding_Request - 6.twx </li> <li>Client_Onboarding_Document_Upload - v1.3.twx</li> </ul> <p>In the top-left corner, click on the menu icon and go to Business applications. </p> <p>Click on import and import both applications, one at at a time.</p> <p>Once both applications have been imported go into each application and export a zip file.  Save these exported zip files somewhere you can find them, you'll need them for the next step.</p> </li> <li> <p>Login to the Navigator admin desktop (URL should end with <code>desktop=admin</code>). Use the URL for Business Automation Navigator for CP4BA and add <code>?desktop=admin</code></p> </li> <li> <p>Click on Connections in the menu on the left.</p> <p></p> </li> <li> <p>Select the the APPENGO connection and click on the Edit button.</p> <p></p> </li> <li> <p>Click on the the Connect... button.</p> </li> <li> <p>Click on the Applications tab.</p> </li> <li> <p>Click on the Import button.</p> <p></p> </li> <li> <p>Choose the the previously downloaded file ClientOnboardingRequest.zip and click on the Import button. If an information dialog pops up after import, you can close it.</p> <p>NOTE: The target object store name specified in the Application is TARGET. If you are using a production environment, the target store name could be different (eg: BAWTOS). In that case, the 2nd page of the application won't show any documents as it looks for the documents in the TARGET object store. To fix this, you can import the application template in Studio from the main SWAT COB Repo and rebuild the app.</p> </li> <li> <p>Click on the 3-dot menu for the Client Onboarding application and select Details,</p> </li> <li> <p>Click on the Permissions tab.</p> </li> <li> <p>Click on the Add Teams button.</p> </li> <li> <p>In the popup, enter # in the Filter by Teams field and hit Enter.</p> <p></p> </li> <li> <p>Select the #AUTHENTICATED-USERS team and click on the right-facing arrow to move the team to the Selected column.</p> </li> <li> <p>Click on the Add button.</p> </li> <li> <p>Click on the Close button.</p> </li> <li> <p>Similarly, import the application ClientOnboardingDocumentUpload.zip and add the #AUTHENTICATED-USERS team to it.</p> </li> <li> <p>Close the APPENGO and Connections tabs at the top.</p> </li> <li> <p>In the Desktops tab, click on the Import button.</p> </li> <li> <p>Download the two desktop properties files   </p> <ul> <li>Client Onboarding Request Desktop </li> <li>CO Document Upload Desktop </li> </ul> </li> <li> <p>Click the import button to import each desktop. Select each file in turn and click import, you can ignore any warnings.</p> </li> </ol> <p>Success</p> <p>\u2139\ufe0f \u00a0 You have successfully imported the Business Automation Application app</p> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-deploy-integrate/#7-import-the-business-automation-insights-data","title":"7. Import the Business Automation Insights data","text":"Expand to view <p>Warning</p> <p>There is a defect in CP4BA 21.0.3 IF008 affecting Case/BAI, events from Case are not emitted. If you are building dashboards make sure these work with preloaded data and don't depend on something you've added to the Case solution. Process events are unaffected.</p>"},{"location":"client-onboarding/co-deploy-integrate/#71-prepare-data-and-import-dashboard","title":"7.1 Prepare Data and Import Dashboard","text":"Expand to view <p>Note</p> <p>These instructions describe how to update the BAI sample data so that it can be uploaded to your CP4BA cluster. If you need more guidance to update the BAI data please refer to Item 84 - Preparing BAI Data in the Troubleshooting section, where you will find a more prescriptive set of instructions to complete steps 1, 2 and 3 below.  </p> <p>It is recommended to read the outline instructions below to understand the goal of this exercise, but then use the steps in the troubleshooting section if you are not confident. If you use the alternative instructions in the troubleshooting section for steps 1-3, please return here to complete steps 4-6. </p> <p>High level instructions :    </p> <ol> <li> <p>Download the contents of the following directory - Business Automation Insights.</p> </li> <li> <p>Replace the index names in the downloaded files:</p> <p>In the <code>process-data.json</code> and <code>case-data.json</code> files, the index name parameter must match the index names of your environment. The index names are dependent on the date of the install. For example, one of the index names in the provided data files is <code>icp4ba-bai-process-summaries-completed-idx-ibm-bai-2021.11.11-000001</code>. The date element <code>2021.11.11</code> must be replaced by the date in your new clusters index name. The image below shows an example of the data we need to update. You can use the tool of your choice to perform a global find &amp; replace to update all the dates so they reflect the index names in your cluster. The VSCode editor works well, but any code editor should work. Section 84 of the troubleshooting guide describes this process and uses the sed command line tool to perform the substitution.  </p> <p></p> <p>You can get the index names for your environment using the following command (Replace <code>{esadmin}</code> with the elasticsearch admin user ID, replace <code>{espassword}</code> with the elastichsearch admin user password &amp; replace <code>{eshost}</code> with the elasticsearch URL):</p> <pre><code>curl -k -XGET -u {esadmin}:{espassword} '{eshost}/_aliases'\n</code></pre> </li> <li> <p>Once you have updated the sample data files, from the folder where you've downloaded the files and replaced the index names, run the following script to import the data (replace <code>{esadmin}</code> with the elasticsearch admin user ID, replace <code>{espassword}</code> with the elastichsearch admin user password &amp; replace <code>{eshost}</code> with the elasticsearch URL):</p> <pre><code>ES_ADMIN={esadmin}\nES_PASSWORD={espassword}\nES_HOST={eshost}\n\ncurl -k -XPOST -H 'Content-Type: application/json' -u ${ES_ADMIN}:${ES_PASSWORD} ${ES_HOST}/_bulk --data-binary @case-data.json\ncurl -k -XPOST -H 'Content-Type: application/json' -u ${ES_ADMIN}:${ES_PASSWORD} ${ES_HOST}/_bulk --data-binary @process-data.json\ncurl -k -XPOST -H 'Content-Type: application/json' -u ${ES_ADMIN}:${ES_PASSWORD} ${ES_HOST}/_bulk --data-binary @ads-data.json\n</code></pre> </li> <li> <p>Open Business Performance Center and click Import. </p> <p></p> </li> <li> <p>Click Browse.</p> </li> <li> <p>Select Client Onboarding Completed.json (downloaded earlier) and click Import.  </p> <p></p> </li> </ol> <p>Go to top of subsection | Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-deploy-integrate/#72-create-and-configure-goals","title":"7.2 Create and configure goals","text":"Expand to view <ol> <li> <p>Open Business Perfomance Center</p> </li> <li> <p>Click on the Goals tab  </p> <p></p> </li> <li> <p>Click Create </p> <p></p> </li> <li> <p>For Name enter Focus Corp's top Client Onboarding KPI Completed</p> </li> <li> <p>For Description enter Focus on the three top KPI identified by senior management team</p> </li> <li> <p>For Priority select High</p> </li> <li> <p>Click Goal color to Purple</p> </li> <li> <p>Your Goal definition should look exactly like this:  </p> <p></p> </li> <li> <p>Click Save</p> </li> <li> <p>Click on the Dashboards tab</p> </li> <li> <p>Open the Client Onboarding Completed dashboard</p> </li> <li> <p>For the Average Revenue from Services Fees for Approved Clients visualization, click on the Edit icon.  </p> <p></p> </li> <li> <p>For the Business goal field, select Focus Corp's top Client Onboarding KPI from the drop-down list  </p> <p></p> </li> <li> <p>Click Done</p> </li> <li> <p>Repeat steps 11-14 to add the same goal to the Highest Service Fee by Industry Sector visualization  </p> </li> </ol> <p></p> <p>Success</p> <p>\u2139\ufe0f \u00a0 You have successfully imported the Business Automation Insights Data  </p> <p>Go to top of subsection | Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-deploy-integrate/#73-prepare-shared-environment","title":"7.3 Prepare shared environment","text":"Expand to view <ol> <li> <p>Open Business Performance Center</p> </li> <li> <p>On Client Onboarding Completed dashboard select the 3-dot menu and click Share with everyone.</p> <p></p> </li> </ol> <p>Success</p> <p>\u2139\ufe0f \u00a0 You have successfully shared the environment</p> <p>Go to top of subsection | Go to top of section | Go to top of page</p> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-deploy-integrate/#8-validate-client-onboarding-solution","title":"8. Validate Client Onboarding solution","text":"Expand to view <p>\u2139\ufe0f [SKO UPDATE] This is a quick test to validate the main parts of the solution. This test does not include RPA integration</p> <p>Warning</p> <p>There is a defect in CP4BA 21.0.3 IF008 affecting Case/BAI, events from Case are not emitted. If you are building dashboards make sure these work with preloaded data and don't depend on something you've added to the Case solution. Process events are unaffected.</p> <ol> <li> <p>Open the Client Onboarding desktop. The URL will begin with the URL from the config map called <code>Business Automation Navigator for CP4BA</code>. Add <code>?desktop=ClientOnboarding</code> to the end of this URL, for example: <code>https://cpd-cp4ba-starter.cp4ba-tech-academy-400310-64b8809ea4bdf3ac103ec2bdb80f1d21-0000.us-south.containers.appdomain.cloud/icn/navigator/?desktop=ClientOnboarding</code></p> </li> <li> <p>Enter <code>el</code> in the client name, a type-ahead search should find Automation Elite Inc. Select Automation Elite Inc then click search. The values for the client should be populated.  </p> </li> <li> <p>Scroll down to Service Onboarding and select Finance the Corporate Credit Card. Click Calculate Services Fee to trigger a calculation in ADS. </p> </li> <li> <p>Click Review Documents and the screen below will appear. It may take a few moments for the screen to appear and the documents to load. </p> </li> <li> <p>Click on Submit Onboarding Request and a confirmation screen will be displayed with a unique reference. </p> </li> <li> <p>Load the BAW Process Inspector, Load Process Admin and select Process Inspector from the top menu bar. As this is a new cluster the only activity will be from from the Onboarding Request just sibmitted.  Click the refresh button and you should see a similar list of tasks as those below.  TODO - help t </p> </li> <li> <p>TODO </p> </li> <li> <p>TODO </p> </li> </ol>"},{"location":"client-onboarding/co-deploy-integrate/#9-validate-rpa-integration-optional","title":"9. Validate RPA Integration (optional)","text":"Expand to view <p>If you have provisioned an RPA server and you'd like to test the integration follow these steps. Please make sure you have requested a userID on the SWAT tenant using this link</p> <ol> <li> <p>Refer to your TechZone reservation email and identify the URL for your RPA node </p> </li> <li> <p>Login to your RPA node and open Firefox and click on the IBM RPA License bookmark. Click Activate. </p> </li> <li> <p>Enter the details of the RPA license (ask an instructor, this should be in Slack).  </p> </li> <li> <p>Open RPA Studio and enter your email address and password to the SWAT RPA tenant, refer to the note above. </p> </li> <li> <p>Open the Client Onboarding Workflow in Process Designer and update the environment variables with your RPA reservation. Make sure the user is cp4admin, also make sure you use https. Save these settings. </p> </li> <li> <p>Run the onboarding scenario described in section 8 with these updated settings with your RPA VM running. If the RPA VM restarts you'll need to repeat the activation in step 2. If correctly configured you should see bot activity in the RPA VM. </p> </li> </ol> <ol> <li> <p>The Client Onboarding assets have been adapted from the IBM TechJam 21.0.3 materials as developed by the IBM SWAT Team\u00a0\u21a9</p> </li> </ol>"},{"location":"client-onboarding/co-instructions/","title":"Instructions","text":"<p>You should now have a good understanding of the Client Onboarding use case scenario. At the end of this course, you will present a customized version of the Client Onboarding solution designed to meet requirements for the client PoC.  </p> <p>Below is a high-level overview of how your team will build the Client Onboarding solution:</p> Item Description 1 Deploy and integrate the Client Onboarding artifacts [Team] 2 Rebuild portions of the lab based on your client PoC requirements [Individual] 3 Once each team member completes a pillar Client Onboarding labs from the Rebuild section, each team will choose one or more customization scenarios they believe they can finish in the allotted time.  This exercise serves to mimic a client surprising the tech seller during a PoC or providing a new requirement that you cannot negotiate out of the scenario, and you need to find a way to show it within the PoC you already built. <p>Additional note   - Please also review the Additional Use Cases for additional ideas on customizations.   - Active participation will be critical to the success of this course. Specifically, there will be tremendous value learning  from your peers not only on the capabilities but by discussing experiences executing a client PoC.</p> <p>Warning</p> <p>Content including the labs are based on the IBM SWAT Client Onboarding1 materials which are configured for the Production pattern.  Our environment uses the Starter pattern and based on that, you may encounter some differences at certain steps.  Below are some differences you should be aware of, please post to the slack channel if you encounter additional differences. - BAWTOS =&gt; TARGET - cp4bausers =&gt; cp4admin  </p>"},{"location":"client-onboarding/co-instructions/#revisit-course-breakdown","title":"Revisit Course Breakdown","text":"Expand to view"},{"location":"client-onboarding/co-instructions/#stand-deliver-guidelines","title":"Stand &amp; Deliver Guidelines","text":"Expand to view <p>You and your team are required to demonstrate the Client Onboarding use case you\u2019ve been working on during the technical academy. You are expected to present a business scenario (not only a technical feature demo) and customize your demo narration and/or content to make it specific to a client\u2019s story and customization request(s) you\u2019ve incorporated.  </p> <p> Click the link below to review the evaluation criteria for your team presentation: Stand &amp; Deliver Guidelines </p> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-instructions/#team-assignment","title":"Team Assignment","text":"Expand to view <p>Team assignments will be posted to the event slack channel.</p> <p>The setup of the Client Onboarding solution should be done as a team using a cluster that has been pre-provisioned for you.  Access information to the cluster will also be identified in the slack channel.  </p> <p>Please ensure every team member is aware of the Stand &amp; Deliver Guidelines and work together to craft your client's story including the client PoC requirements.  </p> <p>Go to top of section | Go to top of page</p> <ol> <li> <p>The Client Onboarding assets have been adapted from the IBM TechJam 21.0.3 materials as developed by the IBM SWAT Team\u00a0\u21a9</p> </li> </ol>"},{"location":"client-onboarding/co-overview/","title":"Overview","text":"<p>IBM Cloud Pak\u00ae for Business Automation offers design, build, run, and automation services to rapidly scale your programs and fully execute and operationalize an automation strategy.</p> <p>The Client Onboarding1 scenario is an end-to-end solution that showcases the art of the possible with IBM Cloud Pak for Business Automation (CP4BA).  </p> <p>Client Onboarding Overview</p> 123456789 <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"client-onboarding/co-overview/#end-to-end-demonstration-assets","title":"End-to-End Demonstration Assets","text":"Expand to view <p>View the end-to-end in action in this video:  </p> <p></p> <p>See below for instructions to demonstrate the Client Onboarding scenario: IBM Cloud Pak for Business Automation: An End-to-End Demonstration Approximate Duration: 2 hours  </p> <p></p> <p>Go to top of section | Go to top of page</p> <ol> <li> <p>The Client Onboarding assets have been adapted from the IBM TechJam 21.0.3 materials as developed by the IBM SWAT Team\u00a0\u21a9</p> </li> </ol>"},{"location":"client-onboarding/co-rebuild-backup-ty/","title":"Rebuild (Individual)","text":"<p>Note</p> <p>These labs assume that you have installed IBM Cloud Pak for Business Automation v21.0.3 on a ROKS cluster as described here and imported the Client Onboarding1 scenario as described here.  </p> <p> The labs below can be used to learn how various capabilities were configured in the Client Onboarding solution. As you already have expertise in one or more capability, you are encouraged to select a lab for a capability that you have the LEAST experience with and/or is the most needed skill in your market.  Further, every member of the team should select a different rebuild lab for their first.  Once the first Rebuild lab is completed, you may select additional rebuild labs or proceed to Customization.  As you perform the lab, think and coordinate with your team members how you might be able to customize the capability to fit client PoC requirements.  This lab is considered an individual activity as each team member should work on a separate rebuild lab.  However, all work should be done on the shared team cluster and collaboration is highly recommended as you build components into your customized Client Onboarding PoC demonstration (Day 4).  </p> <p>Warning</p> <p>Please note that you may need to redeploy your application depending on the rebuild lab update.</p> <p>Warning</p> <p>Content including the labs are based on the IBM SWAT Client Onboarding1 materials which are configured for the Production pattern.  Our environment uses the Starter pattern and based on that, you may encounter some differences at certain steps.  Below are some differences you should be aware of, please post to the slack channel if you encounter additional differences. - BAWTOS =&gt; TARGET - cp4bausers =&gt; cp4admin  </p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#overview","title":"Overview","text":"Expand to view <p>The table below lists all capabilities that we currently have labs for. A capability may contain one or more labs depending on the complexity.</p> Capability Approximate Duration IBM Business Automation Application 2-3 hours IBM Business Automation Insights 1 hour IBM FileNet Content Manager 2-4 hours IBM Automation Decision Services 3 hours IBM Robotic Process Automation 3-4 hours IBM Business Automation Workflow 5-6 hours <p>Select the links below to view:     - Labs     - Solution Exports </p> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#1-ibm-business-automation-application","title":"1. IBM Business Automation Application","text":"Expand to view"},{"location":"client-onboarding/co-rebuild-backup-ty/#overview_1","title":"Overview","text":"<p>This is a low-code capability of IBM Cloud Pak for Business Automation that let's business users leverage services built by developers in other parts of the platform such as Workflow, Decisions &amp; Content.</p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#labs","title":"Labs","text":"<ul> <li>Introduction to IBM Business Automation Application  (View) This lab introduces you to the key concepts of IBM Business Automation Application including Application Designer. In this you will learn how to create toolkits, templates and applications that integrate with the Workflow, Decisions &amp; Content capabilities of the CP4BA platform. Approximate Duration: 2 Hours </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#2-ibm-business-automation-insights","title":"2. IBM Business Automation Insights","text":"Expand to view"},{"location":"client-onboarding/co-rebuild-backup-ty/#overview_2","title":"Overview","text":"<p>IBM Business Automation Insights (BAI) is a cloud capability to capture end to end business data (events) from Cloud Pak for Automation (CP4A) platform components to operational data store and long-term store (data lake). BAI provides real-time operational visibility to Business Managers via custom or pre-built set of dashboards. Custom dashboards can be built by IT (using Kibana) or business users (using Business Performance Center). The data collected by BAI and stored in the data lake can be used to inject AI into CP4A platform, for example it can be used to make recommendations to business managers and knowledge workers. Business Performance Center is a no-code monitoring application native to IBM Business Automation Insights. You can design and share dashboards in minutes that capture business data in near real time and provide real time awareness of important business activities and processes.</p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#labs_1","title":"Labs","text":"<p>Track 2 - Developer Role / Solution Implementation</p> <ul> <li>Build Business Performance Center Dashboards  (View) In the lab, you will learn how to build and use Business Performance Center dashboards to provide insights into a Client Onboarding workflow solution for line of business users. Approximate Duration: 1 Hour </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#3-ibm-filenet-content-manager","title":"3. IBM FileNet Content Manager","text":"Expand to view"},{"location":"client-onboarding/co-rebuild-backup-ty/#overview_3","title":"Overview","text":"<p>IBM FileNet Content Manager is a flexible, full-featured content management solution that provides the foundation for IBM Cloud Pak\u00ae for Business Automation. In labs you will get introduced to important core concepts of FileNet Content Platform Engine and Content Services GraphQL which will enable you to use FileNet Content Platform Engine to build the information architecture for automation projects realized with Cloud Pak for Business Automation.</p> <p>Warning</p> <p>\u2139\ufe0f [SKO UPDATE] We will NOT use the CLOS object store and instead use the existing CONTENT object store.  </p> Expand to view <ol> <li> <p>Log into the FileNet Administrative Console for Content Engine (ACCE) as cp4admin and select the object store named CONTENT.    Note: the entry in the access-info file will be named: Content Platform Engine administration </p> </li> <li> <p>Right-click on the object store name and invoke the Security Script Wizard. In the dialog which opens click the link \"SecurityScriptWizard.zip\", download the ZIP file, and unpack it. </p> </li> <li> <p>Provide provide the names of the unpacked file on the two entry boxes as shown below, then click \"Next\" </p> </li> <li> <p>On the next page, select \"Object Store Administrators\", then click \"Add User/Group permission\".  </p> </li> <li> <p>Confirm that the cp4admin is in this group.  If not, add cp4admin user to the right side for adding it. Then click OK.  </p> </li> <li> <p>Press Next to let the Wizard run, and dismiss the warning message. Close the \"Execute Script\" window after it completed.  </p> </li> <li> <p>Navigate to Administrative -&gt; Storage -&gt; Storage Policies. Select \"Select the storage area from a list\" and specify content_operations</p> </li> <li> <p>Navigate to Data Design -&gt; Property Templates. Create two property templates with following:  </p> Name Symbolic Name Type Case Reference ID Case_Reference_ID String Client Name Client_Name String </li> <li> <p>Navigate to Data Design -&gt; Classes. Create a new subclass of the \"Folder\" class with the name \"SWAT Jam Case Folder\" (Symbolic ID SWAT_JAM_Case_Folder).  </p> </li> <li> <p>Open the Folder class and add the two new property templates from above. Save the modified folder subclass.  </p> </li> <li> <p>Create the indicated folder structure under the Root folder. The Case Folders folder is a regular folder using the Folder object class.  The Sample Test Folder TEST1 should be created using the SWAT Jam Case Folder class and have the Case Reference ID and Client Name properties set to TEST1.  The Sample Test Folder TEST2 should be created using the SWAT Jam Case Folder class and have the Case Reference ID and Client Name properties set to TEST2.  </p> </li> <li> <p>Open Content Navigator, select the existing content desktop and update the Layout tab and enable the Simple Search feature (if it is not already enabled).</p> </li> </ol> <p>\u2139\ufe0f [SKO UPDATE] Use the symbolic names for Case Reference ID (Case_Reference_ID) and Client Name (Client_Name) properties.  Replace these with where you see usrxxxReferenceID and usrxxxClientName in the lab document.  </p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#labs_2","title":"Labs","text":"<ul> <li> <p>Setting up FileNet Content Manager for Automation Projects on Cloud Pak for Business Automation  (View) In this lab, you will create a small hierarchy of Document classes to capture different kinds of documents together with custom metadata, and will learn about the most important security concepts. You will explore Document storage and learn to migrate documents between different Storage Areas.  Further you will determine how to trigger custom actions for example when new documents have arrived, and how to configure and test functionality of content based retrieval, i.e. searches for documents based on information contained in the documents themselves, not (only) on their metadata.  On this lab, for triggering the custom actions, a custom javascript is used to file a newly uploaded document into a folder, those identity is derived from a search. The script is available in the Lab Data folder. Approximate Duration: 4 - 5 hours </p> </li> <li> <p>Interfacing FileNet Content Platform Engine with GraphQL on Cloud Pak for Business Automation  (View) The second lab on GraphQL builds on top of the first one, as the searches performed using GraphQL use the documents and document classes defined in the first lab. Here you learn by a series of example, how to build the most important queries using GraphQL. The examples also show how to download documents using GraphQL, how to create folders, and modify security settings. Approximate Duration: 1.5 - 2 hours </p> </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#4-ibm-automation-decision-services","title":"4. IBM Automation Decision Services","text":"Expand to view"},{"location":"client-onboarding/co-rebuild-backup-ty/#overview_4","title":"Overview","text":"<p>Part of the IBM Cloud Pak\u00ae for Business Automation platform, IBM Automation Decision Services provides a comprehensive environment for authoring, managing, and running decision services. Business experts can infuse intelligence into business decisions by combining decision models and predictive models into decision services. Business decisions can be published as Automation Services and easily consumed from other capability of the platform such as Workflow.</p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#labs_3","title":"Labs","text":"<ul> <li>Manage Decisions and infuse Machine Learning  (View) This Lab  introduces you to the key concepts of IBM Business Automation Decision Services. It includes three exercises that can be done separately. In this Lab you learn how to model business decisions, infuse intelligence by adding a predictive model, share and publish decision services. Approximate Duration: 3 hours </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#5-ibm-robotic-process-automation","title":"5. IBM Robotic Process Automation","text":"Expand to view"},{"location":"client-onboarding/co-rebuild-backup-ty/#overview_5","title":"Overview","text":"<p>Going from simple, back-office task automation to scaled  automation to handle time-consuming business processes can be a  challenge. The IBM\u00ae Robotic Process Automation offering helps you automate more business and IT processes at scale with the ease and speed of traditional RPA. Software robots, or bots, can act on AI insights to  complete tasks with no lag time and enable you to achieve digital transformation.            </p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#labs_4","title":"Labs","text":"<ul> <li> <p>Application Automation using IBM RPA  (View) In this lab you will learn how to use IBM RPA Studio to develop a bot to automate a Java swing application and a web application. </p> </li> <li> <p>IBM RPA and Workflow Integration  (View) In this lab you will learn how the bot designed in RPA Studio can easily be integrated into a business process developed with the Workflow capability in IBM Cloud Pak for Business Automation.</p> </li> <li> <p>Chatbot Development and Configuration  (View) In this lab you will learn how to develop a chatbot script using IBM RPA Studio, and then shows how to configure the chat mapping through web client.  The labs requires an IBM RPA client environment which can be reserved here. Once you have reserved an environment, you will receive a link to the environment via email. Start the environment by selecting the start button in the upper right corner of the page. Please use your license ID/password to activate RPA client as described in the lab instructions. </p> </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#6-business-automation-workflow","title":"6. Business Automation Workflow","text":"Expand to view <ol> <li> <p>The Client Onboarding assets have been adapted from the IBM TechJam 21.0.3 materials as developed by the IBM SWAT Team\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"client-onboarding/co-rebuild-backup-ty/#overview_6","title":"Overview","text":"<p>IBM Business Automation Workflow is software that combines business process management and case management  capabilities in a single integrated workflow solution. It unites information process, and users to provide a 360-degree view of work to help drive more successful business outcomes.</p>"},{"location":"client-onboarding/co-rebuild-backup-ty/#labs_5","title":"Labs","text":"<ul> <li>Introduction to IBM Business Automation Workflow  (View) This lab showcases how you can create a sample Workflow project using Case features. It covers Case &amp; Process integration and building UIs using Coaches.  Approximate Duration: 3-4 hours </li> <li>Consume &amp; Publish Automation Services in Workflow  (View) This lab showcases how you can consume capabilities from the IBM Automation platform using Automation Services. You will also create an external service to invoke a Java app that sends out emails. Finally, you will learn how an automation service can be published for others to consume. Approximate Duration: 2 hours </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild/","title":"Rebuild (Individual)","text":"<p>Note</p> <p>These labs assume that you have installed IBM Cloud Pak for Business Automation v21.0.3 on a ROKS cluster as described here and imported the Client Onboarding1 scenario as described here.  </p> <p> The labs below can be used to learn how various capabilities were configured in the Client Onboarding solution. As you already have expertise in one or more capability, you are encouraged to select a lab for a capability that you have the LEAST experience with and/or is the most needed skill in your market.  Further, every member of the team should select a different rebuild lab for their first.  Once the first Rebuild lab is completed, you may select additional rebuild labs or proceed to Customization.  As you perform the lab, think and coordinate with your team members how you might be able to customize the capability to fit client PoC requirements.  This lab is considered an individual activity as each team member should work on a separate rebuild lab.  However, all work should be done on the shared team cluster and collaboration is highly recommended as you build components into your customized Client Onboarding PoC demonstration (Day 4).  </p> <p>Warning</p> <p>Please note that you may need to redeploy your application depending on the rebuild lab update.</p> <p>Warning</p> <p>Content including the labs are based on the IBM SWAT Client Onboarding1 materials which are configured for the Production pattern.  Our environment uses the Starter pattern and based on that, you may encounter some differences at certain steps.  Below are some differences you should be aware of, please post to the slack channel if you encounter additional differences. - BAWTOS =&gt; TARGET - CLOS =&gt; CONTENT - cp4bausers =&gt; cp4admin  </p>"},{"location":"client-onboarding/co-rebuild/#overview","title":"Overview","text":"Expand to view <p>The table below lists all capabilities that we currently have labs for. A capability may contain one or more labs depending on the complexity.</p> Capability Approximate Duration IBM Business Automation Application 2-3 hours IBM Business Automation Insights 1 hour IBM FileNet Content Manager 2-4 hours IBM Automation Decision Services 3 hours IBM Robotic Process Automation 3-4 hours IBM Business Automation Workflow 5-6 hours <p>Select the links below to view:     - Labs     - Solution Exports </p> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild/#1-ibm-business-automation-application","title":"1. IBM Business Automation Application","text":"Expand to view"},{"location":"client-onboarding/co-rebuild/#overview_1","title":"Overview","text":"<p>This is a low-code capability of IBM Cloud Pak for Business Automation that let's business users leverage services built by developers in other parts of the platform such as Workflow, Decisions &amp; Content.</p>"},{"location":"client-onboarding/co-rebuild/#labs","title":"Labs","text":"<ul> <li>Introduction to IBM Business Automation Application  (View) This lab introduces you to the key concepts of IBM Business Automation Application including Application Designer. In this you will learn how to create toolkits, templates and applications that integrate with the Workflow, Decisions &amp; Content capabilities of the CP4BA platform. Approximate Duration: 2 Hours </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild/#2-ibm-business-automation-insights","title":"2. IBM Business Automation Insights","text":"Expand to view"},{"location":"client-onboarding/co-rebuild/#overview_2","title":"Overview","text":"<p>IBM Business Automation Insights (BAI) is a cloud capability to capture end to end business data (events) from Cloud Pak for Automation (CP4A) platform components to operational data store and long-term store (data lake). BAI provides real-time operational visibility to Business Managers via custom or pre-built set of dashboards. Custom dashboards can be built by IT (using Kibana) or business users (using Business Performance Center). The data collected by BAI and stored in the data lake can be used to inject AI into CP4A platform, for example it can be used to make recommendations to business managers and knowledge workers. Business Performance Center is a no-code monitoring application native to IBM Business Automation Insights. You can design and share dashboards in minutes that capture business data in near real time and provide real time awareness of important business activities and processes.</p>"},{"location":"client-onboarding/co-rebuild/#labs_1","title":"Labs","text":"<p>Track 2 - Developer Role / Solution Implementation</p> <ul> <li>Build Business Performance Center Dashboards  (View) In the lab, you will learn how to build and use Business Performance Center dashboards to provide insights into a Client Onboarding workflow solution for line of business users. Approximate Duration: 1 Hour </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild/#3-ibm-filenet-content-manager","title":"3. IBM FileNet Content Manager","text":"Expand to view"},{"location":"client-onboarding/co-rebuild/#overview_3","title":"Overview","text":"<p>IBM FileNet Content Manager is a flexible, full-featured content management solution that provides the foundation for IBM Cloud Pak\u00ae for Business Automation. In labs you will get introduced to important core concepts of FileNet Content Platform Engine and Content Services GraphQL which will enable you to use FileNet Content Platform Engine to build the information architecture for automation projects realized with Cloud Pak for Business Automation.</p> <p>Warning</p> <p>\u2139\ufe0f [SKO UPDATE] The TechJam labs were designed for many users on one cluster and hence used prefixed names using usrxxx (ex: usrxxxClientName) for content objects. For our event, please ignore any references to usrxxx (for names, symbolic links, etc.).   For example, if you see usrxxxClientName, please use ClientName</p> <p>\u2139\ufe0f [SKO UPDATE] We will NOT use the CLOS object store and instead use the existing CONTENT object store. Expand the section below to further extend the object store to support the labs below.   </p> Expand to view <ol> <li> <p>Log into the FileNet Administrative Console for Content Engine (ACCE) as cp4admin and select the object store named CONTENT.    Note: the entry in the access-info file will be named: Content Platform Engine administration </p> </li> <li> <p>Navigate to Data Design -&gt; Property Templates. Create the two property templates:</p> Name Symbolic Name Type Case Reference ID Case_Reference_ID String Client Name Client_Name String </li> <li> <p>Navigate to Data Design -&gt; Classes. Create a new subclass of the Folder class with the name SWAT Jam Case Folder (Symbolic ID SWAT_JAM_Case_Folder).  </p> </li> <li> <p>Open the Folder class and add the two new property templates from above. Make sure you Save the modified folder subclass.  </p> </li> <li> <p>Create the indicated folder structure under the Root folder. The Case Folders folder is a regular folder using the Folder object class.  The Sample Test Folder TEST1 should be created using the SWAT Jam Case Folder class and have the Case Reference ID and Client Name properties set to TEST1.  The Sample Test Folder TEST2 should be created using the SWAT Jam Case Folder class and have the Case Reference ID and Client Name properties set to TEST2. </p> </li> <li> <p>Open Content Navigator, select the existing content desktop and update the Layout tab and enable the Simple Search feature (if it is not already enabled).</p> </li> </ol>"},{"location":"client-onboarding/co-rebuild/#labs_2","title":"Labs","text":"<ul> <li> <p>Setting up FileNet Content Manager for Automation Projects on Cloud Pak for Business Automation  (View) </p> <ul> <li>\u2139\ufe0f [SKO UPDATE] Please skip Exercise 4 - Storage and ensure you read the info in the Overview section above. In this lab, you will create a small hierarchy of Document classes to capture different kinds of documents together with custom metadata, and will learn about the most important security concepts. You will explore Document storage and learn to migrate documents between different Storage Areas.  Further you will determine how to trigger custom actions for example when new documents have arrived, and how to configure and test functionality of content based retrieval, i.e. searches for documents based on information contained in the documents themselves, not (only) on their metadata.  On this lab, for triggering the custom actions, a custom javascript is used to file a newly uploaded document into a folder, those identity is derived from a search. The script is available in the Lab Data folder. Approximate Duration: 4 - 5 hours </li> </ul> </li> <li> <p>Interfacing FileNet Content Platform Engine with GraphQL on Cloud Pak for Business Automation  (View) The second lab on GraphQL builds on top of the first one, as the searches performed using GraphQL use the documents and document classes defined in the first lab. Here you learn by a series of example, how to build the most important queries using GraphQL. The examples also show how to download documents using GraphQL, how to create folders, and modify security settings. Approximate Duration: 1.5 - 2 hours </p> </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild/#4-ibm-automation-decision-services","title":"4. IBM Automation Decision Services","text":"Expand to view"},{"location":"client-onboarding/co-rebuild/#overview_4","title":"Overview","text":"<p>Part of the IBM Cloud Pak\u00ae for Business Automation platform, IBM Automation Decision Services provides a comprehensive environment for authoring, managing, and running decision services. Business experts can infuse intelligence into business decisions by combining decision models and predictive models into decision services. Business decisions can be published as Automation Services and easily consumed from other capability of the platform such as Workflow.</p>"},{"location":"client-onboarding/co-rebuild/#labs_3","title":"Labs","text":"<ul> <li>Manage Decisions and infuse Machine Learning  (View) This Lab  introduces you to the key concepts of IBM Business Automation Decision Services. It includes three exercises that can be done separately. In this Lab you learn how to model business decisions, infuse intelligence by adding a predictive model, share and publish decision services. Approximate Duration: 3 hours </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild/#5-ibm-robotic-process-automation","title":"5. IBM Robotic Process Automation","text":"Expand to view"},{"location":"client-onboarding/co-rebuild/#overview_5","title":"Overview","text":"<p>Going from simple, back-office task automation to scaled  automation to handle time-consuming business processes can be a  challenge. The IBM\u00ae Robotic Process Automation offering helps you automate more business and IT processes at scale with the ease and speed of traditional RPA. Software robots, or bots, can act on AI insights to  complete tasks with no lag time and enable you to achieve digital transformation.            </p>"},{"location":"client-onboarding/co-rebuild/#labs_4","title":"Labs","text":"<ul> <li> <p>Application Automation using IBM RPA  (View) In this lab you will learn how to use IBM RPA Studio to develop a bot to automate a Java swing application and a web application. </p> </li> <li> <p>IBM RPA and Workflow Integration  (View) In this lab you will learn how the bot designed in RPA Studio can easily be integrated into a business process developed with the Workflow capability in IBM Cloud Pak for Business Automation.</p> </li> <li> <p>Chatbot Development and Configuration  (View) In this lab you will learn how to develop a chatbot script using IBM RPA Studio, and then shows how to configure the chat mapping through web client.  The labs requires an IBM RPA client environment which can be reserved here. Once you have reserved an environment, you will receive a link to the environment via email. Start the environment by selecting the start button in the upper right corner of the page. Please use your license ID/password to activate RPA client as described in the lab instructions. </p> </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-rebuild/#6-business-automation-workflow","title":"6. Business Automation Workflow","text":"Expand to view <ol> <li> <p>The Client Onboarding assets have been adapted from the IBM TechJam 21.0.3 materials as developed by the IBM SWAT Team\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"client-onboarding/co-rebuild/#overview_6","title":"Overview","text":"<p>IBM Business Automation Workflow is software that combines business process management and case management  capabilities in a single integrated workflow solution. It unites information process, and users to provide a 360-degree view of work to help drive more successful business outcomes.</p> <p>Warning</p> <p>\u2139\ufe0f [SKO UPDATE] The TechJam labs were designed for many users on one cluster and hence used prefixed names using UsrNNN (ex: UsrNNN Client Onboarding) for naming case types.  For our event, please ignore any references to UsrNNN (for names, symbolic links, etc.).  For example, if you see UsrNNN Client Onboarding, please use Client Onboarding</p>"},{"location":"client-onboarding/co-rebuild/#labs_5","title":"Labs","text":"<ul> <li>Introduction to IBM Business Automation Workflow  (View) This lab showcases how you can create a sample Workflow project using Case features. It covers Case &amp; Process integration and building UIs using Coaches.  Approximate Duration: 3-4 hours </li> <li>Consume &amp; Publish Automation Services in Workflow  (View) This lab showcases how you can consume capabilities from the IBM Automation platform using Automation Services. You will also create an external service to invoke a Java app that sends out emails. Finally, you will learn how an automation service can be published for others to consume. Approximate Duration: 2 hours </li> </ul> <p>Go to top of section | Go to top of page</p>"},{"location":"client-onboarding/co-team-demonstration/","title":"Demonstration","text":""},{"location":"client-onboarding/co-team-demonstration/#demo-rounds","title":"Demo Rounds","text":"<ol> <li>Four teams, each with 3-4 members</li> <li>Deliver a business demo [15 minutes]</li> <li>Receive feedback and score [5 minutes]</li> <li>Short 10 min break between teams and repeat</li> </ol>"},{"location":"client-onboarding/co-team-demonstration/#top-teams-win-bluepoints","title":"Top Teams Win BLUEPOINTS!","text":"<p>Bluepoints will be given to the top 2 teams (1800 total)</p>"},{"location":"client-onboarding/co-team-demonstration/#stand-and-deliver-guidelines","title":"Stand and Deliver Guidelines","text":"<p>As reminder, please ensure you review the stand and deliver guidelines:     - Stand &amp; Deliver Guidelines</p> <p> </p>"},{"location":"client-onboarding/co-team-presentation/","title":"Co team presentation","text":""},{"location":"client-onboarding/co-team-presentation/#instructions","title":"Instructions","text":""},{"location":"client-onboarding/labs/","title":"IBM Cloud Pak for Business Automation Labs","text":"<p>These labs assume that you have installed IBM Cloud Pak for Business Automation v21.0.3 on a ROKS cluster as described here and imported the Client Onboarding scenario as described here.</p> <p>The table below lists all capabilties that we currently have labs for. A capability may contain one or more labs depending on the complexity.</p> Capability Approximate Duration IBM Cloud Pak for Business Automation (End-to-End) 2 hours IBM CP4BA - Bring-up Lab 12 hours IBM Business Automation Application 2-3 hours IBM Business Automation Insights 1 hour IBM FileNet Content Manager 2-4 hours IBM Automation Decision Services 3 hours IBM Process Mining 3 hours IBM Robotic Process Automation 3-4 hours IBM Business Automation Workflow 5-6 hours"},{"location":"client-onboarding/labs/Bring-up/","title":"IBM CP4BA - Bring-up Lab","text":""},{"location":"client-onboarding/labs/Bring-up/#overview","title":"Overview","text":"<p>Install and configure a Production deployment of IBM Cloud Pak for Business Automation (CP4BA, version 21.0.3 IF005) on an OpenShift cluster on IBM Cloud using the CP4BA rapid deployment scripts.</p>"},{"location":"client-onboarding/labs/Bring-up/#labs","title":"Labs","text":"<p>Track 3 - Administrator Role</p> <ul> <li>Bring-up Lab: Learn how to quickly bring-up a Production deployment of IBM CP4BA.</li> </ul> <p>Note: Lab as documented here only can be compleded in the context of the Americas March 2022 Tech Jam. If you like to bring-up your own CP4BA instance independently of a Tech Jam, simply follow the CP4BA rapid deployment project</p> <p>Approximate Duration: 12 hours</p>"},{"location":"client-onboarding/labs/Business%20Automation%20Application/","title":"IBM Business Automation Application","text":""},{"location":"client-onboarding/labs/Business%20Automation%20Application/#overview-2","title":"Overview 2","text":"<p>This is a low-code capability of IBM Cloud Pak for Business Automation that let's business users leverage services built by developers in other parts of the platform such as Workflow, Decisions &amp; Content.</p>"},{"location":"client-onboarding/labs/Business%20Automation%20Application/#labs","title":"Labs","text":"<p>Track 2 - Developer Role / Solution Implementation</p> <ul> <li>Introduction to IBM Business Automation Application: This lab introduces you to the key concepts of IBM Business Automation Application including Application Designer. In this you will learn how to create toolkits, templates and applications that integrate with the Worklow, Decisions &amp; Content capabilitities of the CP4BA platform.</li> </ul> <p>Approximate Duration: 2 Hours</p>"},{"location":"client-onboarding/labs/Business%20Automation%20Insights/","title":"IBM Business Automation Insights","text":""},{"location":"client-onboarding/labs/Business%20Automation%20Insights/#overview","title":"Overview","text":"<p>IBM Business Automation Insights (BAI) is a cloud capability to capture end to end business data (events) from Cloud Pak for Automation (CP4A) platform components to operational data store and long-term store (data lake).  BAI provides real-time operational visibility to Business Managers via custom or pre-built set of dashboards. Custom dashboards can be built by IT (using Kibana) or business users (using Business Performance Center). The data collected by BAI and stored in the data lake can be used to inject AI into CP4A platform, for example it can be used to make recommendations to business managers and knowledge workers. Business Performance Center is a no-code monitoring application native to IBM Business Automation Insights. You can design and share dashboards in minutes that capture business data in near real time and provide real time awareness of important business activities and processes.</p>"},{"location":"client-onboarding/labs/Business%20Automation%20Insights/#labs","title":"Labs","text":"<p>Track 2 - Developer Role / Solution Implementation</p> <p>Build Business Performance Center Dashboards: In the lab, you will learn how to build and use Business Performance Center dashboards to provide insights into a Client Onboarding workflow solution for line of business users.</p> <p>Approximate Duration: 1 Hour</p>"},{"location":"client-onboarding/labs/Content/","title":"IBM FileNet Content Manager","text":""},{"location":"client-onboarding/labs/Content/#overview","title":"Overview","text":"<p>IBM FileNet Content Manager is a flexible, full-featured content management solution that provides the foundation for IBM Cloud Pak\u00ae for Business Automation. In labs you will get introduced to important core concepts of FileNet Content Platform Engine and Content Services GraphQL which will enable you to use FileNet Content Platform Engine to build the information architecture for automation projects realized with Cloud Pak for Business Automation. </p>"},{"location":"client-onboarding/labs/Content/#labs","title":"Labs","text":"<p>Track 2 - Developer Role / Solution Implementation</p> <ul> <li>Setting up FileNet Content Manager for Automation Projects on Cloud Pak for Business Automation:   In this lab, you will create a small hierarchy of Document classes to   capture different kinds of documents together with custom metadata,   and will learn about the most important security concepts. You will   explore Document storage and learn to migrate documents between   different Storage Areas.  Further you will determine how to trigger   custom actions for example when new documents have arrived, and how to   configure and test functionality of content based retrieval,   i.e. searches for documents based on information contained in the   documents themselves, not (only) on their metadata.</li> </ul> <p>On this lab, for triggering the custom actions, a custom javascript is   used to file a newly uploaded document into a folder, those identity   is derived from a search. The script is available in the Lab   Data folder. Be aware that in the script, some   replacements need to be made, to make it refer to the right   properties. The username prefix is denoted by usrxxx in the script,   and the xxx part needs to be updated.</p> <p>Approximate Duration: 4 - 5 hours</p> <ul> <li>Interfacing FileNet Content Platform Engine with GraphQL on Cloud Pak for Business Automation:   The second lab on GraphQL builds on top of the first one, as the   searches performed using GraphQL use the documents and document   classes defined in the first lab.  Here you learn by a series of   example, how to build the most important queries using GraphQL.  The   examples also show how to download documents using GraphQL, how to   create folders, and modify security settings.</li> </ul> <p>Approximate Duration: 1.5 - 2 hours</p>"},{"location":"client-onboarding/labs/Decisions/","title":"IBM Automation Decision Services","text":""},{"location":"client-onboarding/labs/Decisions/#overview","title":"Overview","text":"<p>Part of the IBM Cloud Pak\u00ae for Business Automation platform, IBM Automation Decision Services provides a comprehensive environment for authoring, managing, and running decision services. Business experts can infuse intelligence into business decisions by combining decision models and predictive models into decision services. Business decisions can be published as Automation Services and easily consumed from other capability of the platform such as Workflow.</p>"},{"location":"client-onboarding/labs/Decisions/#labs","title":"Labs","text":"<p>Track 2 - Developer Role / Solution Implementation</p> <ul> <li>Manage Decisions and infuse Machine Learning: This Lab  introduces you to the key concepts of IBM Business Automation Decision Services. It includes 3 exercises that can be done separately. The overall duration of the Lab is 3 hours. In this Lab you learn how to model business decisions, infuse intelligence by adding a predictive model, share and publish decision services.  </li> </ul> <p>Approximate Duration: 3 hours</p>"},{"location":"client-onboarding/labs/IBM%20Cloud%20Pak%20for%20Business%20Automation%20%28End-to-End%29/","title":"IBM Cloud Pak for Business Automation","text":""},{"location":"client-onboarding/labs/IBM%20Cloud%20Pak%20for%20Business%20Automation%20%28End-to-End%29/#overview","title":"Overview","text":"<p>IBM Cloud Pak\u00ae for Business Automation offers design, build, run, and automation services to rapidly scale your programs and fully execute and operationalize an automation strategy.</p>"},{"location":"client-onboarding/labs/IBM%20Cloud%20Pak%20for%20Business%20Automation%20%28End-to-End%29/#labs","title":"Labs","text":"<p>Track 1 - Foundation</p> <ul> <li>IBM Cloud Pak for Business Automation: An End-to-End Demonstration: This lab showcases the art of the possible for the IBM Cloud Pak for Business Automation platform. You will assume the role of an end user to step through an integrated Client Onboarding scenario,</li> </ul> <p>Approximate Duration: 2 hours</p>"},{"location":"client-onboarding/labs/Process%20Mining/","title":"Process Mining","text":""},{"location":"client-onboarding/labs/Process%20Mining/#overview","title":"Overview","text":"<p>Process mining is a family of techniques in the field of process management that support the analysis of real business processes based on event logs.  During process mining, specialized data mining algorithms are applied in order to identify trends, patterns, and details contained in event logs recorded by an information system. Process mining aims to improve process efficiency and understanding of processes.</p>"},{"location":"client-onboarding/labs/Process%20Mining/#labs","title":"Labs","text":"<p>Track 2 - Developer Role / Solution Implementation</p> <ul> <li>Use Process Mining to Create and Explore Process Models Approximate Duration: 30 Mintues</li> <li>Use Process Mining to Improve Procure to Pay Process Approximate Duration: 1 Hour</li> <li>Using BPMN Process Diagrams from IBM Blueworks Live in IBM Process Mining Approximate Duration: 1 Hour</li> </ul>"},{"location":"client-onboarding/labs/Robotic%20Process%20Automation/","title":"IBM Robotic Process Automation","text":""},{"location":"client-onboarding/labs/Robotic%20Process%20Automation/#overview","title":"Overview","text":"<p>Going from simple, back-office task automation to scaled  automation to handle time-consuming business processes can be a  challenge. The IBM\u00ae Robotic Process Automation offering helps you automate more business and IT processes at scale with the ease and speed of traditional RPA. Software robots, or bots, can act on AI insights to  complete tasks with no lag time and enable you to achieve digital transformation.            </p>"},{"location":"client-onboarding/labs/Robotic%20Process%20Automation/#labs","title":"Labs","text":"<p>Track 2 - Developer Role / Solution Implementation</p> <ul> <li>Application Automation using IBM RPA: In this lab you will learn how to use IBM RPA Studio to develop a bot to automate a Java swing application and a web application. </li> <li>IBM RPA and Workflow Integration: In this lab you will learn how the bot designed in RPA Studio can easily be integrated into a business process developed with the Workflow capability in IBM Cloud Pak for Business Automation. </li> <li>Chatbot Development and Configuration In this lab you will learn how to develop a chatbot script using IBM RPA Studio, and then shows how to configure the chat mapping through web client.</li> </ul> <p>The labs requires an IBM RPA client environment which can be reserved here. Once you have reserved an environment, you will receive a link to the environment via email. Start the environment by selecting the start button in the upper right corner of the page. Please use your license ID/password to activate RPA client as described in the lab instructions.</p>"},{"location":"client-onboarding/labs/Workflow/","title":"IBM Business Automation Workflow","text":""},{"location":"client-onboarding/labs/Workflow/#overview","title":"Overview","text":"<p>IBM Business Automation Workflow is software that combines business process management and case management  capabilities in a single integrated workflow solution. It unites information process, and users to provide a 360-degree view of work to help drive more successful business outcomes.</p>"},{"location":"client-onboarding/labs/Workflow/#labs","title":"Labs","text":"<p>Track 2 - Developer Role / Solution Implementation</p> <ul> <li>Introduction to IBM Business Automation Workflow: This lab showcases how you can create a sample Workflow project using Case features. It covers Case &amp; Process integration and building UIs using Coaches.</li> </ul> <p>Approximate Duration: 3-4 hours</p> <ul> <li>Consume &amp; Publish Automation Services in Workflow: This lab showcases how you can consume capabilities from the IBM Automation platform using Automation Services. You will also create an external service to invoke a Java app that sends out emails. Finally, you will learn how an automation service can be published for others to consume.</li> </ul> <p>Approximate Duration: 2 hours</p>"},{"location":"contribute/","title":"Contributing to this Site","text":"<p>Info</p> <p>Anyone can contribute to IBM Cloud Architecture reference applications and their associated projects, whether you are an IBMer or not. We welcome your collaboration &amp; contributions happily, as our reference applications are meant to reflect your real world scenarios. There are multiple ways to contribute: report bugs and improvement suggestions, improve documentation, and contribute code.</p>"},{"location":"contribute/#bug-reports-documentation-changes-and-feature-requests","title":"Bug reports, documentation changes, and feature requests","text":"<p>If you would like to contribute your experience with an IBM Cloud Architecture project back to the project in the form of encountered bug reports, necessary documentation changes, or new feature requests, this can be done through the use of the repository's Issues list.</p> <p>Before opening a new issue, please reference the existing list to make sure a similar or duplicate item does not already exist.  Otherwise, please be as explicit as possible when creating the new item and be sure to include the following:</p> <ul> <li> <p>Bug reports</p> <ul> <li>Specific Project Version</li> <li>Deployment environment</li> <li>A minimal, but complete, setup of steps to recreate the problem</li> </ul> </li> <li> <p>Documentation changes</p> <ul> <li>URL to existing incorrect or incomplete documentation (either in the project's GitHub repo or external product documentation)</li> <li>Updates required to correct current inconsistency</li> <li>If possible, a link to a project fork, sample, or workflow to expose the gap in documentation.</li> </ul> </li> <li> <p>Feature requests</p> <ul> <li>Complete description of project feature request, including but not limited to, components of the existing project that are impacted, as well as additional components that may need to be created.</li> <li>A minimal, but complete, setup of steps to recreate environment necessary to identify the new feature's current gap.</li> </ul> </li> </ul> <p>The more explicit and thorough you are in opening GitHub Issues, the more efficient your interaction with the maintainers will be.  When creating the GitHub Issue for your bug report, documentation change, or feature request, be sure to add as many relevant labels as necessary (that are defined for that specific project).  These will vary by project, but will be helpful to the maintainers in quickly triaging your new GitHub issues.</p>"},{"location":"contribute/#code-contributions","title":"Code contributions","text":"<p>We really value contributions, and to maximize the impact of code contributions, we request that any contributions follow the guidelines below.  If you are new to open source contribution and would like some more pointers or guidance, you may want to check out Your First PR and First Timers Only.  These are a few projects that help on-board new contributors to the overall process.</p>"},{"location":"contribute/#coding-and-pull-requests-best-practices","title":"Coding and Pull Requests best practices","text":"<ul> <li> <p>Please ensure you follow the coding standard and code formatting used throughout the existing code base.</p> <ul> <li>This may vary project by project, but any specific diversion from normal language standards will be explicitly noted.</li> </ul> </li> <li> <p>One feature / bug fix / documentation update per pull request</p> <ul> <li>Always pull the latest changes from upstream and rebase before creating any pull request.</li> <li>New pull requests should be created against the <code>integration</code> branch of the repository, if available.</li> <li>This ensures new code is included in full-stack integration tests before being merged into the <code>master</code> branch</li> </ul> </li> <li> <p>All new features must be accompanied by associated tests.</p> <ul> <li>Make sure all tests pass locally before submitting a pull request.</li> <li>Include tests with every feature enhancement, improve tests with every bug fix</li> </ul> </li> </ul>"},{"location":"contribute/#github-and-git-flow","title":"Github and git flow","text":"<p>The internet is littered with guides and information on how to use and understand git. However, here's a compact guide that follows the suggested workflow</p> <p></p> <ol> <li> <p>Fork the desired repo in github.</p> </li> <li> <p>Clone your repo to your local computer.</p> </li> <li> <p>Add the upstream repository</p> <p>Note: Guide for step 1-3 here: forking a repo</p> </li> <li> <p>Create new development branch off the targeted upstream branch.  This will often be <code>master</code>.</p> <p><code>git checkout -b &lt;my-feature-branch&gt; master</code></p> </li> <li> <p>Do your work:</p> <ul> <li>Write your code</li> <li>Write your tests</li> <li>Pass your tests locally</li> <li>Commit your intermediate changes as you go and as appropriate</li> <li>Repeat until satisfied</li> </ul> </li> <li> <p>Fetch latest upstream changes (in case other changes had been delivered upstream while you were developing your new feature).</p> <p><code>git fetch upstream</code></p> </li> <li> <p>Rebase to the latest upstream changes, resolving any conflicts. This will 'replay' your local commits, one by one, after the changes delivered upstream while you were locally developing, letting you manually resolve any conflict.</p> <p><code>git branch --set-upstream-to=upstream/master git rebase</code> Instructions on how to manually resolve a conflict and commit the new change or skip your local replayed commit will be presented on screen by the git CLI.</p> </li> <li> <p>Push the changes to your repository</p> <p><code>git push origin &lt;my-feature-branch&gt;</code></p> </li> <li> <p>Create a pull request against the same targeted upstream branch.</p> <p>Creating a pull request</p> <p>Once the pull request has been reviewed, accepted and merged into the main github repository, you should synchronise your remote and local forked github repository <code>master</code> branch with the upstream master branch. To do so:</p> </li> <li> <p>Pull to your local forked repository the latest changes upstream (that is, the pull request).</p> <p><code>git pull upstream master</code></p> </li> <li> <p>Push those latest upstream changes pulled locally to your remote forked repository.</p> <p><code>git push origin master</code></p> </li> </ol>"},{"location":"contribute/#what-happens-next","title":"What happens next?","text":"<ul> <li> <p>All pull requests will be automatically built and unit tested by travis-ci, when implemented by that specific project.</p> <ul> <li>You can determine if a given project is enabled for travis-ci unit tests by the existence of a <code>.travis.yml</code> file in the root of the repository or branch.</li> <li>When in use, all travis-ci unit tests must pass completely before any further review or discussion takes place.</li> </ul> </li> <li> <p>The repository maintainer will then inspect the commit and, if accepted, will pull the code into the upstream branch.</p> </li> <li>Should a maintainer or reviewer ask for changes to be made to the pull request, these can be made locally and pushed to your forked repository and branch.</li> <li>Commits passing this stage will make it into the next release cycle for the given project.</li> </ul>"},{"location":"daffy/daffy/","title":"Overview","text":"<p>Note</p> <p>The Daffy setup should have been completed as part of your prework and for IBMers, this information was identified in your learning plan and also identified in the Prework - Daffy section.  If you are not an IBMer -and/or- you have not completed your Daffy Prework, please use this section to set up Daffy.</p> <p>Use the navigation menu to complete the following steps:     - Pre-requirements         - Core Steps - Access your Bastion server         - Core Steps - Install Daffy         - Core Steps - Create -env.sh </p> <p></p> <p>Steps to Cloud Pak Deployment The Daffy (Deployment Automation Framework For You) automation process is a three step process resulting in the deployment of a Cloud Pak.  You can start at step 1 or step 2 or even step 3. For example, if you do not have a cluster, start at step 1.  If you have OpenShift Cluster already, start at step 2. If you have a OpenShift cluster and already have base cloud pak installed, but just need to deploy a cloud pak service, start at step 3.</p> <p></p> Automation ProcessArchitectureDemo Video <p></p> <p></p> <p> </p> <p>Top of page | Top of section </p>"},{"location":"daffy/requirements/","title":"Pre-Requirements","text":"<p>What is required to use Daffy? Before you can use the Daffy scripts, you must have the following.</p> <ol> <li>You must have a SSH client on your local workstation.  <ul> <li>We highly recommend installing Termius as your SSH client.</li> <li>The Termius installer can be found here:  Windows or Mac  (Only the Free Version is needed)</li> </ul> </li> <li>A Bastion Machine (Create Bastion Instructions)<ul> <li>Ubuntu 20.04 (Minimum Requirements: 2 CPU, 2GB Memory) with full root access (VSphere-UPI, * -IPI and *  -MSP)</li> <li>Ubuntu 20.04 (Minimum Requirements: 60+ CPU, 128GB+ Memory) with full root access (KVM-UPI)</li> <li>RHEL 8.X         (Minimum Requirements: 2 CPU, 2GB Memory) with full root access ( -IPI and * -MSP*)</li> </ul> </li> <li> <p>Red Hat pull secret</p> <ul> <li>If you or your customer does not have a Red Hat pull secret  <ul> <li>Sign up for 60 day trail for OpenShift: Red Hat pull secret site<ul> <li>Important: If your installing on a customer owned platform account or a on-prem customer datacenter, you MUST instruct your customer to register for a trial account and use their pull secret for the install. Do not use your own pull secret for customer engagements.</li> </ul> </li> <li>Sign up for IBM / Red Hat partner program \u200b\u200b\u200b\u200b\u200b\u200b     NOTE: All IBMer's are entitled to the Red Hat partner program. Your Red Hat pull secret can ONLY be used for training and demo purposes. Do not provide your personal pull secret to customers.</li> </ul> </li> <li>If you have a Red Hat account, you can find your existing pull secret here.<ul> <li>Login to Red Hat</li> <li>Scroll down the page until you see \"Tokens\" and download the pull secret.</li> </ul> </li> <li>Accessing Red Hat entitlements from your IBM Cloud Paks: Accessing-red-hat-entitlements-from-your-cloud-paks</li> </ul> </li> <li> <p>IBM Entitlement Key</p> <ul> <li>If you need to get your own IBM entitlement key you can get here.<ul> <li>Copy to clipboard and save to a local file</li> </ul> </li> <li>If you need create one for a customer you can submit request here.</li> <li>Customer can there own 60-day trial key here</li> </ul> </li> </ol> <p>Top of page</p>"},{"location":"daffy/step1/","title":"Access your Bastion Server","text":"<p>What is a bastion server? Why does it exist? A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. Because of its exposure to potential attack, a bastion host must minimize the chances of penetration. Openshift uses a bastion to help create a running cluster. A bastion can be reused for multiple clusters. In some scenarios for POC purposes such as User Provisioned Infrastructure (UPI), the bastion can be used as the proxy to the cluster.  </p> <p>Bastion servers can be installed anywhere. This guide assumes the bastion server is Ubuntu 20.04 Minimal and will be in the IBM Cloud.  </p> <p>Requirements for completing this task is to have an IBMid, an IBM cloud account, and a local SSH key. For more information, go to Daffy Prerequisites.  </p> <p>Detailed below are the instructions to build your own bastion to do an IPI or MSP install.  </p> <p>If you do not have a bastion, select one of the options below.</p> IBM Cloud BastionIBM Technology Zone Bastion <p>1. First, open a web browser and go to http://cloud.ibm.com </p> <p>2. Enter your id and click Continue </p> <p>3. Once logged in, select Catalog in the top menu bar </p> <p>4. Once the Catalog is loaded, select Compute or search the catalog for Virtual Servers. Select Virtual Server for Classic </p> <p>Alternative: Skip step 3 and search for virtual server, choosing virtual server for classic. Both options achieve the same thing. </p> <p>5. Fill out the details. (Public, hostname can be anything, and so can domain \u2013 feel free to leave what is there originally for your domain). Choose your billing method based on needs to be either Hourly or Monthly (~$40/mo.) and choose a Location. </p> <p>6. Scroll down and fill out the remainder of the information. Choose a server type and select your SSH key so you can login. Finally, make sure you have the Ubuntu 20.04 Operating System selected. </p> <p>Note</p> <p>You can use any other available tool to create a key if needed</p> <p>7. Click the agreement on the right-hand pane and select Create </p> <p>8. This will take you to a device page. You can search for your bastion you have created. Once your server is done provisioning and has a start date, you can login to it via Termius using the Public IP address. </p> <p>Note</p> <p>If connecting to a VPN to connect to the network, you will use the Private IP address, but Public will be used more frequently.</p> <p>9. Create a new host in Termius: use your SSH key as the password, use root as the username, input the Public IP Address from the device list as your host's address, and create a label. </p> <p>Note</p> <p>If you don't use a SSH Key, you can go into the details of the bastion you created by double clicking on it and going to the passwords section. This password will not show until provisioning is complete.</p> <p>10. Once you have connected your bastion to Termius, install Daffy to the terminal of your newly created host.  </p> <p>An alternative to creating a bastion using a paid IBM Cloud account is to use IBM Technology Zone.  </p> <ol> <li>Navigate to IBM Technology Zone </li> <li> <p>Scroll down to environments, and choose IBM Virtual Server Instance (Classic) </p> <p></p> </li> <li> <p>From there, complete your reservation. Make sure to fill out items 1 \u2013 4, leaving the other fields blank.</p> <p></p> </li> </ol> <p>Go to top of section | Go to top of page</p>"},{"location":"daffy/step2/","title":"Install Daffy","text":"<p>Log into your Bastion Machine (as root) and run the following command to download the latest Daffy Scripts.</p> <pre><code>wget http://get.daffy-installer.com/download-scripts/daffy-init.sh; chmod 777 daffy-init.sh;./daffy-init.sh\n</code></pre> <p>Go to top of section | Go to top of page</p>"},{"location":"daffy/step3/","title":"Create -env.sh","text":"<p>This file is where you store values that will define your environment and that Daffy will use to build your environment.</p> <p>Go to the samples directory:</p> <pre><code>cd /data/daffy/env/samples\n</code></pre> <p>As a starting point, copy a sample environment file from the samples folder located here:  </p> <pre><code>/data/daffy/env/samples/&lt;platform&gt;-env.sh\n</code></pre> <p>For SKO Tech Academy, we will use CP4BA on the ROKS platform:</p> <pre><code>/data/daffy/env/samples/cp4ba-env.sh\n</code></pre> <p>You will copy the environment file and rename it using this format:</p> <pre><code>/data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh\n</code></pre> <p>** Best practice is  is your cluster name but not required. <p> This command will copy the the sample file and place it in the /data/daffy/env directory (back one folder)</p> <pre><code>cp cp4ba-env.sh /data/daffy/env/&lt;ENVIRONMENT_NAME&gt;-env.sh\n</code></pre> <p>Here are the values of the file:</p> <pre><code>DAFFY_UNIQUE_ID=\"&lt;YourID@ibm.com&gt;\"\n#This is required - Values POC/Demo/Enablement/HCCX/TechZone\nDAFFY_DEPLOYMENT_TYPE=\"\"\n#If POC/Demo, these are required.\n#ISC number must be 18 characters\n#DAFFY_ISC_NUMBER=\"0045h00000w1nvKAAG\"\n#DAFFY_CUSTOMER_NAME=\"Acme Customer\"\n\nBASE_DOMAIN=\"&lt;YOUR.BASEDOMAIN.COM&gt;\"\nCLUSTER_NAME=\"&lt;ENVIRONMENT_NAME&gt;\"\nOCP_RELEASE=\"4.8.36\"\nVM_TSHIRT_SIZE=\"Large\"\n</code></pre> Property Description DAFFY_DEPLOYMENT_TYPE Values POC/Demo/Enablement/HCCX/TechZone DAFFY_ISC_NUMBER If Demo or POC, the ISC Record Number DAFFY_CUSTOMER_NAME If Demo or POC, the Customer Name BASE_DOMAIN Is your DNS name your cluster will use CLUSTER_NAME The name you want to give your OpenShift Cluster OCP_RELEASE Version of OpenShift you want to Install VM_TSHIRT_SIZE Size of OpenShift Cluster, Min and Large supported today <p>Note</p> <p>If MSP type install like ROKS, BASE_DOMAIN is not needed.</p> <p></p> <p>Success</p> <p>\u2139\ufe0f \u00a0 You are NOW ready to begin making the necessary edits to your /data/daffy/env/-env.sh file for a deployment of OCP to a specific platform.    \u00a0 \u00a0 \u00a0 \u00a0    Please proceed to the Deployment Overview section. <p></p> <p>Go to top of section | Go to top of page</p>"},{"location":"daffy/step4/","title":"Step4","text":"<p>Please proceed to the Deployment Overview section.</p>"},{"location":"deploy/cluster/","title":"Cluster Build","text":"<p>To create your cluster execute the following command:</p> <pre><code>/data/daffy/ocp/build.sh &lt;your environment&gt;\n</code></pre> <p>As a concrete example, if my environment file is called : cp4ba-tech-academy-lab25-env.sh then I would execute the following command:</p> <pre><code>/data/daffy/ocp/build.sh cp4ba-tech-academy-lab25\n</code></pre> <p>Once the build has completed some pre-checks you'll be asked to provide your RedHat pull secret. You should have  registered and obtained this pull secret as part of the pre-work. Daffy will provide a link for you to get your pull  secret if you haven't copied it already :</p> <pre><code>To get your OpenShift Pull secret, you can go here:   https://console.redhat.com/openshift/downloads#tool-pull-secret\n\nMissing PULL_SECRET, Please enter here so we can save to your ~/.profile\nPULL_SECRET=\n</code></pre> <p>Login to the Red Hat website using the URL above and scroll down until you see the pull secret. Click on the highlighted  copy icon then paste the secret into the Daffy terminal.</p> <p></p> <p>Daffy will store this pull secret in the user profile.</p> <p>Next, you'll be asked to login into IBM Cloud. You'll see output like this:</p> <pre><code>Validate ROKS Settings (LOG /data/daffy/log/acme-demo/ocp/ibm-*.log )\n################################################################\nYou are not logged into ibmcloud. PLease login:\nAPI endpoint: https://cloud.ibm.com\nRegion: eu-gb\n\nGet a one-time code from https://identity-2.eu-central.iam.cloud.ibm.com/identity/passcode to proceed.\nOpen the URL in the default browser? [Y/n] &gt; \n</code></pre> <p>Enter 'n' as we don't have a browser on the bastion server. Daffy is now waiting for a one time passcode that you'll obtain by logging into IBM Cloud using the URL shown.</p> <p>Copy the URL from your terminal and paste the URL into a web browser. Once you've logged into IBM Cloud you'll see the  one time passcode that's been generated for you. Copy the one time passcode by clicking on the highlighted copy icon and paste the one time code into the Daffy terminal.</p> <p></p> <p>After pasting in the one time passcode Daffy should confirm your successful authentication:</p> <pre><code>Get a one-time code from https://identity-2.eu-central.iam.cloud.ibm.com/identity/passcode to proceed.\nOpen the URL in the default browser? [Y/n] &gt; n\nOne-time code &gt; \nAuthenticating...\nOK\n</code></pre> <p>Very soon you'll be asked to confirm which IBM Cloud account you wish to use. Here's an example :</p> <pre><code>Select an account:\n1. GERALD BAIRD's Account (b158d441aaee226a9e57ece0e2764ffa) &lt;-&gt; 1896853\n2. IBM Cloud UKI (4a88eb681ef4e9f4494c356954d2d25b)\n3. IBM (71d0fea63129b7a147a3c207c67efbb2) &lt;-&gt; 1503769\n4. Daniel Crow's Account (f5455df33587c3c80903085975de4a98) &lt;-&gt; 1528385\n5. ITZ - SQUAD (5ac779bd279b4301adc82d45bea4fd85) &lt;-&gt; 2071092\n6. ITZ - V2 (ead8711ba2cc4d08a16fd37427f4f01a) &lt;-&gt; 2112072\n7. itztsglenablement01 (e1c69b1f9c6a412b8c73b3b8e78aaf10) &lt;-&gt; 2326304\n8. Kyle Dawson - IBM Account (1df0f17a7f0213c2c6ee94b97e3712b7) &lt;-&gt; 1582481\nEnter a number&gt; 7\nTargeted account itztsglenablement01 (e1c69b1f9c6a412b8c73b3b8e78aaf10) &lt;-&gt; 2326304\n</code></pre> <p>Please use your own account if you have infrastructure priviledges, if not make sure you select itztsglenablement01,  in the example above this was listed as account (7).</p> <p>Daffy will now create the Openshift cluster, you see output like this :</p> <pre><code>Create ROKS Cluster\n################################################################\nCreation of ROKS cluster is dependent on the IBM Provider, this typical takes 30-45 minutes.\nibmcloud oc cluster create classic --name acme-demo --version 4.8_openshift --zone lon06 --flavor b3c.16x64 --hardware shared --workers 7   --public-vlan 3226246 --private-vlan 3226248 --entitlement cloud_pak\nCreating cluster...\nOK\nCluster created with ID ca35fmfl0ujedhu1f0rg\nWaiting for cluster to be ready                                                                                  </code></pre> <p>After about 30-40 mins cluster creation will complete and you'll see output like this:</p> <pre><code>Creating cluster...\nOK\nCluster created with ID ca35fmfl0ujedhu1f0rg\n\nCOMPLETE  ROKS Cluster ready\n\nHere is the login info you can use for all services and console:   ##########################################################################################################\nCurrent User          :      system:admin\nOpenShift Web Console :      https://c109-e.eu-gb.containers.cloud.ibm.com:30563\nOC Commandline        :      export KUBECONFIG=/var/ibm-ocp/acme-demo/kubeconfig\nOC Login command      :      oc login https://api.acme-demo.:6443 -u  -p  --insecure-skip-tls-verify\nOC Client Download    :      https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.8.36\nInstall Temp Files    :      /data/daffy/tmp/acme-demo/ocp\nopenshift-install Dir :      /data/daffy/tmp/acme-demo/ocp/ocp-install\n</code></pre> <p>Take note of the OpenShift Web Console URL as you will need this in the next step. The URL for your OpenShift Web Console is displayed, if this isn't working it will be due to a delay creating the ingress networking, try again in a few minutes. </p> <p>The Openshift cluster has now been created but nothing has been installed. In the next step we'll install the CP4BA  Operators. Operators automate the creation, configuration, and management of instances of Kubernetes-native applications. Please proceed to Operators</p>"},{"location":"deploy/daffy-cp4ba/","title":"Daffy - CP4BA Deployment","text":"<p>As part of the pre-work you created a bastion server, installed Daffy and created a daffy environment file. Before we create the cluster please update your daffy environment by running the install command again :</p> <pre><code>wget http://get.daffy-installer.com/download-scripts/daffy-init.sh; chmod 777 daffy-init.sh;./daffy-init.sh\n</code></pre> <p>This environment file needs to be updated before Daffy can deploy the CP4BA cluster. Login to your bastion server and update the environment file you created using the sample below as a template.</p> <p>The values that you will update are:     - DAFFY_UNIQUE_ID     - CLUSTER_NAME     - ROKS_ZONE </p> <p>Here is a sample environment file:</p> <pre><code>#No spaces and only AlphaNumeric and  @ or . or -\nDAFFY_UNIQUE_ID=\"PLEASE-SET-TO-EMAIL-ADDRESS\"\n\n#OpenShift Cluster info\n#####################################\nCLUSTER_NAME=\"PLEASE-SET-CLUSTER-NAME\"\nOCP_INSTALL_TYPE=\"roks-msp\"\nDAFFY_DEPLOYMENT_TYPE=Enablement\n\n###### This only works in the itztsglenablement01, this is the account used for Tech Academy\nIBMCLOUD_RESOURCE_GROUP=TechAcademyBA\n\n# Version must match support version of ROKS only\nOCP_RELEASE=4.8.42\n\n# Supported VM_TSHIRT_SIZE values: Large, Min\n#    Large - 6 Worker Nodes\n#    Min   - 3 Worker Nodes\nVM_TSHIRT_SIZE=\"Large\"\nVM_NUMBER_OF_WORKERS_LARGE=7\n\nCP4BA_VERSION=\"21.0.3\"\nCP4BA_IFIX=IF008\nCP4BA_DEPLOYMENT_STARTER_SERVICE=samples\nCP4BA_DEPLOYMENT_STARTER_SERVICE_SAMPLE=roks-starter-all-IF008\nCP4BA_ENABLE_SERVICE_OPS=true\n\n#Overrides\n#ROKS Platform\n# Zone that ROKS will be installed\n# https://cloud.ibm.com/docs/containers?topic=containers-regions-and-zones\nROKS_ZONE=\"PLEASE-SET-TO-DATA-CENTER\"\n\n#Storage\n#####################################\n#OCP_CREATE_NFS_STORAGE=true\n#CP4BA_AUTO_STORAGE_CLASS_FAST_ROKS=managed-nfs-storage\n#CP4BA_AUTO_STORAGE_CLASS_OCP=managed-nfs-storage\n#CP4BA_AUTO_STORAGE_CLASS_OCP_BLOCK=managed-nfs-storage\n#CP4BA_AUTO_STORAGE_CLASS_OCP_SLOW=managed-nfs-storage\n#CP4BA_AUTO_STORAGE_CLASS_OCP_MEDIUM=managed-nfs-storage\n#CP4BA_AUTO_STORAGE_CLASS_OCP_FAST=managed-nfs-storage\n</code></pre> <p>Copy the contents of this file into the Daffy environment file on your bastion server and update the three placeholders for DAFFY_UNIQUE_ID, CLUSTER_NAME, ROKS_ZONE.  </p> <p>Warning</p> <p>Based on your geograhic preference, please select a data center (DC) from either the Americas, EMEA, or APAC column. There is an upper limit for the number of storage volumes we can create in each DC so we must try to distribute cluster creation across several DC's. You won't be using these clusters for the labs so their location isn't important.</p> Americas EMEA APAC dal13 dal13 sjc03 wdc04 sjc03 ams03 sjc03 ams03 fra02 sjc04 fra02 fra04 lon02 fra04 wdc04 lon06 ams03 ----- fra02 fra05 lon06 fra04 sjc04 dal13 ams03 wdc04 ----- <p>Note</p> <p>If you have your own IBM Cloud account with infrastructure please use this account when Daffy asks you to log in.  Please note that if you use your own account, you may see a FAIL message regarding resource group (ie. Default or TechAcademyBA), you can ignore this message.  </p> <p>Please name your clusters <code>daffy-yourfirstname-yourlastname</code> eg. <code>daffy-tom-cruise</code>. This is so we can find them easily.</p> <p>Daffy will deploy CP4BA and other CloudPaks using a CR yaml file (also known as a Custom Resource file) that specifies exactly what to install. These yaml CR are not part of Daffy itself, they can be customized and will evolve. The CR that installs the CP4BA in this exercise is located here :</p> <pre><code>/data/daffy/cp4ba/templates/services/samples/roks-starter-all-IF008.yaml\n</code></pre> <p>The CP4BA environment created from this CR will contain the major components of CP4BA, look in the yaml file to see exactly what is installed. The CR we'll be using later in the Tech Academy is very similar to this sample, the only difference is made to enable communication to the Tech Zone RPA environment (these changes will be described in the FAQ for those interested).</p> <p>Note</p> <p>If you have previously run a Daffy deployment from your bastion, please run the cleanup script <code>/data/daffy/cleanup.sh</code> before proceeding.</p> <p>Once you have updated your environment file, save it and exit the editor. Proceed to the next step Cluster Build</p>"},{"location":"deploy/monitor/","title":"Monitor / Credentials","text":"<p>To monitor the creation of your services execute the following command.</p> <pre><code>/data/daffy/cp4ba/service.sh &lt;your environment&gt; --StarterStatus\n</code></pre> <p>As a concrete example, if my environment file is called : acme-demo-env.sh then I would execute the following command:</p> <pre><code>/data/daffy/cp4ba/service.sh acme-demo --StarterStatus\n</code></pre> <p>Example Output : </p> <pre><code>CP4BA Service Status\n################################################################\nDaffy Version                            :  v2022-05-BETA\nBastion OS                               :  ubuntu - 20.04\nPlatform Install Type                    :  roks-msp\nOpenShift Version                        :  4.8.36\nCP4BA Version                            :  21.0.3 IF008\nProject/Namespace                        :  cp4ba-starter\nZen Version                              :  4.4.3 \nMessage 1                                :  Successful\nMessage 2                                :  Running reconciliation\nStatus Dump                              :  /data/daffy/log/cp4ba-acme-demo/cp4ba/icp4adeploy-cp4ba-status-info.yaml\n\nCP4BA Service Status - Content\n################################################################\ncpeDeployment                            :  Ready\ncpeJDBCDriver                            :  Ready\ncpeRoute                                 :  Ready\ncpeService                               :  Ready\ncpeStorage                               :  Ready\ncpeZenInegration                         :  Ready\ngraphqlDeployment                        :  Ready\ngraphqlRoute                             :  Ready\ngraphqlService                           :  Ready\ngraphqlStorage                           :  Ready\nnavigatorDeployment                      :  Ready\nnavigatorService                         :  Ready\nnavigatorStorage                         :  Ready\nnavigatorZenInegration                   :  Ready\n</code></pre> <p>To get the credentials for your CP4BA services execute the following command :</p> <pre><code>/data/daffy/cp4ba/service.sh &lt;your environment&gt; --StarterConsole\n</code></pre> <p>Here is a concrete example:</p> <pre><code>/data/daffy/cp4ba/service.sh acme-demo --StarterConsole\n</code></pre> <p>Along with sample output:</p> <pre><code>Decision Console\n################################################################\nUsername                                   : cp4admin\nPassword                                   : BQ**************e\nDecision Center                            : https://cpd-cp4ba-starter.cp4ba-acme-demo-64b8809ea4bdf3ac103ec2bdb80f1d21-0000.eu-gb.containers.appdomain.cloud/odm/decisioncenter\nDecision Runner                            : https://cpd-cp4ba-starter.cp4ba-acme-demo-64b8809ea4bdf3ac103ec2bdb80f1d21-0000.eu-gb.containers.appdomain.cloud/odm/DecisionRunner\nDecision Server Console                    : https://cpd-cp4ba-starter.cp4ba-acme-demo-64b8809ea4bdf3ac103ec2bdb80f1d21-0000.eu-gb.containers.appdomain.cloud/odm/res\nDecision Server Runtime                    : https://cpd-cp4ba-starter.cp4ba-acme-demo-64b8809ea4bdf3ac103ec2bdb80f1d21-0000.eu-gb.containers.appdomain.cloud/odm/DecisionService\n</code></pre>"},{"location":"deploy/operators/","title":"Operators","text":"<p>Operators automate the creation, configuration, and management of instances of Kubernetes-native applications.To install  the CP4BA Operator execute the following command in Daffy:</p> <pre><code>/data/daffy/cp4ba/build.sh &lt;your environment&gt;\n</code></pre> <p>Take care, this build command is in a different directory to the previous ocp build command.</p> <p>As a concrete example, if my environment file is called : cp4ba-tech-academy-lab25-env.sh then I would execute the following command:</p> <pre><code>/data/daffy/cp4ba/build.sh cp4ba-tech-academy-lab25\n</code></pre> <p>Daffy will need your IBM entitlement key to access the CP4BA software in the IBM Container Registry. You should have  obtained this entitlement key as part of the pre-work but you can get it here IBM Key</p> <p></p> <p>Paste this entitlement key ino the Daffy terminal when prompted.</p> <pre><code>Missing IBM Entitlement Key, to get your key, go here with your browser:\nhttps://myibm.ibm.com/products-services/containerlibrary\nPlease enter here so we can save to your ~/.profile\nIBM_ENTITLEMENT_KEY=eyJ0eXAiOiJILOCCdYpdv1hvXk...etc etc etc....\n</code></pre> <p>Daffy will store this key and restart the cluster with these credentials added :</p> <pre><code>Updating PullSecret to add IBM Entitlement Key\n################################################################\nChecking to see if IBM Entitlement Key exists in cluster\nAdding Your IBM Token to the existing Pull Secret\nsecret/pull-secret data updated\n\nRestart ROKS Nodes\n################################################################\nibmcloud oc worker reload -q  -f -c cp4ba-acme-demo  -w kube-ca3ohi4l08bq4l61f0v0-cp4baacmede-default-000001ed  -w kube-ca3ohi4l08bq4l61f0v0-cp4baacmede-default-00000236  -w kube-ca3ohi4l08bq4l61f0v0-cp4baacmede-default-000003d2  -w kube-ca3ohi4l08bq4l61f0v0-cp4baacmede-default-00000445  -w kube-ca3ohi4l08bq4l61f0v0-cp4baacmede-default-000005d9  -w kube-ca3ohi4l08bq4l61f0v0-cp4baacmede-default-00000610  -w kube-ca3ohi4l08bq4l61f0v0-cp4baacmede-default-000007e4 Waiting for ROKS nodes to be ready -  6 Min(s) so far                                                            </code></pre> <p>Note how Daffy is waiting for the ROKS nodes to be ready. This is normal when running these commands immediately. It can take 15-20 mins for this step to complete.</p> <p>Next, Daffy will need to login to your new cluster and run some commands to install the Operators. You'll see a request from the Daffy terminal to login to your cluster console and copy/paste the oc admin login command:</p> <pre><code>Cluster Admin setup (LOG -&gt; /data/daffy/log/cp4ba-acme-demo/cp4ba/cp4a-clusteradmin-setup.log)\n################################################################\nwget https://github.com/IBM/cloud-pak/raw/master/repo/case/ibm-cp-automation-3.2.8.tgz\nPlease login to your cluster console and copy/paste the oc admin login command below:\nhttps://c109-e.eu-gb.containers.cloud.ibm.com:32245\n</code></pre> <p>To get the oc login command you first need to login to the OpenShift web console. The URL for this was displayed when the cluster was created. Look back in your Daffy output and look for a block of output like the example below: </p> <pre><code>Creating cluster...\nOK\nCluster created with ID ca35fmfl0ujedhu1f0rg\n\nCOMPLETE  ROKS Cluster ready\n\nHere is the login info you can use for all services and console:   ##########################################################################################################\nCurrent User          :      system:admin\nOpenShift Web Console :      https://c109-e.eu-gb.containers.cloud.ibm.com:30563\nOC Commandline        :      export KUBECONFIG=/var/ibm-ocp/acme-demo/kubeconfig\nOC Login command      :      oc login https://api.acme-demo.:6443 -u  -p  --insecure-skip-tls-verify\nOC Client Download    :      https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.8.36\nInstall Temp Files    :      /data/daffy/tmp/acme-demo/ocp\nopenshift-install Dir :      /data/daffy/tmp/acme-demo/ocp/ocp-install\n</code></pre> <p>Copy the URL for the OpenShift Web Console into a browser and open it (you will need to login to IBM Cloud with your IBM ID). In the top right corner there is a drop-down, with  your username as a label. Click the label and a Copy Login Command link will be shown, this is highlighted in red below. (If the OpenShift Web Console is not loading on a newly created cluster, you'll just need to wait a few minutes the  application to be deployed and the necessary networking to finish provisioning).</p> <p></p> <p>The next screen will display a link \"Display Token\". Click on this link and the page below will be displayed. Copy the  oc login command as highlighted in red.</p> <p></p> <p>Paste this command into the Daffy terminal and Daffy will proceed with the operator install:</p> <pre><code>Entitlement Registry key is valid.\nsecret/admin.registrykey created\nsecret/ibm-entitlement-key created\nApplying the persistent volumes for the Cloud Pak operator by using the storage classname: ibmc-file-gold-gid...\npersistentvolumeclaim/operator-shared-pvc created\npersistentvolumeclaim/cp4a-shared-log-pvc created\ncatalogsource.operators.coreos.com/ibm-cp4a-operator-catalog created\ncatalogsource.operators.coreos.com/ibm-cp-automation-foundation-catalog created\ncatalogsource.operators.coreos.com/ibm-automation-foundation-core-catalog created\ncatalogsource.operators.coreos.com/opencloud-operators created\ncatalogsource.operators.coreos.com/ibm-db2uoperator-catalog created\ncatalogsource.operators.coreos.com/bts-operator created\ncatalogsource.operators.coreos.com/cloud-native-postgresql-catalog created\nIBM Operator Catalog source created!\noperatorgroup.operators.coreos.com/ibm-cp4a-operator-catalog-group created\nCP4BA Operator Group Created!\nsubscription.operators.coreos.com/ibm-cp4a-operator created\nCP4BA Operator Subscription Created!\nAdding the user IAM#gerry.baird@uk.ibm.com to the ibm-cp4a-operator role...Done!\nApplying no_root_squash for demo DB2 deployment on ROKS using CLI\nNew project cp4ba-starter has been setup for CP4BA in your cluster\n\nserviceaccount/ibm-cp4ba-anyuid created\nclusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \"ibm-cp4ba-anyuid\"\n\nCopy JDBC Files to ibm-cp4a-operator Pod\n################################################################\n\nSuccessfully copied JDBC files -&gt; /data/daffy/db2/jdbc cp4ba-starter/ibm-cp4a-operator-7d968fd78b-n48fk:/opt/ansible/share\n\n##########################################################################################################\nEnd Time: Fri May 20 14:39:26 UTC 2022\nCP4BA Build Completed in 1 hour(s), 27 minute(s) and 32 second(s)\n##########################################################################################################\n</code></pre> <p>To confirm the operators are installed return to the OpenShift Web Console and select Operators from the left side menu, then installed operators. You should see the various IBM Automation Foundation operators and the IBM Cloud Pak for  Business Automation.</p> <p></p> <p>At this stage the operators are ready to install the CP4BA components. This will be covered in the next step.</p>"},{"location":"deploy/overview/","title":"Deployment Overview","text":"<p>Daffy (Deployment Automation Framework For You) makes it easy to create pre-configured CP4BA clusters. Daffy takes care of creating the OpenShift cluster and installing the CP4BA software. Once the installation is complete you'll have a new CP4BA cluster with the following components installed:</p> Component IBM Cloud Platform Foundation IBM Automation Decision Services IBM Automation Document Processing IBM Business Automation Application IBM Business Automation Workflow IBM Automation Workstream Services IBM FileNet Content Manager IBM Operational Decision Manager <p>The Daffy installation is driven by the environment file that tells Daffy what type of infrastructure is being used and what to deploy. During this Tech Academy we'll be using Daffy to install a cp4b-starter template on OpenShift running in IBM Cloud, but you could also use Daffy to install CP4D on AWS or CP4I on Azure.</p> <p>For more details on Daffy please go to the main Daffy site on W3 Daffy on W3 (IBM Only - Daffy will be available to IBM Partners Q3 2022).</p> <p>To perform a CP4BA installation as part of Tech Academy you'll need to perform the following steps:</p> <ol> <li>Update the Daffy installation and cluster environment file</li> <li>Build the OpenShift cluster on IBM Cloud using the Daffy OCP build command</li> <li>Install the CP4BA Operators using the Daffy Cp4BA build command</li> <li>Create the CP4BA instance using the Daffy service command</li> <li>Monitor the status of the installation and display URL's and credentials</li> </ol> <p>These steps will be described in detail in the coming sections. Please proceed to Daffy Environment</p>"},{"location":"deploy/rpa/","title":"RPA Connectivity","text":"<p>To enable BAW to call the RPA server we have to make a couple of changes to the configuration of the cluster. Further  background is provided below the instructions.</p>"},{"location":"deploy/rpa/#instructions","title":"Instructions","text":"<p>Run the following command on the bastion server from your env directory :</p> <pre><code>wget https://github.com/thomasyang44/sko-tech-academy/blob/main/cp4a-rpa.pem\n</code></pre> <p>This command downloads the certificate used by the TechZone RPA server.</p> <p>Next create the OpenShift secret using the certificate (the cert is located in the file called cp4ba-rpa.pem). </p> <pre><code>oc create secret generic rpa-secret  --from-file=tls.crt=cp4a-rpa.pem -n cp4ba-starter\n</code></pre> <p>You can now proceed to the CP4BA services installation. If you'd like to know a little more about the config changes to allow BAW connectivity to the RPA server then read on.</p>"},{"location":"deploy/rpa/#background","title":"Background","text":"<p>The client onboarding solution includes an API call to an RPA bot to update backend systems. BAW does not trust  external systems that lack properly signed SSL certificates, such as our Techzone RPA server. For BAW to trust  this otherwise smelly certificate we need to add the Techzone RPA certificate to BAW's trusted certificate list and deactivate some validation checks checks. </p> <p>The main configuration of the CP4BA deployment is located in this file :</p> <pre><code>/data/daffy/cp4ba/templates/services/samples/roks-starter-nfs-all-IF008.yaml\n</code></pre> <p>On line 48 we have added a secret which contains the SSL certificate used by the Techzone RPA environment.</p> <p></p> <p>This secret needs to be created manually before the deployment of CP4BA services (see instructions above). </p>"},{"location":"deploy/services/","title":"Services","text":"<p>To create your CP4BA services execute the following command.</p> <pre><code>/data/daffy/cp4ba/service.sh &lt;your environment&gt;\n</code></pre> <p>As a concrete example, if my environment file is called : cp4ba-tech-academy-lab25-env.sh then I would execute the following command:</p> <pre><code>/data/daffy/cp4ba/service.sh cp4ba-tech-academy-lab25\n</code></pre> <p>Very soon you'll see output like this :</p> <pre><code>Validate Storage Class to be used with Cloud Pak ################################################################\nPASSED  Storage class -&gt; ibmc-file-gold-gid exist. PreCheck OPS\n################################################################\n\nAll prechecks passed, lets get to work.\n</code></pre> <p>Daffy has instructed the operators to begin installing the CP4BA components. This process will take a few hours. To  monitor the progress of the deployment open the OpenShift Web Console. Select workloads from the left hand menu, and  select the sub-menu for pods. At the top change the project to cp4ba-starter and finally order the pods by creation  date to show the newest pods at the top.</p> <p></p> <p>Various pods will be created during the deployment after several hours you may recognise pods such as the ADS runtime. At this point completion is very close.</p> <p>Alternatively you can watch the operator logs using the example command in the Daffy console : </p> <pre><code>oc logs -n cp4ba-starter --tail=100 -f deployment/ibm-cp4a-operator | grep -v proxy\n</code></pre>"},{"location":"learning-plan/learning-plan/","title":"Learning Plan including prework","text":""},{"location":"learning-plan/learning-plan/#your-learning-learning-plan","title":"Your Learning - Learning Plan","text":"Expand to view the learning plan <p>Please review the Your Learning site for the prerequisites and prework required for this event at: 2022 SKO Tech Academy - Business Automation: Prerequisite Learning, Session Prework, and Exercise</p> <p></p>"},{"location":"learning-plan/learning-plan/#prework-daffy","title":"Prework - Daffy","text":"Expand to view IBMerBusiness Partner <ul> <li> <p>Daffy Overview </p> </li> <li> <p>Daffy Prerequisites </p> </li> <li> <p>Daffy Core Steps (Steps 1-3) </p> </li> <li> <p>Daffy Core Steps - Walkthrough Video </p> </li> </ul> <ul> <li> <p>Daffy Setup</p> </li> <li> <p>Daffy Core Steps - Walkthrough Video </p> </li> </ul>"},{"location":"learning-plan/learning-plan/#prework-vi","title":"Prework - vi","text":"<p>If you can open a file with vi, make changes and save it, then you're probably ok.</p> Expand to view vi commands <p>Basic vi commands you will use to edit your environment file</p> <p>Use the arrows on your keyboard to go to location you want to edit, you can not use your mouse.</p> <p>[ESC] starts all modes and [ESC] will end all modes.</p> <p>Modes</p> <p>[ESC] x     = deletes one character where you cursor is. Once done [ESC]</p> <p>[ESC] i      =  puts you in insert mode, just start typing what you want to add. Once done [ESC]</p> <p>[ESC] dd  = delete entire line cursor is on.  Once done [ESC]</p> <p>[ESC] u     = will undo your last action.  Once done [ESC]</p> <p>Once done, the following will save your file</p> <p>[ESC] [SHIFT] : wq [ENTER]   </p> <p>If you messed up the file and you want to exit and not save your file</p> <p>[ESC] [SHIFT] :  q! [ENTER]   </p> <p>Cheat Sheets</p> <p>VI Cheat-sheet </p> <p>Nano Cheat-sheet </p>"},{"location":"reference/architecture/","title":"Architecture","text":"<p>The current Digital Business Automation architecture is documented here in the IBM Cloud Architecture Center.  </p> <p>It includes detailed architecture capabilities on the following components:  </p> <ul> <li>Workflow automation</li> <li>Content services</li> <li>Document processing</li> <li>Decision management</li> <li>Robotic process automation</li> <li>Operational intelligence</li> </ul>"},{"location":"reference/links-backup/","title":"Links backup","text":"Description IBM Cloud Pak for Business Automation Sales Kit (Seismic) Technology Zone - Business Automation Activation Kit Tech Seller Links (Publisher) SWAT Labs CP4BA 21.0.3 - Troubelshooting CP4BA Badge Info CP4BA Technical Accelerators DBA Labs IBM Business Automation Community  PxT Presentations TBD Using Workflow to Orchestrate Asynchronous Long-Running RPA Tasks"},{"location":"reference/links/","title":"Links","text":"<p>Note</p> <p>Use the Table of Contents navigation on the right to view all the links at one time.  Please note that some links require an IBMID to access.  </p>"},{"location":"reference/links/#general","title":"General","text":"Expand to view"},{"location":"reference/links/#ibm-business-automation-community","title":"IBM Business Automation Community","text":""},{"location":"reference/links/#ibm-software-map","title":"IBM Software Map","text":""},{"location":"reference/links/#tech-seller-links-ibm-only","title":"Tech Seller Links (IBM Only)","text":""},{"location":"reference/links/#cloud-pak-for-business-automation-badge-info-seismic","title":"Cloud Pak for Business Automation - Badge Info (seismic)","text":""},{"location":"reference/links/#cloud-pak-for-business-automation-2201","title":"Cloud Pak for Business Automation - 22.0.1","text":"Expand to view"},{"location":"reference/links/#cp4ba-2203-overview","title":"CP4BA 22.0.3 - Overview","text":""},{"location":"reference/links/#cp4ba-2203-capabilities-for-starter-deployments","title":"CP4BA 22.0.3 - Capabilities for starter deployments","text":""},{"location":"reference/links/#cp4ba-2203-capabilities-for-production-deployments","title":"CP4BA 22.0.3  Capabilities for production deployments","text":""},{"location":"reference/links/#cp4ba-2203-troubleshooting","title":"CP4BA 22.0.3 - Troubleshooting","text":""},{"location":"reference/links/#cloud-pak-for-business-automation-2103","title":"Cloud Pak for Business Automation - 21.0.3","text":"Expand to view"},{"location":"reference/links/#cp4ba-2103-overview","title":"CP4BA 21.0.3 - Overview","text":""},{"location":"reference/links/#cp4ba-2103-capabilities-for-starter-deployments","title":"CP4BA 21.0.3 - Capabilities for starter deployments","text":""},{"location":"reference/links/#cp4ba-2103-capabilities-for-production-deployments","title":"CP4BA 21.0.3  Capabilities for production deployments","text":""},{"location":"reference/links/#cp4ba-2103-troubleshooting","title":"CP4BA 21.0.3 - Troubleshooting","text":""},{"location":"reference/links/#sales-kits","title":"Sales Kits","text":"Expand to view"},{"location":"reference/links/#business-automation-sales-kits-seismic","title":"Business Automation Sales Kits (Seismic)","text":""},{"location":"reference/links/#ibm-cloud-pak-for-business-automation-sales-kit-seismic","title":"IBM Cloud Pak for Business Automation Sales Kit (Seismic)","text":""},{"location":"reference/links/#business-automation-workflow","title":"Business Automation Workflow","text":""},{"location":"reference/links/#operational-decision-manager-odm","title":"Operational Decision Manager (ODM)","text":""},{"location":"reference/links/#automation-decision-services-ads","title":"Automation Decision Services (ADS)","text":""},{"location":"reference/links/#filenet-content-manager-fcm","title":"FileNet Content Manager (FCM)","text":""},{"location":"reference/links/#datacap","title":"Datacap","text":""},{"location":"reference/links/#robotic-process-automation-rpa","title":"Robotic Process Automation (RPA)","text":""},{"location":"reference/links/#enterprise-records-ier","title":"Enterprise Records (IER)","text":""},{"location":"reference/links/#content-collector-icc","title":"Content Collector (ICC)","text":""},{"location":"reference/links/#automation-workstream-services-iaws","title":"Automation Workstream Services (IAWS)","text":""},{"location":"reference/links/#process-mining-pm","title":"Process Mining (PM)","text":""},{"location":"reference/links/#process-automation-manager-and-decision-manager-formerly-red-hat","title":"Process Automation Manager and Decision Manager (formerly Red Hat)","text":""},{"location":"reference/links/#grow-my-deal-request-form-ibm-only","title":"Grow My Deal Request Form (IBM Only)","text":""},{"location":"reference/links/#additional-labs-and-accelerators","title":"Additional Labs and Accelerators","text":"Expand to view"},{"location":"reference/links/#technology-zone-business-automation-activation-kit","title":"Technology Zone - Business Automation Activation Kit","text":""},{"location":"reference/links/#ibm-swat-labs","title":"IBM SWAT Labs","text":""},{"location":"reference/links/#cp4ba-technical-accelerators-ibm-only","title":"CP4BA Technical Accelerators (IBM Only)","text":""},{"location":"reference/links/#center-of-competency-coc-managed-clusters-ibm-only","title":"Center of Competency (CoC) Managed Clusters (IBM Only)","text":""},{"location":"reference/links/#dba-labs-ibm-only","title":"DBA Labs (IBM Only)","text":""},{"location":"reference/links/#pxt-presentations-ibm-only","title":"PxT Presentations (IBM Only)","text":""},{"location":"reference/links/#ocp-gymnasium","title":"OCP Gymnasium","text":""},{"location":"reference/links/#cp4ba-deployment-one-shot","title":"CP4BA deployment One-shot","text":""},{"location":"reference/links/#datacap-toolkit","title":"Datacap toolkit","text":""},{"location":"reference/links/#baw-localization","title":"BAW localization","text":""},{"location":"reference/links/#using-workflow-to-orchestrate-asynchronous-long-running-rpa-tasks-ibm-only","title":"Using Workflow to Orchestrate Asynchronous Long-Running RPA Tasks (IBM Only)","text":""},{"location":"reference/links/#datacap-accelerator-tech-zone","title":"Datacap Accelerator - Tech Zone","text":""},{"location":"reference/links/#datacap-accelerator-assets-ibm-only","title":"Datacap Accelerator Assets (IBM Only)","text":""},{"location":"reference/links/#automation-innovation-workshop-aiw","title":"Automation Innovation Workshop (AIW)","text":""},{"location":"reference/links/#trial-software","title":"Trial Software","text":"Expand to view"},{"location":"reference/links/#cloud-pak-for-business-automation-60-day-trial","title":"Cloud Pak for Business Automation (60 Day Trial)","text":""},{"location":"reference/links/#cp4baaas-30-day-trial","title":"CP4BAaaS (30 Day Trial)","text":""},{"location":"reference/links/#rpa-30-day-trial","title":"RPA (30 Day Trial)","text":""},{"location":"reference/links/#bawoc-30-day-trial","title":"BAWoC (30 Day Trial)","text":""},{"location":"reference/links/#process-mining-30-day-trial","title":"Process Mining (30 Day Trial)","text":""},{"location":"reference/links/#blueworks-live-30-day-trial","title":"Blueworks Live (30 Day Trial)","text":""},{"location":"reference/links/#red-hat-cp4ba-30-day-trial","title":"Red Hat - CP4BA (30 Day Trial)","text":""},{"location":"reference/links/#distributed-software-evaluation-program-ibm-only","title":"Distributed Software Evaluation Program (IBM Only)","text":""},{"location":"reference/links/#ibm-community","title":"IBM Community","text":"Expand to view"},{"location":"reference/links/#ibm-business-automation-community_1","title":"IBM Business Automation Community","text":""},{"location":"reference/links/#americas-automation-tech-sales-publisher","title":"Americas Automation Tech Sales - Publisher","text":""},{"location":"reference/links/#2022-ibm-automation-worldwide-sales-technical-sales-enablement-series-events-and-classes","title":"2022 IBM Automation Worldwide Sales &amp; Technical Sales Enablement Series - Events and Classes","text":""},{"location":"reference/links/#community-americas-business-automation-technical-sales-events-and-classes","title":"Community - Americas Business Automation Technical Sales - Events and Classes","text":""},{"location":"reference/links/#2022-one-technical-team-community-calls-for-bizauto-in-emea-events-and-classes","title":"2022 One Technical Team Community Calls for BizAuto in EMEA - Events and Classes","text":""},{"location":"reference/links/#ibm-capture-center-of-competency-box","title":"IBM Capture - Center of Competency - Box","text":""},{"location":"reference/links/#community-calls-2022-rpa-process-mining-events-and-classes","title":"Community Calls 2022 - RPA &amp; Process Mining - Events and Classes","text":""},{"location":"reference/links/#ibm-tech-sales-community-assets-business-automatoin","title":"IBM Tech Sales Community Assets - Business Automatoin","text":""},{"location":"site-info/site-info/","title":"Site Info","text":""},{"location":"site-info/site-info/#americas-dallas-june-13-17","title":"Americas - Dallas (June 13-17)","text":"Expand to view LocationSlack ChannelsSite InfoSite Contact Address Map Directions from Airport (DFW) IBM Global Solution Center  1177 S Belt Line Rd  Coppell, TX 75019  +14695498444 Client/BP Entrance: East Lobby facing South Belt Line Road  Cafeteria Hours: 7:30 am - 1:30 pm <p>Workspace: IBM CloudPak Ecosystem      Lobby: private-cp4ba-2022-sko-tech-academy-americas-lobby     General questions about the event, logistics, where to access info      Main channel: private-cp4ba-2022-sko-tech-academy-americas     Questions about SKO Tech Academy content (daffy, deployment, labs, etc)  </p> <p>Site Map Nearby Restaurants Welcome to Dallas </p> <p> </p> <p>Cierra Roessler - IBM Client Center Event Lead - 803.528.9143 - @Cierra Roessler (she/they)</p> <p>Marla Jankowski - IBM Innovation Studio Dallas Concierge - @marla.jankowski</p> <p>Top of page | Top of section</p>"},{"location":"site-info/site-info/#emea-hursely-june-20-24","title":"EMEA - Hursely (June 20-24)","text":"Expand to view LocationSlack ChannelsSite Info Address Map Directions from Airport (LHR) IBM Hursley  Hursley Park Rd, Hursley  Winchester SO21 2JN, United Kingdom  +441962815000   IBM Briefing Center - Auditorium Room  H002 - Heathcote  Please be sure to first check-in on  Monday, June 20th, at the Main Reception, no later then 12:45 <p>Workspace: IBM CloudPak Ecosystem      Lobby: private-cp4ba-2022-sko-tech-academy-emea-lobby     Main channel: private-cp4ba-2022-sko-tech-academy-emea </p> <p>Site Map TBD - Nearby Restaurants *TBD - Welcome to Hursley </p> <p> </p> <p>Top of page | Top of section</p>"},{"location":"site-info/site-info/#apac-tokyo-july-25-28","title":"APAC - Tokyo (July 25 - 28)","text":"Expand to view LocationSlack ChannelsSite Info Address Map Directions from Airport (HND) IBM Japan Headquarters  19-21 Nihonbashihakozakicho  Chuo City, Tokyo 103-8510, Japan  +81366671111 <p>Workspace: IBM CloudPak Ecosystem      Lobby: private-cp4ba-2022-sko-tech-academy-japan-lobby     Main channel: private-cp4ba-2022-sko-tech-academy-japan </p> <p> </p> <p>Top of page | Top of section</p>"},{"location":"site-info/site-info/#apac-korea-august-1-4-cancelled","title":"APAC - Korea (August 1-4 - CANCELLED)","text":"Expand to view <p>Top of page | Top of section</p>"},{"location":"site-info/slack-americas/","title":"Slack americas","text":"<p>Workspace: IBM CloudPak Ecosystem  Lobby: private-cp4ba-2022-sko-tech-academy-americas-lobby General questions about the event, logistics, where to access info Examples: Where is the classroom located?, what time is lunch today, what is the URL for our event?  Main channel: private-cp4ba-2022-sko-tech-academy-americas Questions about SKO Tech Academy content (daffy, deployment, labs, etc) Example: What environment file do I use for Daffy?, I am getting an error importing the BAN desktop - can somebody help me? </p> <p> Please reference Site Info for information on other/past events and also the Troubleshooting section for any known issues.  </p>"},{"location":"site-info/slack-tokyo/","title":"Slack tokyo","text":"<p>Workspace: IBM CloudPak Ecosystem </p> <p>Lobby: private-cp4ba-2022-sko-tech-academy-japan-lobby General questions about the event, logistics, where to access info Examples: Where is the classroom located?, what time is lunch today, what is the URL for our event?  Main: private-cp4ba-2022-sko-tech-academy-japan Questions about SKO Tech Academy content (daffy, deployment, labs, etc) Example: What environment file do I use for Daffy?, I am getting an error importing the BAN desktop - can somebody help me? </p> <p> Please reference Site Info for information on other/past events and also the Troubleshooting section for any known issues.  </p>"},{"location":"site-info/slack/","title":"Slack","text":"<p>Workspace: IBM CloudPak Ecosystem </p>"},{"location":"site-info/slack/#americas","title":"Americas","text":"Expand to view <p>Lobby: private-cp4ba-2022-sko-tech-academy-americas-lobby General questions about the event, logistics, where to access info Examples: Where is the classroom located?, what time is lunch today, what is the URL for our event?  Main channel: private-cp4ba-2022-sko-tech-academy-americas Questions about SKO Tech Academy content (daffy, deployment, labs, etc) Example: What environment file do I use for Daffy?, I am getting an error importing the BAN desktop - can somebody help me? </p>"},{"location":"site-info/slack/#emea","title":"EMEA","text":"Expand to view <p>Lobby: private-cp4ba-2022-sko-tech-academy-emea  Questions about SKO Tech Academy content (daffy, deployment, labs, etc) and general questions about the event, logistics, where to access info. Examples:     - What environment file do I use for Daffy?, I am getting an error importing the BAN desktop - can somebody help me?     - Where is the classroom located?, what time is lunch today, what is the URL for our event?  </p>"},{"location":"site-info/slack/#apac-tokyo","title":"APAC - Tokyo","text":"Expand to view <p>Lobby: private-cp4ba-2022-sko-tech-academy-japan-lobby  General questions about the event, logistics, where to access info  Examples: Where is the classroom located?, what time is lunch today, what is the URL for our event?  Main: private-cp4ba-2022-sko-tech-academy-japan Questions about SKO Tech Academy content (daffy, deployment, labs, etc)  Example: What environment file do I use for Daffy?, I am getting an error importing the BAN desktop - can somebody help me? </p> <p>Please reference Site Info for information on other/past events and also the Troubleshooting section for any known issues.  </p>"},{"location":"technology/","title":"Technology","text":""},{"location":"technology/#workflow","title":"Workflow","text":""},{"location":"technology/#decision","title":"Decision","text":""},{"location":"technology/decision/","title":"Automation Decision Service summary","text":"<p> Updated 10/22/2020 - Work in progress </p>"},{"location":"technology/decision/#introduction","title":"Introduction","text":"<p>Decision management supports three main use cases:</p> <ul> <li>Let users model, author and validate decisions in a low-code environment.</li> <li>Externalize decision logic from microservice code so it can be developed, managed and tested by business users, but still integrated as a service in a cloud native microservice based.</li> <li>Integrate with AI model for combining predictive scoring and business rules </li> </ul> <p>The product documentation</p>"},{"location":"technology/decision/#getting-started","title":"Getting started","text":""},{"location":"technology/decision/#installing-on-ibm-cloud-openshift-cluster","title":"Installing on IBM Cloud Openshift cluster","text":"<p>This section is a quick summary of the steps to do to deploy version 2020.2.</p>"},{"location":"technology/decision/#pre-requisites","title":"Pre-requisites","text":"<p>Access to an Opendhift cluster and get <code>oc</code> CLI.</p>"},{"location":"technology/decision/#get-access-to-the-container-images","title":"Get access to the container images","text":"<ul> <li>Get your product entitlement key at https://myibm.ibm.com/products-services/containerlibrary site.</li> <li>If not done already create an openShift project to host the decision service</li> </ul> <p><code>shell  oc -new-project cp4automation</code></p> <ul> <li>Create an image pull secret to access the IBM image registry </li> </ul> <p>```shell  oc create secret docker-registry admin.registrykey --docker-username=cp --docker-password= --docker-server=cp.icr.io -n cp4automation <p>secret/admin.registrykey created  ```</p> <ul> <li>Check that you can access and download the Cloud Pak for Automation images: </li> </ul> <p><code>shell  docker login cp.icr.io -u cp -p &lt;Generated-Key&gt;  docker pull cp.icr.io/cp/cp4a/aae/dba-dbcompatibility-initcontainer:20.0.2</code></p>"},{"location":"technology/decision/#install-common-services","title":"Install common services","text":"<ul> <li>Get scripts and config files archive from IBM github cloud pak for automation: </li> </ul> <p><code>shell  wget https://github.com/icp4a/cert-kubernetes/archive/20.0.2.1.tar.gz</code></p> <p>Those scripts define custom resource definitions for kubernetes deployment.</p> <ul> <li>Expand archive: <code>tar xvf 20.0.2.tar.gz</code></li> <li>Change to folder cert-kubernetes-20.0.2</li> <li>If not already present on the cluster create the <code>common-service</code> OpenShift project</li> </ul> <p><code>shell  # verify project does not exist  oc new-project common-service</code> * In common-service project, run the following script to install the different operators. If you do want to set the trace of the execution, you can uncomment the second line <code>set -x</code> in this script.</p> <p><code>shell   ./scripts/deploy_CS3.4.sh nonbai</code></p>"},{"location":"technology/decision/#prepare-the-operator-storage","title":"Prepare the operator storage","text":"<ul> <li>Edit <code>descriptors/operator-shared-pvc.yaml</code> and use ibmc-file-retain-gold-gid</li> <li>Create the PVC: </li> </ul> <p><code>shell  oc create -f descriptors/operator-shared-pvc.yaml</code></p> <ul> <li>Copy the drivers: </li> </ul> <p><code>oc cp scripts/jdbc cp4a/&lt;operator-podname&gt;:/opt/ansible/share -c ansible</code></p>"},{"location":"technology/decision/#install-the-operator","title":"Install the operator","text":"<ul> <li>Deploy operator: </li> </ul> <p><code>./scripts/deployOperator.sh -i cp.icr.io/cp/cp4a/icp4a-operator:20.0.2 -p admin.registrykey -n cp4a -a accept`</code></p>"},{"location":"technology/decision/#deploy-odm","title":"Deploy ODM","text":"<ul> <li>Apply the CR:   (note: this CR also includes a simple install of BAI)</li> <li>Update the network policies for ODM: </li> </ul> <p><code>oc get netpol</code></p> <ul> <li>Edit the odm-dc-networkpolicy, odm-dr-network-policy, odm-ds-console-network-policy, odm-ds-runtime-network-policy to add a blanket egress policy, for example:</li> </ul> <p><code>yaml   spec:   egress:   - {}   ingress:   - ports:     - port: &lt;PORT&gt;       protocol: TCP   podSelector:     matchLabels:       run: ppe-odm-decisioncenter   policyTypes:   - Egress   - Ingress</code></p>"},{"location":"technology/rpa/","title":"Robot Process Automation","text":"<p> Updated 10/21/2020 - Work in progress </p>"},{"location":"troubleshooting/troubleshooting-backup/","title":"Troubleshooting backup","text":""},{"location":"troubleshooting/troubleshooting-backup/#1-i-cant-access-my-team-cluster","title":"1. I can't access my team cluster","text":"Expand to view <p>If your OpenShift Web Console link does not appear to work, please obtain the link from your IBM Cloud account by doing the following:  1. Log into your IBM Cloud account 2. Select the enterprise account: 2326304 - itztsglenablement01 3. From the left navigation area, select: Resource list 4. From the Excel file provided in slack for team assignments, obtain your team's Bastion Cluster Name (ie. tech-academy-labXX) 5. Using the value from above, filter the Resource list using the Name column 6. Locate your cluster and double-check you have the correct team cluster. Upon confirmation, click on the cluster to display the cluster 7. From the cluster, select the OpenShift web console blue button in top-right area of the screen.     </p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#2-i-cant-find-or-get-an-error-when-trying-to-access-a-lab-or-artifact","title":"2. I can't find or get an error when trying to access a lab or artifact","text":"Expand to view <p>Please report the issue to an instructor or in the slack channel.  In the meantime, all the labs and artifacts can be accessed here:     - Solution Exports     - Labs </p> <p>Please note: within each Rebuild capability lab, there may be a reference to a Lab Data folder. This Lab Data folder is located within the capability folder in the Labs folder.  For example, below is the Lab Data folder for ADS (in the Decisions capability folder): ADS - Lab Data </p> <p>Note</p> <p>Recommendation is to clone this GitHub repository so that you have all the artifacts and labs on your local machine.  </p> <p>Reference: the source SWAT TechJam materials are located here: IBM TechJam 21.0.3 </p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#3-what-are-the-logins-for-each-cp4ba-capability","title":"3. What are the logins for each CP4BA capability?","text":"Expand to view <p>Please refer to the Client Onboarding - Deploy and Integrate - Installation Links section that identifies how to locate and download the icp4adeploy-cp4ba-access-info config map from the OpenShift Web Console.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#10-what-you-need-on-ibm-cloud-infrastructure-permissions","title":"10. What you need on IBM Cloud : infrastructure permissions","text":"Expand to view <p>Before you can order virtual machines and create clusters you need to convert your IBM Cloud account to a Pay-As-You-Go account. This is option can be found under account settings. If you have an IBM provided account your manage will need to approve this upgrade.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#11-daffy-cluster-permission-error","title":"11. Daffy cluster permission error","text":"Expand to view <p>If you have errors with permissions, you may have selected the wrong account. In this situation, please access your IBM Cloud account and then log out of your account. Next, run your Daffy command again to have it prompt you again for your account selection.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#12-error-with-environment-file-edited-using-microsoft-notepad","title":"12. Error with environment file edited using Microsoft Notepad","text":"Expand to view <p>If you edit your environment file using a Microsoft Windows machine and/or Microsoft Notepad, you may see extra characters at the end of each line (ie. \\r\\r\\r\\r).</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#20-getting-help-sharing-an-oc-login-command","title":"20. Getting Help : sharing an oc login command","text":"Expand to view <p>If the software on your cluster is not working as expected you may be asked by an expert to provide a login command or login token. This token allows them to log into your cluster using the CLI. To get the login token log into the OpenShift Web console, in the top right corner there is a drop-down, with your username as a label. Click the label and a Copy Login Command link will be shown, this is highlighted in red below.</p> <p></p> <p>The next screen will display a link \"Display Token\". Click on this link and the page below will be displayed. Copy the oc login command as highlighted in red.</p> <p></p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#30-finding-links-most-from-daffy","title":"30. Finding Links : Most from Daffy","text":"Expand to view <p>You can find the common link for CP4BA in the cp4ba-access-info config map as shown below. Open the config map and scroll down to find the URL details. </p> <p>You can also use a Daffy command run from your bastion, Daffy will output the same information from the config map. <code>/data/daffy/cp4ba/service.sh &lt;your environment&gt; --StarterConsole</code></p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#31-daffy-what-is-build-cleanup-and-rebuild","title":"31. Daffy - What is build, cleanup and rebuild?","text":"Expand to view <p>/data/daffy</p> Script Description build.sh This will build the cluster based on your environment file cleanup.sh This will cleanup/destroy the cluster based on your environment file rebuild.sh This will call the cleanup.sh then it will call the build.sh <p>/data/daffy/ocp</p> Script Description build.sh This will build the cluster based on your environment file cleanup.sh This will cleanup/destroy the cluster based on your environment file rebuild.sh This will call the cleanup.sh then it will call the build.sh <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#32-obscure-links-baw-process-admin-and-rest-ui","title":"32. Obscure Links : BAW Process Admin and Rest UI","text":"Expand to view <p>The link for Process Admin is not recorded in the access configmap. To get to Process Admin create the URL using the template below. Find the URL labeled Cloud Pak Dashboard and use it as the basis for the Process Admin URL: <code>&lt;Cloud Pak Dashboard&gt;/bas/ProcessAdmin</code></p> <p>To get to BPM Rest UI create the URL using the template below. Find the URL labeled Cloud Pak Dashboard and use it as the basis for the Rest UI URL: <code>&lt;Cloud Pak Dashboard&gt;/bas/bpmrest-ui</code></p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#40-pods-what-to-look-for-and-how-to-restart","title":"40. Pods : what to look for and how to restart","text":"Expand to view <p>In the CP4BA Starter pattern many of the functional components run within the BA Studio pod. If your software is not behaving as expected (infinite blue spinning wheels, cases not starting) try restarting the BA Studio pod. Expand workloads and select pods, filter using studio and find the running BA Studio pod, click on the three dots and delete the pod. This will cause a new pod to be created, in several minutes login to CP4BA again and see if your fault has cleared.</p> <p></p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#50-resource-registry-automation-service-not-found-republish","title":"50. Resource Registry : automation service not found - republish","text":"Expand to view <p>If you have published an automation service but the client apps that try to use it reports an error then try unpublishing the automtion service and republish it.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#60-slack-groups-for-help-ibm-only","title":"60. Slack groups for help (IBM Only)","text":"Expand to view <p>For issues with the SWAT COB assets : #dba-swat-asset-qna</p> <p>For Daffy: #daffy-user-group</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#70-logs-where-do-the-different-components-log","title":"70. Logs : where do the different components log","text":"Expand to view <p>To access logs from your pods click on the pod name then select the log tab :   </p> <p></p> <p>It</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#71-better-logging-using-an-external-log-service","title":"71. Better logging : Using an external log service","text":"Expand to view <p>If you are using ROKS on IBM Cloud you can attach a log aggregation service running on IBM Cloud to your CP4BA cluster.</p> <p>Find the Log Analysis service in the IBM Cloud catalog and create an instance. The lite service doesn't have any log retention so choose the 7 Day search option. </p> <p></p> <p>You'll be taken to the logging service page in IBM Cloud, refresh the page in a couple of minutes and your logging service will appear. </p> <p>Find your cluster and click on its name to open the cluster details page. </p> <p>Scroll down to the integrations area and connect to the logging service. Once connected the connect button will be replaced with a launch button. </p> <p>In your apps your log output will now flow through to the log analysis service. In this example a BAW Toolkit is logging info messages, see \"Darth Vader\" below. </p> <p>The log analysis service is now receiving all logs from the cluster. Y can now filter by source, here we are filtering for the bastudio pod but this isn't necessary, a global text search is still very effective. </p> <p>At the bottom of the screen you can enter your search term to find the specific log output.  It is also useful to note the timestamp for the event then use \"Jump To Timeframe\" to find other events from other pods at the same timestamp for faultfinding. Log Analysis has many other features such as saved searches</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#80-common-faults-fixes-eg-restart-bastudio-edited","title":"80. Common faults &amp; fixes : eg restart bastudio (edited)","text":"Expand to view <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#81-i-cant-find-bawtos-clos-or-cp4bausers","title":"81. I can't find BAWTOS, CLOS or cp4bausers","text":"Expand to view <p>Content including the labs are based on the IBM SWAT Client Onboarding1 materials which are configured for the Production pattern.  Our environment uses the Starter pattern and based on that, you may encounter some differences at certain steps.  Below are some differences you should be aware of, please post to the slack channel if you encounter additional differences.  </p> <p>General For the Starter pattern, use cp4admin instead of cp4bausers which is used in the Production pattern.   </p> <p>Content Target object store: TARGET in Starter vs BAWTOS in Production Anywhere you see CLOS object store, use the CONTENT object store.</p> <p>Decisions Published automation service: The decisions project name contains the admin username which is different in Starter vs Production. The project name is included in the resource registry which would be different.</p> <p>Navigator The desktop uses the target object store and because of the difference in target object store names, we have to create different desktops for Starter vs Production. This GitHub has been updated with the correct artifacts to use the correct object store for the Starter pattern.</p> <p>App Designer The Client Onboarding app points to the target object store and this cannot be done with environment variables so we have to create different applications for Starter vs Production.  This GitHub has been updated with the correct artifacts to use the correct object store for the Starter pattern. Otherwise, you need to republish the Decisions automation service to match the correct resource registry name</p> <p>Workflow Environment variable needs an update to point to the right target object store. This GitHub has been updated with the correct artifacts to use the correct object store for the Starter pattern. Otherwise, you need to republish the Decisions automation service to match the correct resource registry name</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#90-using-workflow-to-orchestrate-asynchronous-long-running-rpa-tasks","title":"90. Using Workflow to Orchestrate Asynchronous Long-Running RPA Tasks","text":"Expand to view <p>Using Workflow to Orchestrate Asynchronous Long-Running RPA Tasks </p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#95-adding-certificate-for-rpa-server","title":"95. Adding Certificate for RPA Server","text":"Expand to view <p>There be dragons here  !!</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting-backup/#100-solution-exports-and-labs","title":"100. Solution Exports and Labs","text":"Expand to view <p>Solution Exports Labs </p> <p>Note</p> <p>Recommendation is to clone this GitHub repository so that you have all the artifacts and labs on your local machine.  </p> <p>Go to top of section | Go to top of page</p> <ol> <li> <p>The Client Onboarding assets have been adapted from the IBM TechJam 21.0.3 materials as developed by the IBM SWAT Team\u00a0\u21a9</p> </li> </ol>"},{"location":"troubleshooting/troubleshooting/","title":"Overview","text":""},{"location":"troubleshooting/troubleshooting/#1-i-cant-access-my-team-cluster","title":"1. I can't access my team cluster","text":"Expand to view <p>If your OpenShift Web Console link does not appear to work, please obtain the link from your IBM Cloud account by doing the following:  1. Log into your IBM Cloud account 2. Select the enterprise account: 2326304 - itztsglenablement01 3. From the left navigation area, select: Resource list 4. From the Excel file provided in slack for team assignments, obtain your team's Bastion Cluster Name (ie. tech-academy-labXX) 5. Using the value from above, filter the Resource list using the Name column 6. Locate your cluster and double-check you have the correct team cluster. Upon confirmation, click on the cluster to display the cluster 7. From the cluster, select the OpenShift web console blue button in top-right area of the screen.     </p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#2-i-cant-find-or-get-an-error-when-trying-to-access-a-lab-or-artifact","title":"2. I can't find or get an error when trying to access a lab or artifact","text":"Expand to view <p>Please report the issue to an instructor or in the slack channel.  In the meantime, all the labs and artifacts can be accessed here:     - Solution Exports     - Labs </p> <p>Please note: within each Rebuild capability lab, there may be a reference to a Lab Data folder. This Lab Data folder is located within the capability folder in the Labs folder.  For example, below is the Lab Data folder for ADS (in the Decisions capability folder): ADS - Lab Data </p> <p>Note</p> <p>Recommendation is to clone this GitHub repository so that you have all the artifacts and labs on your local machine.  </p> <p>Reference: the source SWAT TechJam materials are located here: IBM TechJam 21.0.3 </p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#3-what-are-the-logins-for-each-cp4ba-capability","title":"3. What are the logins for each CP4BA capability?","text":"Expand to view <p>Please refer to the Client Onboarding - Deploy and Integrate - Installation Links section that identifies how to locate and download the icp4adeploy-cp4ba-access-info config map from the OpenShift Web Console.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#4-accessing-business-applications-from-the-internet","title":"4. Accessing business applications from the internet","text":"Expand to view <p>If your business application is accessible from the internet, consider the following security configurations for your application data.</p> <p>Securing business application data</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#10-what-you-need-on-ibm-cloud-infrastructure-permissions","title":"10. What you need on IBM Cloud : infrastructure permissions","text":"Expand to view <p>Before you can order virtual machines and create clusters you need to convert your IBM Cloud account to a Pay-As-You-Go account. This is option can be found under account settings. If you have an IBM provided account your manage will need to approve this upgrade.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#11-daffy-cluster-permission-error","title":"11. Daffy cluster permission error","text":"Expand to view <p>If you have errors with permissions, you may have selected the wrong account. In this situation, please access your IBM Cloud account and then log out of your account. Next, run your Daffy command again to have it prompt you again for your account selection.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#12-error-with-environment-file-edited-using-microsoft-notepad","title":"12. Error with environment file edited using Microsoft Notepad","text":"Expand to view <p>If you edit your environment file using a Microsoft Windows machine and/or Microsoft Notepad, you may see extra characters at the end of each line (ie. \\r\\r\\r\\r).</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#20-getting-help-sharing-an-oc-login-command","title":"20. Getting Help : sharing an oc login command","text":"Expand to view <p>If the software on your cluster is not working as expected you may be asked by an expert to provide a login command or login token. This token allows them to log into your cluster using the CLI. To get the login token log into the OpenShift Web console, in the top right corner there is a drop-down, with your username as a label. Click the label and a Copy Login Command link will be shown, this is highlighted in red below.</p> <p></p> <p>The next screen will display a link \"Display Token\". Click on this link and the page below will be displayed. Copy the oc login command as highlighted in red.</p> <p></p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#30-finding-links-most-from-daffy","title":"30. Finding Links : Most from Daffy","text":"Expand to view <p>You can find the common link for CP4BA in the cp4ba-access-info config map as shown below. Open the config map and scroll down to find the URL details. </p> <p>You can also use a Daffy command run from your bastion, Daffy will output the same information from the config map. <code>/data/daffy/cp4ba/service.sh &lt;your environment&gt; --StarterConsole</code></p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#31-daffy-what-is-build-cleanup-and-rebuild","title":"31. Daffy - What is build, cleanup and rebuild?","text":"Expand to view <p>/data/daffy</p> Script Description build.sh This will build the cluster based on your environment file cleanup.sh This will cleanup/destroy the cluster based on your environment file rebuild.sh This will call the cleanup.sh then it will call the build.sh <p>/data/daffy/ocp</p> Script Description build.sh This will build the cluster based on your environment file cleanup.sh This will cleanup/destroy the cluster based on your environment file rebuild.sh This will call the cleanup.sh then it will call the build.sh <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#32-obscure-links-baw-process-admin-and-rest-ui","title":"32. Obscure Links : BAW Process Admin and Rest UI","text":"Expand to view <p>The link for Process Admin is not recorded in the access configmap. To get to Process Admin create the URL using the template below. Find the URL labeled Cloud Pak Dashboard and use it as the basis for the Process Admin URL: <code>&lt;Cloud Pak Dashboard&gt;/bas/ProcessAdmin</code></p> <p>To get to BPM Rest UI create the URL using the template below. Find the URL labeled Cloud Pak Dashboard and use it as the basis for the Rest UI URL: <code>&lt;Cloud Pak Dashboard&gt;/bas/bpmrest-ui</code></p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#40-pods-what-to-look-for-and-how-to-restart","title":"40. Pods : what to look for and how to restart","text":"Expand to view <p>In the CP4BA Starter pattern many of the functional components run within the BA Studio pod. If your software is not behaving as expected (infinite blue spinning wheels, cases not starting) try restarting the BA Studio pod. Expand workloads and select pods, filter using studio and find the running BA Studio pod, click on the three dots and delete the pod. This will cause a new pod to be created, in several minutes login to CP4BA again and see if your fault has cleared.</p> <p></p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#50-resource-registry-automation-service-not-found-republish","title":"50. Resource Registry : automation service not found - republish","text":"Expand to view <p>If you have published an automation service but the client apps that try to use it reports an error then try unpublishing the automtion service and republish it.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#60-slack-groups-for-help-ibm-only","title":"60. Slack groups for help (IBM Only)","text":"Expand to view <p>For issues with the SWAT COB assets : #dba-swat-asset-qna</p> <p>For Daffy: #daffy-user-group</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#70-general-troubleshooting-and-logs","title":"70. General Troubleshooting and Logs","text":"Expand to view <p>If you encounter an issue, try to determine the associated pod and then review the log file for that pod to further troubleshoot your issue.  </p> <p>To access the log file, locate and click on the pod name then select the Logs tab. Start reviewing the log file from the end of the file.  </p> <p></p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#71-better-logging-using-an-external-log-service","title":"71. Better logging : Using an external log service","text":"Expand to view <p>If you are using ROKS on IBM Cloud you can attach a log aggregation service running on IBM Cloud to your CP4BA cluster.</p> <p>Find the Log Analysis service in the IBM Cloud catalog and create an instance. The lite service doesn't have any log retention so choose the 7 Day search option. </p> <p></p> <p>You'll be taken to the logging service page in IBM Cloud, refresh the page in a couple of minutes and your logging service will appear. </p> <p>Find your cluster and click on its name to open the cluster details page. </p> <p>Scroll down to the integrations area and connect to the logging service. Once connected the connect button will be replaced with a launch button. </p> <p>In your apps your log output will now flow through to the log analysis service. In this example a BAW Toolkit is logging info messages, see \"Darth Vader\" below. </p> <p>The log analysis service is now receiving all logs from the cluster. Y can now filter by source, here we are filtering for the bastudio pod but this isn't necessary, a global text search is still very effective. </p> <p>At the bottom of the screen you can enter your search term to find the specific log output.  It is also useful to note the timestamp for the event then use \"Jump To Timeframe\" to find other events from other pods at the same timestamp for faultfinding. Log Analysis has many other features such as saved searches</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#80-cases-or-a-not-starting-up-classloader-issue","title":"80. Cases or a not starting up - ClassLoader issue","text":"Expand to view <p>When a user submits a client onboarding request (either from the end-to-end, or from the Workflow lab), the process to launch the Case is triggered and the case is triggered too but the case does not trigger any activities. This can be verified using Process Inspector and the Case Client. This is due to a ClassLoader known issue/bug. The way to resolve this is to re-create the cpe-deploy pod, load up ACCE and then re-create the bastudio pod.  </p> <p>Instructions to fix CPE Classloader problem (aka race condition) 1. Restart the CPE (cpe-deploy) Pod 2. Load the CPE Admin Console (ACCE), this will load the Java classes. 3. Restart the BA Studio (bastudio-deploy) Pod  </p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#81-error-when-launch-a-case","title":"81. Error when launch a case","text":"Expand to view <p>If you encounter an error when you run the Process to launch the Case, this is most likely due to the fact that the properties in script does NOT match the properties defined in the Case  </p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#82-i-cant-find-bawtos-clos-or-cp4bausers","title":"82. I can't find BAWTOS, CLOS or cp4bausers","text":"Expand to view <p>Content including the labs are based on the IBM SWAT Client Onboarding1 materials which are configured for the Production pattern.  Our environment uses the Starter pattern and based on that, you may encounter some differences at certain steps.  Below are some differences you should be aware of, please post to the slack channel if you encounter additional differences.  </p> <p>General For the Starter pattern, use cp4admin instead of cp4bausers which is used in the Production pattern.   </p> <p>Content Target object store: TARGET in Starter vs BAWTOS in Production Anywhere you see CLOS object store, use the CONTENT object store.</p> <p>Decisions Published automation service: The decisions project name contains the admin username which is different in Starter vs Production. The project name is included in the resource registry which would be different.</p> <p>Navigator The desktop uses the target object store and because of the difference in target object store names, we have to create different desktops for Starter vs Production. This GitHub has been updated with the correct artifacts to use the correct object store for the Starter pattern.</p> <p>App Designer The Client Onboarding app points to the target object store and this cannot be done with environment variables so we have to create different applications for Starter vs Production.  This GitHub has been updated with the correct artifacts to use the correct object store for the Starter pattern. Otherwise, you need to republish the Decisions automation service to match the correct resource registry name</p> <p>Workflow Environment variable needs an update to point to the right target object store. This GitHub has been updated with the correct artifacts to use the correct object store for the Starter pattern. Otherwise, you need to republish the Decisions automation service to match the correct resource registry name</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#83-rules-changed-in-ads-but-same-result-in-business-app","title":"83. Rules changed in ADS but same result in Business App","text":"Expand to view <p>When rules are changed in a published ADS Automation Service you must publish a new version and update the app to use the newly published automation service. (This is a bit odd after using ODM and not requiring apps to change as ODM and ADS both have the concept of <code>latest</code>, however this isn't supported in Automaiton Services until the next release).</p> <p>Updating the version of an Automation Service in a Business App requires us to add the new version of the Automation Service. DO NOT try to edit the existing version of the Automation Service in App Designer.  In the example below the app currently uses v4.0 of an Automation Service called <code>life_ds</code> published from ADS. A new version of the business logic has been published as <code>life_ds</code> v4.1. To upgrade from v4.0 to v4.1 the user would click on the Add buttnon.</p> <p></p> <p>Then select the new version of the Automation Service as shown, then click the Add button. </p> <p>As there are no breaking changes (such as changes to the interface) the update will be integrated into the app automatically. When an automatic update isn't possible it is often easier to remove any previsouly generated code before adding the new version of the Automation Service.</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#84-preparing-bai-data","title":"84. Preparing BAI Data","text":"Expand to view <p>It is strongly recommended that you prepare the BAI data on a Mac as Windows can easily corrupt the JSON. The instructions below assume you are using a Mac.</p> <p>Download or clone the github repository for the event to your Mac. Within this repo go to the location below:  <code>docs/client-onboarding/Solution Exports/Business Automation Insights</code></p> <p>Within this directory you'll find the sample files : </p> <pre><code>Client Onboarding Completed.json\nads-data.json\ncase-data.json\nfinalDashboardLayout.json\nprocess-data.json\n</code></pre> <p>Two of these files have to be updated to reflect the index names in your CP4BA deployment. Using a text editor create a new file to called <code>commands.txt</code> , the name of the file doesn't matter, we'll just be using it to build and edit the commands. Copy the text below into your <code>commands.txt</code> file.</p> <pre><code>ES_ADMIN=\nES_PASSWORD=\nES_HOST=\ncurl -k -XGET -u ${ES_ADMIN}:${ES_PASSWORD} ${ES_HOST}/_aliases\n</code></pre> <p>Use the BAI values from the config map on your OpenShift cluster to update the <code>command.txt</code> file. Here is an example with some of the password &amp; URL obscured as an example:</p> <pre><code>ES_ADMIN=icp4ba\nES_PASSWORD=1Z*****Rj\nES_HOST=https://iaf-system-es-cp4ba-starter.some-url.eu-gb.containers.appdomain.cloud\ncurl -k -XGET -u ${ES_ADMIN}:${ES_PASSWORD} ${ES_HOST}/_aliases\n</code></pre> <p>Copy the commands from your file and paste them into a terminal window on your Mac. This will set the user, password and URL and execute a curl command that will fetch the index names in your cluster. Take a note of the date used within the index names, in the example below it is <code>2022.06.28</code> which is the date on which CP4BA was installed.</p> <p></p> <p>To update <code>process-data.json</code> paste the following command into your <code>command.txt</code> file.</p> <pre><code>sed -i.bak 's/2021.11.11/2022.06.28/g' process-data.json\n</code></pre> <p>The sed command is a stream editor and in the example above it will perform a global find and replace, wherever it finds <code>2021.11.11</code> it will replace it with <code>2022.06.28</code>. Update this command to replace <code>2021.11.11</code> with the date from the curl command, (it won't be <code>2022.06.28</code> you must use the value for your system).</p> <p>When you execute the command the sample data in <code>process-data.json</code> should be updated, the command will also create a backup file called <code>process-data.json.bak</code></p> <p>To update <code>case-data.json</code> paste the following command into your <code>command.txt</code> file.</p> <pre><code>sed -i.bak 's/2021.11.11/2022.06.28/g' case-data.json\n</code></pre> <p>Again, make sure you update this command before you execute it. If you've made an error you can restore the original files from the backup.</p> <p>You can now upload the three sample data files (we only had to update two of them) by executing the following command in your terminal:</p> <pre><code>curl -k -XPOST -H 'Content-Type: application/json' -u ${ES_ADMIN}:${ES_PASSWORD} ${ES_HOST}/_bulk --data-binary @case-data.json\ncurl -k -XPOST -H 'Content-Type: application/json' -u ${ES_ADMIN}:${ES_PASSWORD} ${ES_HOST}/_bulk --data-binary @process-data.json\ncurl -k -XPOST -H 'Content-Type: application/json' -u ${ES_ADMIN}:${ES_PASSWORD} ${ES_HOST}/_bulk --data-binary @ads-data.json\n</code></pre> <p>The BAI sample data has now been uploaded. You can now return to the deployment instructions and continue</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#90-using-workflow-to-orchestrate-asynchronous-long-running-rpa-tasks","title":"90. Using Workflow to Orchestrate Asynchronous Long-Running RPA Tasks","text":"Expand to view <p>Using Workflow to Orchestrate Asynchronous Long-Running RPA Tasks </p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#91-creating-a-git-repo-for-ads","title":"91. Creating a git repo for ADS","text":"Expand to view <p>From your git home page click on the button to create a new repository and give it a meaningful name and click the green create button. </p> <p>For ADS to connect to this repo you need to copy the URL which includes .git at the end. </p> <p>ADS also requires a personal access token that it uses for authenticating with git. To create this token go to settings. You get this by clicking on you icon in the top right corner. </p> <p>Within settings, now click on developer settings, this is in the left hand section at the bottom. </p> <p>In developer settings select personal access tokens, then click <code>Generate new token</code> </p> <p>Give your token a name, this name isn't important it just so you can remember what the token is used for. </p> <p>ADS requires repo priveledges, these are at the top. No other priveleges are needed by ADS. </p> <p>When the token is created copy it and paste it into the ADS github connection details along with the URL. You will not be able to retrieve this token again so make sure you copy it down now. </p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#95-adding-certificate-for-rpa-server-cp4ba-starter","title":"95. Adding Certificate for RPA Server (CP4BA Starter)","text":"Expand to view <p>These are the steps required to enable connectivity between BAW and a remote RPA node. The example below will connect BAW (in this case running within BA Studio as part of CP4BA Starter) to a TechZone VM running RPA studio using a simple synchronous Rest API call. You should be able to adapt these instructions to enable BAW to call other untrusted servers.  </p> <p>These steps should be performed after Daffy steps 1 &amp; 2, and before step 3.  </p> <ol> <li>Obtain the pem file (certificate) for the remote server. Log into the RPA VM and open Firefox. Click on the bookmark for IBM RPA Server, click the icon next to the address then click the button labeled 'Connection not secure'  On the next dialogue box click 'More information'.  When the information panel opens click on the view button.  Now scroll down until you find the link to download the PEM cert. </li> <li>Create the pem file to your bastion. You can open the pem file in an editor and copy the contents of the file into a new file on your bastion.</li> <li>Log into your OpenShift cluster using the CLI (you may need a login token from the OpenShift Web Console) and create an OpenShift secret in the cp4ba-starter project using the command below. You may have to adapt the command if you created the pem file in a different location. <code>oc create secret generic rpa-secret --from-file=tls.crt=/data/daffy/env/rpa.pem -n cp4ba-starter</code></li> <li> <p>Update the CR to add the secret containing the pem file. As the formatting in YAML is critical it is not safe to provide the code here. Use the image below to help you find the updated code in this sample YAML, update your YAML file accordingly:  Pay extra attention to indentation using spaces. It is recommended that you use an editor that understands the syntax of yaml.</p> </li> <li> <p>The final change is to deactivate the SSL validation in the Liberty server used to host BAW. Again this change is present in the sample YAML. Use the image below to help you find the changes in the sample yaml file and apply this to your CR. </p> </li> <li> <p>You can now proceed with step 3 of the Daffy build and create the CP4BA services. It is possible to modify the CR and reapply it to a running CP4BA cluster but this is beyond the scope of this guide.</p> </li> </ol> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#96-adding-certificate-for-rpa-server-cp4ba-production","title":"96. Adding Certificate for RPA Server (CP4BA Production)","text":"Expand to view <p>Waiting for CP4BA 22.0.1 and Daffy Production Template (Q3 2022)</p> <p>Go to top of section | Go to top of page</p>"},{"location":"troubleshooting/troubleshooting/#100-solution-exports-and-labs","title":"100. Solution Exports and Labs","text":"Expand to view <p>Solution Exports Labs </p> <p>Note</p> <p>Recommendation is to clone this GitHub repository so that you have all the artifacts and labs on your local machine.  </p> <p>Go to top of section | Go to top of page</p> <ol> <li> <p>The Client Onboarding assets have been adapted from the IBM TechJam 21.0.3 materials as developed by the IBM SWAT Team\u00a0\u21a9</p> </li> </ol>"},{"location":"use-cases/use-cases/","title":"Additional Use Cases","text":""},{"location":"use-cases/use-cases/#accounts-payable","title":"Accounts Payable","text":"<p>Accounts Payable</p>"},{"location":"use-cases/use-cases/#hr-onboarding-app","title":"HR Onboarding App","text":"<p>HR Onboarding App</p>"},{"location":"use-cases/use-cases/#refund-request","title":"Refund Request","text":"<p>Refund Request</p>"},{"location":"use-cases/use-cases/#onboarding-automation","title":"Onboarding Automation","text":"<p>Onboarding Automation</p>"},{"location":"use-cases/use-cases/#shared-services","title":"Shared Services","text":"<p>Shared Services</p>"},{"location":"use-cases/accounts-pay/","title":"Accounts Payable","text":""},{"location":"use-cases/accounts-pay/#accounts-payable","title":"Accounts Payable","text":"<p>an IBM Cloud Pak for Business Automation use case</p>"},{"location":"use-cases/accounts-pay/#introduction","title":"Introduction","text":"<p>Use Case Overview: Today, your accounts process is entirely manual, with the amount of invoices only increasing. When first capturing an invoice, you must enter the invoice data into multiple systems. From there, different people must validate the invoice data and match the invoice to the Purchase Order. This results in data entry errors and inconsistencies across multiple systems. After all these steps, you can finally process payment in the ERP system. In this demo, you will learn how to adapt business policies to changing conditions with business rules.</p> <p>Choose an option:</p> <ul> <li>Cloud Pak for Business Automation as a Service demo environment (predeployed for IBMers only): continue to the Getting Started Lab section below.</li> <li>Install Yourself: To deploy Accounts Payable on your own environment, and technical architecture information, see the dba-accounts-payable git repository which includes the required deployment artifacts.</li> </ul> <p></p>"},{"location":"use-cases/accounts-pay/#getting-started-lab","title":"Getting Started Lab","text":"<p>Are you ready to see flexible business decisions in action?</p>"},{"location":"use-cases/accounts-pay/#1-scenario-introduction-accounts-payable","title":"1. Scenario Introduction - Accounts Payable","text":"Expand to view <p>Demo Video</p> <p>In this demo, you will learn how to adapt business policies to changing conditions with business rules</p> <p></p> Demo Outline <p>Demo Outline</p> <p>Full Demo Narration</p> <ol> <li>Use Case Overview</li> <li>Focus Corp Demo Dashboard review</li> <li>Customer persona<ol> <li>Submit rejected invoice</li> <li>Turn on decision labels, fix $0 and submit approved</li> <li>Submit invoice that should be rejected, but is approved</li> </ol> </li> <li>Operations Specialist persona<ol> <li>Week 1 dashboard</li> </ol> </li> <li>Rule Manager persona<ol> <li>Decision model review</li> <li>Review $0 total amount text rule and Validate PO Against Supplier table</li> <li>Create new branch</li> <li>Add a new row in the table for IN / Inc scenario</li> <li>Validate</li> <li>Simulate rules and compare with original simulation report</li> <li>Deploy new version</li> </ol> </li> <li>Customer persona<ol> <li>Turn on new rule version</li> <li>Same invoice is correctly rejected</li> </ol> </li> <li>Operations Specialist persona<ol> <li>Week 2 dashboard</li> </ol> </li> <li>Use case review and value of decision services</li> </ol> Discovery Map <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/accounts-pay/#2-login-and-setup-your-environment","title":"2. Login and Setup Your Environment","text":"Expand to view <p>Select an option for your environment</p> Option 2A - Using a Cloud Pak for Business Automation as a Service environment (predeployed for IBMers only) ? <p></p> <p>IBM maintains multiple internal SaaS tenants for IBMers only.</p> <p>Please login to IBM Technology Zone and navigate to here to learn about these demo environments via the User Guide.</p> <p>1. Once you have access to an environment, please continue here: \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Enablement Users, this demo is not available on enablement tenants at this time. \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Demo Users, once your account administrator completes the below setup and provides you access, please continue. \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Administrator Users, please reference the Administration Guide (IBM only) for any additional setup information including onboarding users. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 &gt; Note: Navigate to the Demo tenants tab after login.</p> <p>2. Login to your Cloud Pak for Business Automation as a Service demo environment here to access your portal.</p> <p>3. Open the menu in the upper left.</p> <p>4. Select Production and then Run.</p> <p>5. Click the Process Portal tile</p> <p>6. Wait for the portal to load in a new browser window/tab</p> <p></p> <p>7. On the left menu, under the Dashboards, click Show more/less\u2026</p> <p>8. Click Focus Corp Demos Dashboard</p> <p>9. Wait for the dashboard to load on the right</p> <p></p> <p>10. Start by reviewing the available demos, they all represent a business automation use case so you can easily get started.</p> <p></p> <p>11. When ready, click the green Accounts Payable button to launch the use case.</p> <p>Go to top (Option 2a) | Go to Getting Start Lab</p> Option 2B - Are You Using Your Own Environment (not a SaaS demo) ? <p></p> <p>1. Standard Users, once your account administrator completes the below setup and provides you access, please continue.</p> <p>2. Administrator Users, expand the following section to access additional information to setup access for yourself and others in your environment:</p> Additional Administrator Setup For Your Own Environment <p>See the dba-accounts-payable git repository to deploy on your own platform.</p> <p>Standard Users, continue here...</p> <p>3. Ask your administrator for the Process Portal URL and your login credentials</p> <p>4. Wait for the desktop to load in a new browser window/tab (it can take some time) and log in</p> <p></p> <p>5. Start by reviewing the available demos, they all represent a business automation use case so you can easily get started.</p> <p>6. When ready, click the Accounts Payable tile to launch the demo and continue to the next section.</p> <p>Go to top (Option 2b) | Go to Getting Started Lab</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/accounts-pay/#3-submit-invoices-and-review-rules","title":"3. Submit invoices and review rules","text":"Expand to view"},{"location":"use-cases/accounts-pay/#be-your-customer","title":"Be your customer!","text":"<p> You are now a customer with an invoice to submit for payment and you want to make sure it is valid.  What do you experience? As a customer, you want to be paid for your services on time and make sure you are doing your part to submit valid invoice data.  Clear and consistent business rules ensure you know what you need to do.</p> <p></p> <ol> <li>Check the tabs to learn more about the demo<ol> <li>Storyboard Outline</li> <li>Demo Discovery Map</li> </ol> </li> </ol> <p></p> <p>2. Navigate to the Launch the Demo tab and then click on the picture of the Customer</p> <p></p> <p>3. Use the drop down to select the first invoice ending in 101R.</p> <p>NOTE: invoices ending in R should be rejected and A should be approved</p> <p></p> <p>4. Click Validate Invoice to see the results, the invoice is rejected</p> <p>5. If you want, you can read the reason code and resubmit the invoice to fix the error</p> <p>6. Choose to Submit Another Invoice</p> <p>7. Select the invoice ending in 105R</p> <p>8. At the bottom, expand the Demo Control Panel</p> <p>9. Turn on the decision labels to see which data is used to make the decision</p> <p>10. Submit the invoice and see that it is not rejected as expected which leads to rework in downstream business processes</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/accounts-pay/#4-review-the-operations-dashboard","title":"4. Review the Operations Dashboard","text":"Expand to view <p>NOTE: If your environment does not have Kibana dashboards, you may need to skip executing this section and just read it as Business Performance Center dasbboards are not available at this time.</p> <p>You are now a Focus Corp employee, first an Operations Specialist and later a Rule Manager.  Time to see how invoice validation works.</p> <p>1. Navigate back to the Focus Corp Demos Dashboard browser window/tab</p> <p>2. Click on Week 1 under the picture of the Operations Specialist</p> <p>3. Login to the Insights Dashboard</p> <p>NOTE: If this is your FIRST TIME, your password may not work; go back to the SaaS portal, mouse over your name in the upper right and click Set Password.  Once complete, come back and login to the Open Distro for Elasticsearch / Kibana.</p> <p></p> <p>4. Review the Week 1 dashboard metrics and note the following:     1. Average invoice processing time is unmanageable at above 100 minutes     1. The rules deviated from the final result on more than 60 invoices this week</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/accounts-pay/#5-review-the-business-rules","title":"5. Review the business rules","text":"Expand to view <p>Continuing as the Focus Corp Rule Manager, how can you adapt your rules to reduce rework and process invoices faster with less exceptions?</p> <p>Rule Managers can adjust rules, validate and simulate the decision service and deploy quickly and easily.</p> <ol> <li>Navigate back to the Focus Corp Demos Dashboard browser window/tab</li> <li>Click the picture of the Rule Manager</li> <li>Click Validate Invoice followed by main to open the decision model</li> <li>Review the decision model, green ovals are input data and blue boxes are decisions, each with their own business logic leading to the Final Result decision</li> </ol> <p></p> <p>5. Click the blue Validate PO against Supplier decision node</p> <p>6. On the left pane, scroll to the bottom and click to open the Validate PO against Supplier logic</p> <p>7. Review the decision table and note there is no row addressing Inc. suppliers.</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/accounts-pay/#6-update-and-simulate-the-business-rules","title":"6. Update and simulate the business rules","text":"Expand to view <ol> <li>At the upper left, click Validate Invoice</li> <li>On the left, just above main, click the plus icon</li> <li>Create a new branch name using a unique phrase such as your name, ensure main is the parent branch and click Create</li> </ol> <p>4. On the upper right, click the pencil to edit the decision model</p> <p>5. Open the same Validate PO against Supplier decision node and table</p> <p>6. Right click on the row number for row 3 and select Insert row -&gt; Below</p> <p>7. Enter values in the new row 4 to look for IN when suppliers are Inc type as pictured</p> <p>NOTE: you can copy and paste cells as you would in a spreadsheet editor</p> <p></p> <p>8. On the upper right, Save and Close when done editing the rules</p> <p>In the full lifecycle, a Rule Manager runs validations and test suites before deploying the rules, if you wish, explore by clicking Validate on the left side of the editor or selecting the Tests tab at the top to run the test suite provided.</p> <p>9. Click the Simulations tab and Simulations sub tab</p> <p>10. Place the mouse over the Accounts Payable Simulation and click the run icon on the right</p> <p>11. After clicking OK, you will see the simulation running with a spinning status icon</p> <p>12. Once a checkmark appears, place the mouse over the report name and click the compare icon on the right</p> <p></p> <p>13. Under main, select the simulation report that is furthest in the past</p> <p>14. Click Compare</p> <p>On the left are the results for the new rules and the right are the results for the original rules</p> <p>15. Compare and notice some important differences:     1. (pie chart) more invoices were rejected (the new rule rejects a new incorrect pattern)     1. (orange bar chart) higher value invoices are rejected more</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/accounts-pay/#7-deploy-the-business-rules-and-see-the-results","title":"7. Deploy the business rules and see the results","text":"Expand to view <ol> <li>On the upper left, to the right of Validate Invoice, click the name of the branch you created</li> <li>Click the Deployments tab and Configurations sub tab</li> <li>Place the mouse over Validate Invoice Dev Deployment and click the deploy icon on the right</li> <li>Review the deployment details and click Deploy</li> <li>Once a checkmark appears, click the name of the deployment report to open it and take note of the number under New Version at the bottom (such as 1.1)</li> </ol> <p>6. Navigate to the Customer\u2019s browser window/tab or back to the Focus Corp Demos Dashboard browser window/tab and click on the picture of the Customer</p> <p>7. Select the same invoice that was accepted earlier, ending in 105R</p> <p>8. At the bottom, expand the Demo Control Panel and activate the Upgrade rules scenario</p> <p>9. In the Your ruleset version field that appears, enter the version you deployed (such as 1.1)</p> <p>10. Submit the invoice and confirm it is rejected with the reason code from your new rule table row</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/accounts-pay/#8-review-latest-operational-performance","title":"8. Review latest operational performance","text":"Expand to view <p>NOTE: If your environment does not have Kibana dashboards, you may need to skip executing this section and just read it as Business Performance Center dasbboards are not available at this time.</p> <ol> <li>Navigate back to the Focus Corp Demos Dashboard browser window/tab</li> <li>Click on Week 2 under the picture of the Operations Specialist</li> <li>Review the dashboard metrics and note the following:<ol> <li>Lower average invoice processing time below 100 minutes</li> <li>Less deviation from the final result per week</li> </ol> </li> </ol> <p>The upgraded rules are reducing rework already!</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/accounts-pay/#9-summary","title":"9. Summary","text":"Expand to view <p>We just used decision automation, a capability within IBM Cloud Pak for Automation, to automate an accounts payable process.  With growing volumes of invoices, business rule automation helps reduce the amount of human intervention for account processing, detect issues earlier, and incorporate changes quickly when needed.</p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/accounts-pay/#full-demo-narration","title":"Full Demo Narration","text":"Expand to view <p>Demo Narration and Flow</p> <p>Demo Start Page</p> <p>Reviewing the demo launch page we see a text summary describing the demo and a discovery map diagram for our reference.  We will only focus on the Validate Invoice Data step in the flow.  OK, let's launch the demo.</p> <p>Customer invoice submissions</p> <p>We start by becoming the customer.  From the Focus Corp Accounts Payable Invoices portal, we select an invoice ending with R (therefore it should be rejected) and submit for rule validation.  We review five key data fields extracted from the invoice using intelligent document processing and then submit for validation.  This invoice is rejected by the business rules as the total amount must be greater than 0.</p> <p>Let's resubmit that invoice by fixing the total amount to $100 and see what the rules say.  We can also take a look at the fields that the business rules use to make their decision.  And this time, the corrected invoice is accepted and passes rule validation.</p> <p>We have various other invoices to try, let's select another one ending in R that should be rejected, but this time the rules do not find an issue and it is accepted.  This type of result means more rework in downstream business processes as humans are involved in more review and the invoice is routed back and forth to be fixed.</p> <p>Operations week 1</p> <p>Further, we can see from the Operations Specialist's Accounts Payable dashboard that the average invoice processing time in the upper right is unmanageable at above 100 minutes.  Further, the rules deviated from the final result on more than 60 invoices this week, adding to rework. Rules must change at the speed of the changing business so let's see how we can improve the business rules.</p> <p>Rules manager</p> <p>As a rules manager, we have access to the invoice validation decision model. The invoice data move from the oval at the bottom through multiple sub-decisions including a text rule to check the total amount is more than $0 which resulted in our first invoice being rejected.  Another decision table looks for combinations of PO numbers and supplier types that do not follow the correct pattern (update!).  This is where the second invoice should have be rejected but the table is missing a new invalid pattern.</p> <p>The rules manager, a business user, edits the decision model, adds a new row, easily does a copy/paste of the existing row to get a head start, exactly like a spreadsheet, adjusts the values to match the new Inc. based rejection pattern and the rule is ready.  Tables automatically generate multiple text rules that follow the same pattern so we do not have to write them all from scratch.</p> <p>Let's validate our work in real time by running the same test case as before that should have been rejected.  Great, and with the new rule we get the rejection reason we just created.</p> <p>Once they save the new decision model for audit and compliance, the rule manager can run an automated regression test but instead we'll move on to simulation.  Loading an old simulation report before the rule change, we see rule metrics including the number of invoices approved and rejected as well as trends by date and invoice amount.  No high invoice amounts are being rejected, interesting.  The business knows most of the high value invoices are Inc. vendors so the new rule should catch more of these errors.</p> <p>After running a new simulation and comparing side by side, the business can make an informed decision about the impact of new rules before ever deploying to production.  Yes, this looks as expected so we are ready to deploy.  In this case, the business can deploy on their own but this can be configured for only certain safe rules and others must go through IT testing.</p> <p>Customer approved to rejected</p> <p>After the rules manager deploys the new rules, we go back to the invoice submission and choose the same invoice that should now be rejected.  We see the invalid pattern of PO number and supplier type and we select in the demo control panel to upgrade to the latest rule version.  The results, rejected with the reason code deployed by the business rule manager.</p> <p>Operations week 2</p> <p>After these new rules are in place for a week, the Operations Specialist reviews their updated dashboard and sees solid improvement with a reduction of average invoice processing time from less invoice rework and also less deviation between the rules and the final invoice decision.  Our rules are more accurate.</p> <p>Use Case Summary</p> <p>We just used decision automation, a capability within IBM Cloud Pak for Automation, to automate an accounts payable process.  With growing volumes of invoices, business rule automation helps reduce the amount of human intervention for account processing, detect issues earlier, and incorporate changes quickly when needed.</p> <p>Go to top | Go to Getting Started Lab | Go to Introduction</p>"},{"location":"use-cases/hr-onboard-app/","title":"HR Onboarding App","text":""},{"location":"use-cases/hr-onboard-app/#hr-onboarding-application","title":"HR Onboarding Application","text":"<p>an IBM Cloud Pak for Business Automation use case</p>"},{"location":"use-cases/hr-onboard-app/#introduction","title":"Introduction","text":"<p>Use Case Overview: Existing HR applications are not easily modified and often require the IT team to work with the vendor and make code changes.\u00a0 There are often higher priority development projects that deliver customer value than HR.\u00a0 The result: adhoc spreadsheets, emails and local file shares and a disorganized process inhibits the flow of applicants and HR teams are frustrated with slow results.\u00a0 It's time for an easy to use low-code build environment where HR can create dynamic applications that connect to existing systems and deliver value quickly.</p> <p>Choose an option:</p> <ul> <li>Cloud Pak for Business Automation as a Service demo environment (predeployed for IBMers only): continue to the Getting Started Lab section below.</li> <li>Install Yourself: To deploy HR Onboarding Application on your own environment, and technical architecture information, see the dba-hr-onboarding-app git repository which includes the required deployment artifacts.</li> </ul> <p></p>"},{"location":"use-cases/hr-onboard-app/#getting-started-lab","title":"Getting Started Lab","text":"<p>Are you ready to build low-code business applications?</p>"},{"location":"use-cases/hr-onboard-app/#1-scenario-introduction-hr-onboarding-application","title":"1. Scenario Introduction - HR Onboarding Application","text":"Expand to view <p>Demo Video</p> <p>In this demo, you will build a low-code business application that connect to existing systems for workflow and decisions</p> <p></p> Demo Outline <p>Demo Outline</p> <p>Full Demo Narration</p> <ol> <li>Use Case Overview</li> <li>Focus Corp Demo Dashboard review</li> <li>Application Assembler persona<ol> <li>Automation service creation</li> <li>Preview onboarding application template</li> <li>Create application from template</li> <li>Add automation services to application</li> <li>Preview application</li> </ol> </li> <li>HR Specialist persona<ol> <li>Run the application</li> </ol> </li> <li>Use case review and value of low-code applications</li> </ol> Discovery Map <p></p> Process Diagram <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/hr-onboard-app/#2-login-and-setup-your-environment","title":"2. Login and Setup Your Environment","text":"Expand to view <p>Select an option for your environment</p> Option 2A - Using a Cloud Pak for Business Automation as a Service environment (predeployed for IBMers only) ? <p></p> <p>IBM maintains multiple internal SaaS tenants for IBMers only.</p> <p>Please login to IBM Technology Zone and navigate to here to learn about these demo environments via the User Guide.</p> <p>1. Once you have access to an environment, please continue here: \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Enablement Users, this demo is not available on enablement tenants at this time. \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Demo Users, once your account administrator completes the below setup and provides you access, please continue. \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Administrator Users, please reference the Administration Guide (IBM only) for any additional setup information including onboarding users. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 &gt; Note: Navigate to the Demo tenants tab after login.</p> <p>2. Login to your Cloud Pak for Business Automation as a Service demo environment here to access your portal.</p> <p>3. Open the menu in the upper left.</p> <p>4. Select Production and then Run.</p> <p>5. Click the Process Portal tile</p> <p>6. Wait for the portal to load in a new browser window/tab</p> <p></p> <p>7. On the left menu, under the Dashboards, click Show more/less\u2026</p> <p>8. Click Focus Corp Demos Dashboard</p> <p>9. Wait for the dashboard to load on the right</p> <p></p> <p>10. Start by reviewing the available demos, they all represent a business automation use case so you can easily get started.</p> <p></p> <p>11. When ready, click the green HR Onboarding App button to launch the use case.</p> <p>Go to top (Option 2a) | Go to Getting Start Lab</p> Option 2B - Are You Using Your Own Environment (not a SaaS demo) ? <p></p> <p>1. Standard Users, once your account administrator completes the below setup and provides you access, please continue.</p> <p>2. Administrator Users, expand the following section to access additional information to setup access for yourself and others in your environment:</p> Additional Administrator Setup For Your Own Environment <p>See the dba-hr-onboarding-app git repository to deploy on your own platform.</p> <p>Standard Users, continue here...</p> <p>3. Ask your administrator for the Process Portal URL and your login credentials</p> <p>4. Wait for the desktop to load in a new browser window/tab (it can take some time) and log in</p> <p></p> <p>5. Start by reviewing the available demos, they all represent a business automation use case so you can easily get started.</p> <p>6. When ready, click the HR Onboarding App tile to launch the demo and continue to the next section.</p> <p>Go to top (Option 2b) | Go to Getting Started Lab</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/hr-onboard-app/#3-tour-business-automation-studio","title":"3. Tour Business Automation Studio","text":"Expand to view <p>Create your own business app and drive automation</p> <p> You are now a member of the HR team that has some interest and experience building simple apps such as office scripts or web sites and would like to help your HR Specialist colleagues and make their jobs easier. As an Application Assembler, you decide to create a low-code business appllication that connects to existing systems for workflow and decisions in a unified experience.</p> <p></p> <ol> <li>Check the tabs to learn more about the demo<ol> <li>Storyboard Outline</li> <li>Demo Discovery Map</li> <li>Demo Diagram</li> </ol> </li> </ol> <p></p> <p>2. Navigate to the Launch the Demo tab and then click on the picture of the Application Assembler</p> <p></p> <p>3. Allow Studio to load and then take a look around.</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/hr-onboard-app/#4-create-an-automation-service","title":"4. Create an automation service","text":"Expand to view <p>Before you build your application, let's find and connect to some existing workflow and decision services to use within the application.  Studio can connect to external business automation services you already have today and unify them in a single application user experience with the low-code Application Designer.</p> <ol> <li>In the middle, click Automations to load the business automations section</li> </ol> <p></p> <p>2. Click Create -&gt; External and wait for the dialog to appear (this can take multiple seconds the first time you use the interface).</p> <p></p> <p>3. Select to create your automation service From a new connection unless another person using the environment already created a connection to the Business Automation Workflow instance that contains the HR Onboarding Application Services process application.</p> <p></p> <p>4. Enter your connection information as below and please be exact as the format is important.  Note, depending on your version of CP4BA, the form below may be different and not have all the fields such as Scheme or Port.</p> Field Value Connection Name any name you will recognize Scheme <code>https</code> Host the URL for your BAW environment, this is currently the following format <code>https://&lt;hostname&gt;/dba/dev</code> so for example <code>https://cp4ba-trial01.automationcloud.ibm.com/dba/dev</code> Port <code>443</code> Username the username of a non-SSO service credential created by the administrator of your BAW environment Password the password of a non-SSO service credential created by the administrator of your BAW environment <p>5. When done, click Next</p> <p></p> <p>6. Select the HR Onboarding Application Services process application in the drop down.</p> <p>7. Select all operation checkboxes and click Next.</p> <p></p> <p>8. Update the name and description as desired and click Publish</p> <p></p> <p>NOTE: be sure to remember the name as you will use this in a future section.</p> <p>9. Wait to receive an External service published notification in the upper right before proceeding.</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/hr-onboard-app/#5-preview-onboarding-application-template","title":"5. Preview onboarding application template","text":"Expand to view <p>Now that you published the external automation services, you are ready to create your busienss application.  You decide to look for a template to accelerate application assembly.</p> <ol> <li>Click the upper left menu and select Applications</li> </ol> <p></p> <p>2. Click Templates to show all the templates in the Studio environment.</p> <p>3. Locate or search for the Onboarding Application template and click Preview on the tile.</p> <p>NOTE: it may take some time for the template to be deployed and run, more than 10 or 20 seconds in some cases.</p> <p>NOTE: the application will launch in a new browser window/tab so be sure to look for the browser notification and allow it to launch.</p> <p></p> <p>Explore the application, including:</p> <p>4a. Try using the filters or search on the left side to find a candidate.</p> <p>4b. Once you select a Candidate it will load a details page (that can take a moment).</p> <p>4c. Try out some of the drop downs for status on the right side.</p> <p></p> <p>5. Once you are done, close that browser window/tab and come back to the Studio page.</p> <p>6. Click Create -&gt; Application (this can take multiple seconds the first time you use the interface).</p> <p></p> <p>7. Select the Onboarding Application template and give it a name and optional purpose.</p> <p></p> <p>Congratulations, you created your business application!</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/hr-onboard-app/#6-add-automation-services-to-your-application","title":"6. Add automation services to your application","text":"Expand to view <p>Now that you have an application, you can add the automation services you published so you can use them in the low-code build environment.</p> <ol> <li>On the right, locate the All views drop down under Drag a component to your page header, click and select Automation service at the bottom.</li> </ol> <p></p> <p>2. Click the Add + button</p> <p></p> <p>3. Click to select the name of the automation service you published earlier and then select all the operations and click Next.</p> <p></p> <p>4. Now that you the automation service is in the palette on the right, click and drag it into the application as pictured below (or you can choose another location if you prefer).</p> <p></p> <p>5. First, select the operation at the top as HR_Send_Offer_Letter.</p> <p></p> <p>6. Now, we need to map data stored in the application (provided by the template) to the automation service so it will be sent to the HR Send Offer Letter process.  Make the following changes for the first variable row in the Inputs section:</p> <p>6a. under Variable options, click the drop down and select Use existing variable</p> <p>6b. under Variable names, click Select and expand selectedCandidateDetails and select fullname.</p> <p>6c. under Create field, uncheck the box as we do not need to see the candidates name since it is already in the upper left corner.</p> <p>7. Repeat the same three steps above, selecting selectedCandidateDetails -&gt; personalEmail for the second step.</p> <p></p> <p>8. Click Done.</p> <p> A new button is added to the application that will launch the process and send the two variables automatically! </p> <p>9. Feel free to click the button and edit it using the palette that appears directly below.  I would recommend using the pencil to change the name and you can also use the paint bucket to change the color.</p> <p></p> <p>Next, we will add the second automation service to the application, a decision service to calculate a suggested salary based on information from the candidate and position.</p> <p>10. Use the right palette to click and drag the same automation service on to the application, this time below the blue header (or in a location you prefer).</p> <p></p> <p>11. Select the operation at the top as Calculate_Candidate_Salary_Range.</p> <p>12. Map the data stored in the application to the automation service as before so the business rules can use it to make the decision.  Make the following changes for the first variable row in the Inputs section:</p> <p>12a. under Variable options, click the drop down and select Use existing variable</p> <p>12b. under Variable names, click Select and expand selectedCandidateDetails -&gt; jobDetails -&gt; jobStatus.</p> <p>12c. under Create field, uncheck the box.</p> <p>13. Repeat the same three steps above for the remaining three variables using the following values for the second step:</p> <p>13a. selectedCandidateDetails -&gt; country</p> <p>13b. selectedCandidateDetails -&gt; jobDetails -&gt; jobLevel</p> <p>14c. selectedCandidateDetails -&gt; employmentType</p> <p>14. For the output variable at the bottom, you may retain the defaults to Create a new variable and Create field on page.</p> <p>15. Click Done.</p> <p></p> <p>The result is that three elements are added to the page: a button to launch the service and two decimal/numeric fields for the output of minimum and maximum.</p> <p>16. If you wish, you can perform some layout to move them around.  For example, right click on the button just added and create a horizontal layout or panel before it, then drag the three elements into it.</p> <p>17. You may also try clicking on one of the numeric fields such as Minimum and clicking the gear to configure it, try to locate Format and set it to Currency to automatically show a currency symbol for your country.</p> <p>Congratulations, you added a workflow and decision service to your application, all without writing any code!</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/hr-onboard-app/#7-preview-the-completed-application","title":"7. Preview the completed application","text":"Expand to view <p>Let's preview the application again and try out the automation services you added.</p> <ol> <li>In the upper right, click Preview.</li> </ol> <p>NOTE: as before, it may take some time for the template to be deployed and the application will launch in a new browser window/tab which may be blocked by your browser.</p> <p>2. Search or select a requisition and then a candidate to open the candidate details page to see the automation services you added.</p> <p></p> <p>3. Click the second button you added for the Calculate Candidate Salary Range decision service, the values should be updated in the UI within a few seconds.</p> <p></p> <p>4. Click the first button you added for the HR Send Offer Letter process, this will launch a process in the background.</p> <p></p> <p>5. To see the process task you will need to open the workflow server's task list which is visible in a few locations:</p> Option 1 - a Cloud Pak for Business Automation as a Service (predeployed for IBMers only) <ul> <li>go back to the SaaS portal that you first logged into and use the menu at the upper left to select Development (only select Production if that is the envionment you used for tour automation service connection) and then Run and finally the Navigator tile.</li> <li>Use the upper left menu in Navigator and look for the Work Dashboard feature to load the task list.</li> <li>If the above feature is not available in Navigator, you may also load the Process Portal tile from the Development -&gt; Run page.     </li> </ul> Option 2 - your own environment <p>open the Process Portal or Navigator -&gt; Work Dashboard for the Business Automation Workflow server you used in the external automation service connection above.</p> <p>6. Once you locate the Send Offer Letter task you can click to open it and see the data passed from your application to the process.</p> <p></p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/hr-onboard-app/#8-summary","title":"8. Summary","text":"Expand to view <p>In just a few minutes, you created, tested and published a new, low-code business application from a template and connected it to existing workflow and decision automation services.  This low-code experience enables greater control and simplicity in application authoring while accelerating time to value for the business teams that use the applications.</p> <p>Go to top | Go to Getting Start Lab</p>"},{"location":"use-cases/hr-onboard-app/#full-demo-narration","title":"Full Demo Narration","text":"Expand to view <p>Demo Narration and Flow</p> <p>Intro</p> <p>HR teams manage adhoc processes outside their systems of record, gathering data, performing reviews, sending offers, looping around to do rework unfortunately before finally obtaining the goal to onboard an employee.  Additionally, HR Specialists use multiple, disparate systems to complete their work and can easily lose track of the process.</p> <p>It's time for an easy to use low-code build environment where HR can create dynamic, unified business applications that connect to existing systems and deliver value quickly.</p> <p>Demo Start Page</p> <p>Reviewing the demo launch page we see a text summary describing the demo along with a discovery map and diagram for our reference.  OK, let's launch the demo.</p> <p>Studio Intro</p> <p>Welcome to the Studio low-code build environment.  As an Application Assembler you can use Studio to create Applications and connect to Business Automations.</p> <p>Automation service creation</p> <p>Before we build our application, let's find and connect to some existing workflow and decision services that we want to use.  Studio can connect to external business automation services you already have today and unify them in a single application user experience with the low-code Application Designer.</p> <p>After we connect to Business Automation Workflow HR environment, Studio prompts us to select our the application and operations to automatically import.  We'll select send offer letter and calculate salary range.  The Studio imports and automatically publishes a service and we can confirm the operations and relevant inputs and outputs.  Now we are ready to create our application!</p> <p>Preview onboarding application template</p> <p>Studio includes multiple templates to start from, let's preview and try out the Onboarding Application template.  The app pulls in our sample requisition and candidate data and provides an easy to use search and filter on the left.  After selecting a candidate, we see their details and status; let's mark Alicia as Prepare to onboard, and then change some document status values.</p> <p>Create application from template</p> <p>This template looks good, let's use it to create our own application.</p> <p>Add automation service</p> <p>We start in the low-code Application Designer and it's time to import our automation services from before so we can add them to the app.  It's as simple as drag and drop.  Let's add a button for send offer letter at the upper right, we select the operation, map the data since it already exists in our app from the template, and my button is automatically added.  Don't like grey buttons?  Change it to yellow, rename it, add an icon and make it a little smaller.  All with no code, we just integrated to an existing external workflow from inside our app!</p> <p>We also need to add that calculation service that uses business rules to decide a recommended salary range.  Let's drag it just below the right header and map the data in very much the same way as the other service.  This service returns an output so we let Application Designer create a new variable to hold it and add it to the page automatically, so simple.  With multiple elements on the screen, let's do some simple layout by adding a panel and dragging the fields into it.  We can also do advanced formatting such as setting the field to currency for the symbols and decimal places or moving to advanced mode to set a read only status and much more.  After doing the same to the second field, and formatting the button, we are ready to preview our app!</p> <p>Preview and use automation services</p> <p>Now in the running app, after selecting one of the candidates we immediately see our edits live.  First, let's calculate the salary range and see the values that the decision service brings back based on the candidate data.  Now, before we launch the send offer letter process, let's bring up the current work list.  On the right we see one task from an older process.  Now when we click to send a new offer letter, another task appears on the right for the assignee to complete.  For testing, we can even open the task and see the application passed along the required data.  Looks good to me.</p> <p>Run in Navigator</p> <p>After deploying to the production runtime, called Navigator, we can see the app appear on our home screen, run it and use it as before during the preview but now in a unified Navigator environment with multiple applications and tabs all in one experience and interface!</p> <p>Use Case Summary</p> <p>We just used the low-code Studio environment and its Application Designer to create a dynamic business application from a pre-built template to accelerate time to value.  Then we connected to external automation services including existing workflows and decision services, to help automate the onboarding process.  It's time to remove frustration and slow business results and replace them with productive and happy employees and customers.</p> <p>Go to top | Go to Getting Started Lab | Go to Introduction</p>"},{"location":"use-cases/onboarding-automation/","title":"Onboarding Automation","text":""},{"location":"use-cases/onboarding-automation/#onboarding-automation","title":"Onboarding Automation","text":"<p>an IBM Cloud Pak for Business Automation use case</p>"},{"location":"use-cases/onboarding-automation/#introduction","title":"Introduction","text":"<p>Use Case: Content and document services</p> <p>Use Case Overview: Focus Corp\u00a0accelerates the use of unstructured content in an employee onboarding use case\u00a0using teamspaces and secure external file sharing. You will assume the role of Lucy, an HR employee onboarding specialist at Focus Corp.\u00a0Lucy\u2019s objective is to improve Focus Corp\u2019s process\u00a0and\u00a0ensure various onboarding requirements are met in a secure, structured, consistent and timely manner to onboard the new employees. Focus Corp\u00a0must\u00a0collaborate both internally and externally during the employee onboarding process\u00a0as well as enforce structured and adhoc workflows.\u00a0</p> <p>Choose an option:</p> <ul> <li>Cloud Pak for Business Automation as a Service demo environment (likely an IBMer or Business Partner): your environment is predeployed, continue to the Getting Started Lab section below.</li> <li>Install Yourself: To deploy Onboarding Automation on your own environment, and technical architecture information, see the dba-onboarding-automation git repository which includes the required deployment artifacts.</li> </ul> <p></p>"},{"location":"use-cases/onboarding-automation/#getting-started-lab","title":"Getting Started Lab","text":""},{"location":"use-cases/onboarding-automation/#0-general-info","title":"0. General Info","text":"Expand to view icons used throughout this lab Icon Description \u2139\ufe0f Informational note \u26a0\ufe0f Warning note \u2699\ufe0f Sections identified with a \ufe61 indicates that the section may require deeper technical expertise and should be skipped by non-technical users"},{"location":"use-cases/onboarding-automation/#1-scenario-introduction-onboarding-automation","title":"1. Scenario Introduction - Onboarding Automation","text":"Expand to view <p>Use Case Overview Employee applicant (John Doe) submitted an employee application and video interview along with other materials as part of the employment application process. During this process, HR processors initially reviewed the employment application making comments and annotations on the document along with entering video bookmarks, allowing additional reviewers to quickly locate key responses. Lucy along with other reviewers are able to review all the information and documents pertaining to applicant John Doe in the teamspace in a consistent and secure manner using an intuitive user-interface. Some of the information such as the applicant's SSN, is available on some of the documents but they are redacted so that only reviewers with the appropriate permission are able to see the redacted content. The use of video bookmarks also allows Lucy to quickly jump to key sections of the video without necessarily having to watch the entire video. Finally, Lucy is able to integrate with enterprise workflow to launch additional manual and automated processes.</p> <p>Discovery Map</p> <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#2-login-and-setup-your-environment","title":"2. Login and Setup Your Environment","text":"Expand to view <p>Select an option for your environment</p> Option 2A - Using a Cloud Pak for Business Automation as a Service environment (likely an IBMer or Business Partner) ? <p>     IBM maintains multiple SaaS tenants that can be reserved and accessed by both IBMers and Business Partners.</p> <p>Please login to IBM Technology Zone and navigate to here to reserve an environment.</p> <p>1. Once you have access to an environment, please continue here: \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Enablement Users, your environment setup should already be completed, please continue. \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Demo Users, once your account administrator completes the below setup and provides you access, please continue. \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Administrator Users, please reference the Administration Guide (IBM only) for any additional setup information including onboarding users. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 &gt; Note: Navigate to the Demo tenants tab after login.         2. Login to your Cloud Pak for Business Automation as a Service demo environment here to access your portal.</p> <p>3. Open the menu in the upper left.</p> <p>4. Select Production and then Run.</p> <p>5. Click the Business Automation Apps tile to launch the Business Automation Navigator desktop</p> <p></p> <p>6. Wait for the desktop to load in a new browser window/tab (it can take some time)</p> <p>7. Click the Business Automation Apps tile to launch the Business Automation Navigator desktop  </p> <p></p> <p>8. Wait for the desktop to load in a new browser window/tab (it can take some time)  </p> <p>9. Start by reviewing the available demos, they all represent a business automation use case so you can easily get started.  </p> <p></p> <p>10. When ready, click the Onboarding Automation tile to launch the demo.  </p> <p></p> <p>11. Check the tabs to learn more about the demo \u00a0 \u00a0\u00a0 \u00a0 a. Storyboard \u00a0 \u00a0\u00a0 \u00a0 b. Discovery Map </p> <p></p> <p>12. Navigate to the Run the demo! tab and then click on the picture of the Lucy - HR Employee Onboarding Specialist </p> <p></p> <p>Please continue to the next section</p> <p>Go to top (Option 2A) | Go to Getting Started Lab</p> Option 2B - Are You Using Your Own Environment (not an on Cloud trial/demo) ?  <p>     1. Standard Users, once your account administrator completes the below setup and provides you access, please continue.</p> <p>2. Administrator Users, expand the following section to access additional information to setup access for yourself and others in your environment:</p> Additional Administrator Setup For Your Own Environment <p>See the dba-onboarding-automation git repository to deploy on your own platform.</p> <p>Standard Users, continue here...</p> <p>3. Ask your administrator for the URL to the desktop in Business Automation Navigator and your login credentials</p> <p>4. Wait for the desktop to load in a new browser window/tab (it can take some time) and log in</p> <p>5. Start by reviewing the available demos, they all represent a business automation use case so you can easily get started.</p> <p></p> <p>6. When ready, click the Onboarding Automation tile to launch the demo.  </p> <p></p> <p>7. Check the tabs to learn more about the demo \u00a0 \u00a0\u00a0 \u00a0 a. Storyboard \u00a0 \u00a0\u00a0 \u00a0 b. Discovery Map </p> <p></p> <p>8. Navigate to the Run the demo! tab and then click on the picture of the Lucy - HR Employee Onboarding Specialist </p> <p></p> <p>Please continue to the next section</p> <p>Go to top (Option 2B) | Go to Getting Started Lab</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#3-tour-ibm-navigator","title":"3. Tour IBM Navigator","text":"Expand to view <p>IBM Navigator is a desktop for an organization's workers to find and organize information.</p> <p>You are now a Human Resources (HR) Onboarding Specialist, Lucy, at Focus Corp reviewing an applicant, John Doe, for employment. As an onboarding specialist, Lucy will use the IBM Navigator user interface to review content submitted by John such as the employment application and video interview.</p> <p>Let's take a quick tour of the IBM Navigator user interface.</p> <p>1. Click on the navigation menu icon in the upper left corner of the screen  </p> <p>The navigation menu icon (also commonly referred to as the hamburger icon) is used to access the navigation menu and features such as Browse, Search, Share, Teamspaces, Work, Cases and Reports. Additional features can also be added using plugins. This interface including the top banner, list of features and menu options are configurable through the use of desktops.</p> <p>Note: The Reports feature is used for Records Management and may or may not be installed on your environment. Records Management related activities such as declaring a record can be done automatically and also manually using the Browse feature. The Work and Cases feature show workflow tasks assigned to the user and case manager activities (respectively).</p> <p></p> <p>2. Click the Browse feature  </p> <p></p> <p>You are now viewing a repository called Corporate Operations which is an IBM FileNet Content Manager repository.</p> <p>Note</p> <p>You can also access other repositories and perform cross-repository searches using CM8 (IBM Content Manager), CMOD (IBM Content Manager OnDemand), Box\u00ae and CMIS (Content Management Interoperability Services) compliant repositories like Alfresco\u00ae and Microsoft SharePoint\u00ae On-Premises.</p> <p>3. From the folder structure on the left side of the screen, traverse and click on the Focus Corp Docs / Human Resources / Onboarded Employees folder representing content for onboarded employees.  </p> <p></p> <p>As shown the diagram above, the center or content area of the screen shows the selected content which may also be used to traverse the folder structure. Additionally, the top of the content area displays the full folder structure of the selected content.</p> <p>4. Click on the Selena Swift checkbox as shown in the diagram below.  </p> <p>Please note that the folder can be selected by either clicking on the checkbox -OR- by clicking on the folder row. If a checkbox is not visible, contact your administrator to enable Content list checkboxes at the desktop level. Clicking on the folder name text will select the folder and additionally traverse into the folder.</p> <p></p> <p>For onboarded employee \u2013 Selena Swift, you can see custom folder properties such as the First Name, Last Name, Employee ID, Onboarded status and Hire Date.</p> <p>5. Navigate to the Focus Corp Docs / Human Resources / Onboarded Employees / Selena Swift / Employee Packet folder and then click on the checkbox for the first document. Next, in the upper-right corner, select the different Views (Details, Magazine, Filmstrip). As shown in the diagram below, the Details view is shown as the default view.  </p> <p></p> <p>In our example, Focus Corp maintains an Employee Packet folder that contains new employee info documents such as the Confidentiality Agreement and Employee Manual. In the Details view, you are able to see the thumbnail and properties of the selected content.</p> <p>6. Select the Magazine view  </p> <p></p> <p>In the Magazine view, you can now see the same content along with social features such as Likes, Tags, Downloads and Comments.</p> <p>7. Select the Filmstrip view. The Filmstrip view is very useful for looking at media such pictures. Select the Photos folder below the Employee Packet folder and then select each picture document.</p> <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#4-create-and-review-teamspace","title":"4. Create and Review Teamspace","text":"Expand to view <p>Let\u2019s now look at teamspaces. Teamspaces represent a focused view of the relevant folders, documents and searches that a team needs to complete their tasks.</p> <p>Our use case revolves around Lucy reviewing an existing teamspace for employee applicant John Doe. Typicaly, the teamspace would already be created, processed and reviewed by a few HR processors before reaching Lucy to further review and approve the applicant for onboarding. In this section, you first create an employee onboarding teamspace for the employee applicant John Doe. Next, you will review how a teamspace is used once it is created. </p> <p></p> 4.1 - Create Teamspace <p>An Employee Onboarding teamspace template should already be available. If not or if you want to learn more about creating teamspace templates, please proceed to the section below - 4.1.1 - Teamspace Template Builder. Otherwise, proceed to section 4.1.2 - Teamspace Builder. </p> <p></p> 4.1.1 - Teamspace Template Builder <p>\u2139\ufe0f  This section is recommended but optional. If you do not want to perform this section and use the existing teamspace template, please proceed to section 4.1.2 Teamspace Builder.</p> <p>This section covers the creation of a teamspace template using the teamspace template builder.</p> <p>The use of teamspace templates allows for the configuration of the folder hierarchy, documents, search templates, and security in a user-friendly wizard interface which can then be used to create teamspaces in a timely, secure and consistent manner.</p> <p>1. From the navigation menu, click on Teamspaces </p> <p></p> <p>2. Click on the Templates tab and then click on New Template </p> <p></p> <p>As shown in the diagram below, the Teamspace Template Builder is displayed.</p> <p>The teamspace template builder uses a wizard-driven user interface that guides you through the following steps: - Define Teamspace Template - Select Searches - Select Classes or Entry Templates - Folders and Documents Tree - Select Roles</p> <p></p> <p>3. Teamspace Template Builder - Define Teamspace Template  </p> <p>Enter a value for the Template name and Template description. If you are using a shared environment and plan to share the template, please select a value that distinguishes your template from others. In the diagram below, a generic template name and description is used for the employee onboarding use case and specified not to share the template.</p> <p>At this point, you can continue to the next step by clicking on the Next button for the remaining configuration items (searches, classes/entry templates, folders/documents and roles). At any point, you can complete the configuration of the template by clicking on the Finish button and optionally edit the configuration at a later time.</p> <p>When complete, click on the Next button to continue to the next step.</p> <p></p> <p>4. Teamspace Template Builder - Select Searches  </p> <p>You will not specify any searches for the teamspace, click the Next button to continue to the next step</p> <p></p> <p>5. Teamspace Template Builder - Select Classes or Entry Templates  </p> <p>This step specifies either document classes or entry templates that can be used to store documents into the teamspace. You will configure this step using classes. As shown in the diagram below, first click the option for Use classes to add documents. Select the Employment Application and the Document classes and click the Add button. Finally, click on the Next button to continue to the next step.</p> <p></p> <p>\u2139\ufe0f If the Employment Application class is not available, please contact your Administrator to make it available to you. Otherwise, selecting the default Document class is sufficient - you will just not see the custom properties (First Name, Last Name, Application Date) when adding the application document to the teamspace.</p> <p>6. Teamspace Template Builder - Folders and Documents Tree</p> <p>This step configures the folder structure that will be used for the teamspace. Additionally, documents can be added to the folder(s). The use of a common folder structure and default documents provides a consistent process so that any teamspaces created with this template uses a consistent and standardized folder hierarchy and documents.</p> <p>Click New Folder and then build the folder structure shown below or create your own custom folder structure. To add a document to a folder, click on the Add Document button and add a document. When complete, click on the Next button to continue to the last step.</p> <p></p> <p>7. Teamspace Template Builder - Select Roles</p> <p>This steps allows you to either associate existing role templates or create custom roles for the teamspace template. Click New Role, enter a Role Name / Role Description and then set the desired permissions for the role.</p> <p>As shown in the diagram below, create one or more the following custom roles: HR - Human Resources, Payroll - Payroll, Manager - Department Manager, IT - Information Technology. Alternatively, you can also select an existing role template available at the bottom and add them to the teamspace template. When complete, click on the Finish button to complete the setup of the teamspace template. The teamspace template can be updated at a later time if desired.</p> <p></p> <p>Success</p> <p>The teamspace template is now complete, please refer to the next section 4.1.2 Teamspace Builder to create a teamspace from this template.</p> <p>Go to top (Teamspace Template Builder) | Go to Getting Started Lab</p> <p></p> 4.1.2 - Teamspace Builder <p>This section covers how to create a teamspace from the Employee Onboarding teamspace template that was created in the section above.</p> <p>Download the items below, these items will be added later to the teamspace.</p> Item Download Link Employment Application Focus Corp - Employment Application.pdf Video Interview Focus Corp - Video Interview - John Doe - KT.mp4 <p>1. From the navigation menu, click on Teamspaces</p> <p></p> <p>2. Click on the Teamspaces tab and then click on New Teamspace</p> <p></p> <p>As shown in the diagram below, the Teamspace Builder is displayed.</p> <p>Similar to the Teamspace Template Builder, the Teamspace Builder uses a wizard-driven user interface that guides you through the following steps: - Define Teamspace - Select Searches - Select Classes or Entry Templates - Folders and Documents Tree - Select Users</p> <p></p> <p>3. Teamspace Builder - Define Teamspace  </p> <p>Enter a value for the Template name and Template description. If you are using a shared environment, please select a value that distinguishes your teamspace from others. Next, select the Employee Onboarding template or the teamspace template that was created in section 4.1.1 Teamspace Template Builder.</p> <p>When complete, review the information after the diagram below and then click on the Next button to continue to the next step.</p> <p></p> <p>You can directly build the teamspace or you can specify the teamspace template that contains some predefined configuration settings. Using the template allows us to consistently build the teamspace ensuring the proper folder, documents, searches and roles are configured for every new project. At the same time, you are also able to slightly modify these settings as you create the teamspace for any additional configuration that may be needed for the specific teamspace.</p> <p>At this point, you can continue to the next step by clicking on the Next button for the remaining configuration items (searches, classes/entry templates, folders/documents and roles). At any point, you can complete the configuration of the template by clicking on the Finish button and optionally edit the configuration at a later time.</p> <p>When complete, click on the Next button to continue to the next step.</p> <p>4. Teamspace Builder - Select Searches</p> <p>Any saved searches that were included in the teamspace template should be listed in the Selected searches area. If desired, add any searches that you would like to be included in the teamspace. When complete, click the Next button to continue to the next step.</p> <p></p> <p>5. Teamspace Builder - Select Classes or Entry Templates</p> <p>This step identifies the classes or entry templates that were configured with the teamspace template. Any document classes or entry templates that were included in the teamspace template should be listed in the Selected Classes / Entry Templates area.</p> <p>Click the Next button</p> <p></p> <p>6. Teamspace Builder - Folders and Documents Tree</p> <p>As shown below, the folder structure and associated documents were automatically created based on the teamspace template. In this step, you will add the Employment Application into Application folder.</p> <p>Right-click on the Application folder and click on Add Document from Local Drive</p> <p></p> <p>7. Teamspace Builder - Folders and Documents Tree - Add Document - Employment Application</p> <p>As shown in the diagram below, enter the following:</p> Field Value File name Focus Corp - Employment Application.pdf Class Employment Application First Name John Last Name Doe Application Date &lt; select any date &gt; <p>When complete, click on the Add button</p> <p></p> <p>8. Teamspace Builder - Folders and Documents Tree - Add Document - Video Interview</p> <p>Using the same process as the step above, add the Video Interview into the Application / Video Interview folder.</p> <p>As shown in the diagram below, enter the following:  </p> Field Value File name Focus Corp - Video Interview - John Doe - KT.mp4 Class Document <p>When complete, click on the Add button</p> <p></p> <p>9. Teamspace Builder - Select Users</p> <p>As shown in the diagram below, your user name should be identified as an owner of the teamspace. To add yourself to another role, click on a role in the Available roles area. To add additional users to the teamspace, click on Add Users and Groups... and add any additional users or groups to the teamspace.</p> <p>When complete, click the Finish button to complete the setup of your teamspace.</p> <p></p> <p>Go to top (Teamspace Builder) | Go to Getting Started Lab</p> <p>Go to top (Create Teamspace) | Go to Getting Started Lab</p> <p></p> 4.2 - Review Teamspace <p>1. From the navigation menu, select Teamspaces </p> <p></p> <p>A list of teamspaces should be displayed. In the diagram below, the  Employee Onboarding - John Doe teamspace is highlighted.</p> <p></p> <p>\u2139\ufe0f  Please use the teamspace that was created in 4.1.2 - Teamspace Builder. Let's refer to this as your teamspace going forward.</p> <p>2. Locate your teamspace and right-click on it and select Modify, Roles </p> <p></p> <p>A teamspace can be configured with custom roles for granular security permissions. Clicking on a role allows you to view and manage the security permissions of the role. In the diagram below, the HR - Human Resources custom role and associated permission are shown. The Team tab can also be selected to view and manage users/groups assigned to the role.</p> <p></p> <p>3. Click Cancel to exit the Modify Teamspace screen  </p> <p>4. You are now back at the list of teamspaces, click on your teamspace name to view it  </p> <p></p> <p>As shown in the digram below, this is your teamspace - note the three sections on the left side of the screen for Browse to manage content, Search to manage saved searches, Team to view members of the teamspace.</p> <p>Note the folder structure in the Browse section, this and the associated documents were automatically created using the teamspace template.</p> <p>The middle/main section of the screen displays the content selected in the Browse and Search section. As shown in the diagram above, the Application folder is selected in the Browse section with the contents of the folder displayed in the main section.</p> <p>On the right side of the screen, you will see a thumbnail of the document selected in the main section along with a Properties section showing the custom document properties associated with the document (First Name, Last Name, Application Date) along with the document System Properties.</p> <p></p> <p>Go to top (Review Teamspace) | Go to Getting Started Lab</p> <p></p> 4.3 - Prepare Teamspace Content <p>\u26a0\ufe0f This section is optional and covers the preparation of teamspace content, if you do not want to perform this section, please proceed to the next section. The preparation of the Employment Application document assumes you have familiarity with creating annotations.</p> <p>The following teamspace content will be prepared in this section:</p> <ul> <li> <p>Employment Application: prepare the document with initial annotations for lab section 5. View and Edit Documents</p> </li> <li> <p>Video Interview: prepare the video with intial bookmarks for lab section 7. Review Video</p> </li> </ul> <p></p> 4.3.1 - Prepare Employment Application <p>You will create annotations to the application document in prepartion for section 5. View and Edit Documents.</p> <p>1. Continuing from section 4.2 - Review Teamspace - open your teamspace and open the application document (Focus Corp - Employment Application.pdf)  </p> <p></p> <p>2. Using the diagram below as a reference, create the following annotations/comments: \u00a0 \u00a0 \u00a0 \u00a0 a. Redaction annotation on the Social Security Number with redaction reason Social Security Number \u00a0 \u00a0 \u00a0 \u00a0 b. Highlight annotation on the salary desired amount (85,000) \u00a0 \u00a0 \u00a0 \u00a0 c. Sticky note annotation with text: Salary request exceeds job posting - please review \u00a0 \u00a0 \u00a0 \u00a0 d. Text annotation with text: Salary request exceeds job posting - please review \u00a0 \u00a0 \u00a0 \u00a0 e. Stamp annotation with the Received date stamp \u00a0 \u00a0 \u00a0 \u00a0 f. Stamp annotation with the Reviewed date stamp \u00a0 \u00a0 \u00a0 \u00a0 g. Document comment with comment: Salary request exceeds job posting - please review \u00a0 \u00a0 \u00a0 \u00a0 h. Document comment with comment: Salary adjust was approved, please proceed with this candidate \u00a0 \u00a0 \u00a0 \u00a0 i. Document comment with comment: This is a test comment, right-click this comment and select Edit/Delete to edit or delete this comment  </p> <p></p> <p>3. When complete, click on the Save icon to save your updates and then the X icon to exit the viewer.  </p> <p></p> <p>Go to top (Prepare Employment Application) | Go to Getting Started Lab</p> <p></p> 4.3.2 - Prepare Video Interview <p>You will create video bookmarks to the video interview in preparation for section 7. Review Video.</p> <p>1. From your teamspace, click on the video name (Focus Corp - Video Interview - John Doe - KT.mp4) in the folder: Employee Onboarding  Application  Video Interview.  </p> <p></p> <p>2. Click on the Open in New Window icon in the upper right corner of the viewer to open the viewer in a new window so you can access the video bookmark feature.  </p> <p></p> <p>3. Create some bookmarks</p> <p>Info</p> Start Time Name 00:00:07 Tell me about yourself 00:00:31 Why shoud we hire you? 00:00:51 Why did you leave your last job? 00:01:05 What is your greatest weakness? 00:01:21 What are your greatest strength? 00:01:42 Where do you see yourself in 5 years? 00:01:57 What do you like to do outside of work? <p>Locate a time in the video to create a bookmark. This can be done by playing the video and pausing at the desired location. Once you have a good idea where to place the bookmark, click on Bookmarks, New Bookmark and then enter the Name and optionally the Description. If necessary, adjust the Start Time to the desired time in the video. Lastly, click on the Ok button to save the bookmark.</p> <p></p> <p>When complete, exit the viewer.</p> <p>Go to top (Prepare Video Interview) | Go to Getting Started Lab</p> <p>Go to top (Prepare Teamspace Content) | Go to Getting Started Lab</p> <p>Go to top (Create and Review Teamspace) | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#5-view-and-edit-documents","title":"5. View and Edit Documents","text":"Expand to view <p>\u2139\ufe0f Use the application document (Focus Corp - Employment Application.pdf) that was added to the teamspace in section 4.1.2 - Teamspace Builder. This document represents the employee applicant's employment application and also contains an I9 document which will you split into a second document using the Merge and Split option. Additional, please review section 4.3.1 - Prepare Employment Application for the initial annotations that should be placed on the application document.</p> <p>1. From your teamspace, click on the application document (Focus Corp - Employment Application.pdf)  </p> <p></p> <p>2. Review the different areas of the viewer including the various tools/icons as shown in the diagram below.  </p> <p>\u2139\ufe0f Hovering the mouse over each tool displays a tool tip identifying the purpose of the tool.</p> <p></p> <p>3. From view settings area of the screen, click on the Fit to window icon to fit a full page of the document in the display  </p> <p></p> <p>As displayed in the diagram above, the page should now fit in the display window. You can now see the full page of the document including all the redactions and annotations on the first page. Annotations allow you to mark up a document with items such as lines, rectangles, redactions, text, highlights, sticky notes, pencil and stamps. Each annotation has security permissions and includes the user and timestamp the annotation was created/modified.</p> <p>\u2139\ufe0f The annotations displayed in the steps above/below may not be present if you did not create them when you added the document to the teamspace. If so, use the diagram to identify what would have displayed and optionally create the annotation identified in the step. Otherwise, simply ignore the step and continue to the next step.</p> <p>4. For each redaction and annotation, hover the mouse over the item to see who created the item and the creation timestamp.  </p> <p></p> <p>5. Right-click on a redaction then select the first option which is Select redaction reason </p> <p></p> <p>As displayed in the diagram above, the reason for the redaction is displayed and can also be set to a different reason if desired. Redaction Reasons use role-based security so that only users within a configured role can see behind the redaction and/or manage the redaction. Additionally, data capture/processing solutions can be configured to automatically redact values such as a Social Security Number and associate with the redaction reason.</p> <p>6. Click Cancel to exit out of the Select Redaction Reason dialog window.  </p> <p>7. From the annotation toolbar, click on the Filled rectangle annotation (also referred to as a redaction). Next, draw a rectangle around the Driver's License value (Y521793367692).  </p> <p></p> <p>8. Right-click on the redaction created above and click on the Edit redaction reason option to bring up the Select redaction reason dialog. From the dialog, select PII and then click on the OK button.  </p> <p></p> <p>9. On the application document, locate the yellow sticky note on the document and either double-click on the sticky note -OR- right-click on the sticky note and then click on the second menu option for Edit text </p> <p></p> <p>As displayed in the diagram above, the text for the Sticky note annotation is displayed along with the creation/modified timestamp. With appropriate security, you are also able to modify the text. The Sticky note in the annotation toolbar is also identified in the diagram if you want to create a new Sticky note annotation.</p> <p>To the right of the Sticky note annotation on the application document, there is also a Text annotation to display text without having to open the annotation.</p> <p>10. In the upper-right corner of the viewer, click on the View Comments icon  </p> <p></p> <p>As displayed in the diagram above, any existing document comments are displayed here. The Filter box can be used to search for a specific comment. A new comment can be added by entering text at the bottom of the viewer and then pressing the Enter button. If the user has appropriate security, existing comments can be edited or deleted by right-clicking on a comment and then clicking on the Edit or Delete option as shown in the diagram above.</p> <p>Additionally, document comments can be added directly from the document by right-clicking on the document and then selecting the Properties menu option.</p> <p>When complete, click on the View Comments icon at the top of the viewer to toggle out of the View Comments mode.</p> <p>11. After exiting the View Comments mode, review the existing Stamp annotations and enter a new Stamp annotation by clicking on the Stamp annotation in the Annotations bar area on the right side of the viewer. Next, click on the Approved annotation icon and then click where you want the annotation to be located on the document.  </p> <p></p> <p>As displayed in the diagram above, the existing stamp annotations are shown magnified. These stamp annotations represent when the document was received and reviewed. After clicking on the Approved stamp annotation, the Stamp annotation should now appear on the document with your email address along with the date.</p> <p>When complete, click on the Save icon to save the document with the newly created annotation.</p> <p>12. Click on the Thumbnails view icon from the bottom of the viewer.  </p> <p></p> <p>As displayed in the diagram above, the thumbnails of each page of the document are displayed.</p> <p></p> <p>Success</p> <p>Congratulations, you completed the View and Edit Documents steps.</p> <p>Please note the additional features to view and edit documents:</p> IBM Content Navigator Edit Service Edit directly in any desktop application IBM Navigator Sync Sync and collaborate on documents directly from a computer desktop or mobile device IBM Navigator Mobile Access, manage and sync content on your mobile device IBM Navigator for Microsoft Office\u00ae Access and manage content within Microsoft Office Microsoft Office Online Editing\u00ae Collaborative editing using Microsoft Office 365\u00ae or  Microsoft Office Online Server\u00ae IBM Enterprise Video Streaming Stream video with automatic caption generation and automated transcoding for multiple bitrates, video size and quality <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#6-merge-and-split","title":"6. Merge and Split \u2699\ufe0f","text":"Expand to view <p>\u2139\ufe0f \u00a0 This section is optional, if you do not want to perform this section, please proceed to the next section.</p> <p>Continuing from the previous section viewing the thumbnails of the application document - the first five pages represent the actual application document. The next four pages is an I9 - Employment Verification document that does not belong in the same document. You will use the Merge and Split feature to first create a copy of our document and then remove the first five pages to create a separate I9 document. Next, you will remove the last four pages of our employment application document so that it only contains the correct pages.</p> <p>1. From the upper-right corner of the viewer, select the Open in New Window icon to open the viewer in a separate window which allows us to select the Merge and Split option.  </p> <p></p> <p>2. From the separate viewer window, select the Merge and Split icon at the top of the viewer.  </p> <p></p> <p>As displayed in the diagram below, you are now in the Merge and Split mode where you can see the three icons for Cut, Copy and Paste. You are also able to move one or more pages by selecting the page(s) and dragging it to the desired location.</p> <p></p> <p>3. Select the Add document icon from the top menu bar so that you can create the separate document  </p> <p></p> <p>As displayed in the diagram below, the Add Document window appears. Update the Class property to Document and the Document Title property to John Doe - I9.  Next, click the Add button to add the document.</p> <p></p> <p>4. There should now be two tabs in the document viewer with the second tab representing the newly added John Doe - I9 document. You want to remove the employee application pages - select the first five pages of the document and then click on the Cut icon to remove the selected pages.  </p> <p></p> <p>5. As shown in diagram below, there should be four document pages. Click on the Check in document icon to save and check-in the document to the repository.  </p> <p></p> <p>6. Click on the first document tab which is our original employment application document.  </p> <p></p> <p>7. Select the last four pages of the document and then select the Cut icon to remove the I9 document pages.  </p> <p></p> <p>8. Click on the Check in document icon to save and check-in the document to the repository.  </p> <p></p> <p>You can now exit the document viewer to return back to the teamspace.</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#7-review-video","title":"7. Review Video \u2699\ufe0f","text":"Expand to view <p>\u2139\ufe0f \u00a0 This section is optional, if you do not want to perform this section, please proceed to the next section. If you perform this section, please review section 4.3.2 Prepare Video Interview to prepare the video with bookmarks.</p> <p>Let's now take a look at the video viewer to view and stream videos and audio files.</p> <p>1. From your teamspace, click on the video name in the folder: Employee Onboarding  Application  Video Interview.  </p> <p></p> <p>Note that selecting the video will also provide a short live preview of the video in the right pane along with the properties of the video.</p> <p>2. Click on the Open in New Window icon in the upper right corner of the viewer to open the viewer in a new window so you can access the video bookmark feature.  </p> <p></p> <p>3. Click on Bookmarks on the top left area of the viewer  </p> <p></p> <p>After selecting Bookmarks, the list of video bookmarks will be displayed. These were manually entered by an HR specialist during the initial application review process. Each bookmark identifies the name of the bookmark, bookmark description, the time of the bookmark and the user who created the bookmark. The use of video bookmarks is very valuable for Focus Corp to be able to quickly review key segments of the video without viewing the entire video. Additionally, the video viewer enables you to view, collaborate and easily stream large video files without requiring any browser plugins.</p> <p>Entering text in the Filter field also allows you to search on the name of the bookmark allowing for even faster location of a particular bookmark in the event there are numerous bookmarks. For example, enter strengths to quickly go to the bookmark named What is your greatest strengths</p> <p>4. From the bookmark list, click on a bookmark and then cycle through the bookmarks by clicking on the Previous and/or Next buttons  </p> <p></p> <p>Notice that the video starts immediately as you click each bookmark with little or no lag. When the video is initially opened, the video can be played right away as the video is streamed in the background.</p> <p>5. Let's now create a new bookmark. Locate a time in the video to create a bookmark. This can be done by playing the video and pausing at the desired location. Once you have a good idea where to place the bookmark, click on New Bookmark and then enter the Name and optionally the Description. If necessary, adjust the Start Time to the desired time in the video.  </p> <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#8-launch-onboard-employee-process","title":"8. Launch Onboard Employee Process","text":"Expand to view <p>Lucy is now ready to approve the employee applicant, let's trigger a workflow to start the downstream onboarding process.</p> <p>1. From your teamspace, browse to the Application folder and right-click on the employment appliction to select the menu option for Workflow, Launch Process </p> <p></p> <p>2. Select the Onboard Employee process and click the OK button  </p> <p></p> <p>3. Enter any comments in the Review Request Details </p> <p></p> <p>As shown in the diagram above, the launch screen is displayed with properties such as the Document Name, First Name, Last Name, and Application Date automatically populated from the document properties. You are able to view the contents of employement application as a final check and ultimately trigger the process by clicking on the Launch process button.</p> <p>4. Click the Launch process button to start the process.  </p> <p></p> <p>Info</p> <p>Although not fully implemented within this scenario, the process just launched would orchestrate downstream activities to onboard the applicant including manual and automated steps such as sending the offer letter and employee packet, preparing backend systems for enrollment such as benefits, payroll and IT accounts.</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#9-summary","title":"9. Summary","text":"Expand to view <p>As you saw, content services enables secure and compliant content access through the use of teamspaces, viewing and editing capabilities such as role-based Redactions, merge and split, video bookmarks and finally workflow to automate additional processes. The Cloud Pak for Business Automation provides a powerful, pre-integrated platform including Content Services alongside additional business automation services such as Workflow (BAW), RPA, Decisions (ODM and ADS) and more.</p> <p>Please visit the Additional Assets section for additional features not covered in this lab.</p> <p>BAW - Business Automation Workflow, ODM - Operational Decision Manager, ADS - Automation Decision Services</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#10-troubleshooting","title":"10. Troubleshooting","text":"Expand to view <p>1. Focus Corp folder/documents or Teamspaces are not available If using a Cloud Pak for Business Automation as a Service tenant, please ensure you have selected the Production environment.  Otherwise, contact the environment administrator to confirm the set up instructions have been completed.  </p> <p>2. Navigator Browse - I see folders but no documents in the Focus Corp folder structure Confirm with your tenant admin that your user id is added to the TE_DEMO management access group. Refer to dba-onboarding-automation for more information (Section 1ibc - Groups).  In the meantime, please proceed to the next section of the lab as your access to folders and documents when creating a teamspace is not affected.  </p> <p>3. Navigator Browse - Unable to modify folder and document When using an Enablement environment, security permissions are restricted to allow only viewing of folders and documents. One exception is the folder: Corporate Operations / Focus Corp / Human Resources / Onboarded Employees / Unsecured.  Please use this folder and/or proceed to the next section of the lab as your access to folders and documents when creating a teamspace is not affected.  </p> <p>4. Launch Onboard Employee Processing Chrome is the recommended browser to display the document in the viewer. If using Firefox, you may need to temporarily disable the CSP header to view the document at this time.  For both viewers, ensure the default action to display PDF documents is set to use the browser.  </p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#additional-assets","title":"Additional Assets","text":"<p>Optional - additional assets to explore</p>"},{"location":"use-cases/onboarding-automation/#100-automation-document-processing","title":"100 - Automation Document Processing","text":"Expand to view <p>Video: Automation Document Processing Overview (3:46)</p> <p></p> <p>Traditional document capture solutions cannot meet the document processing demands of today\u2019s digital world. Increased market demand for speed and flexibility, combined with exponential growth in regulatory compliance requirements prevent enterprises from realizing the potential for digital transformation. Applying a full document processing workflow with low-code development tools and advanced AI with deep learning, enterprises can use IBM Automation Document Processing to eliminate manual work across the enterprise. Go to top | Go to Introduction | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#105-external-share","title":"105. External Share","text":"Expand to view <p>Video: Securely Share Content with External Users (3:37)</p> <p></p> <p>Content management is a team sport, and some of your team may be trusted external users. In this demo, you'll provide external users access to content in a simplified Navigator interface with security, full access control, and the same redaction of your sensitive or private data.</p> <p>Go to top | Go to Introduction | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#110-graphql","title":"110. GraphQL","text":"Expand to view <p>Video: Simplify Development using the FileNet GraphQL API (7:18)</p> <p></p> <p>The FileNet GraphQL API enables you to query and manipulate data easily through an intuitive and flexible syntax that simplifies application development for your Content Platform Engine. The API allows you to request the exact information you need and receive predictable results reducing the number of network calls and bandwidth requirements improving performance.</p> <p>Go to top | Go to Introduction | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#115-records-management","title":"115. Records Management","text":"Expand to view <p>Video: Records Management (3:15)</p> <p></p> <p>IBM Enterprise Records provides content, processes and connectivity to help you maintain and provide a record of compliance for electronic and physical records. The comprehensive solution enables you to streamline records-based activities and help enforce compliance with retention policies, with or without user participation. Using IBM, you can capture, declare, classify, store and dispose of electronic and physical records according to fiscal, legal and regulatory requirements. IBM Enterprise Records is also available as managed service on cloud and for a containerized deployment in a private or public cloud.</p> <p>Go to top | Go to Introduction | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#120-salesforce-integration","title":"120. Salesforce Integration","text":"Expand to view <p>Video: Salesforce CRM Content to FileNet (6:29)</p> <p></p> <p>Seamlessly connect Salesforce with FileNet Content Manager to eliminate a content silo and manage Salesforce content within the leading, modern FileNet Content Manager platform. This native integration allows Salesforce users to store and manage related content seamlessly in FileNet Content Manager as it relates to Salesforce accounts, cases, opportunities, and more. They will no longer need to switch applications to find related information. With a Salesforce-oriented user experience, this connector will require minimal to no training, and IT teams can take advantage of the feature-rich content management capabilities of FileNet and govern enterprise content in one place.</p> <p>Go to top | Go to Introduction | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#125-enterprise-video-streaming","title":"125. Enterprise Video Streaming","text":"Expand to view <p>Video: Stream and Manage Video with FileNet Enterprise Video Streaming (3:14)</p> <p></p> <p>With the FileNet Enterprise Video Streaming connector, you can upload, manage, quickly access video alongside other documents in FileNet Content Manager and search through automatically generated closed captions. IBM\u2019s Enterprise Video Streaming offering features multi-quality streaming and adaptive bit rates to eliminate download delays and provide users with fast access to relevant video moments.</p> <p>Go to top | Go to Introduction | Go to Getting Started Lab</p> <p></p>"},{"location":"use-cases/onboarding-automation/#130-parallel-serial-workflow","title":"130. Parallel / Serial Workflow","text":"Expand to view <p>Coming soon!</p> <p>Go to top | Go to Introduction | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#135-microsoft-online-editing","title":"135. Microsoft Online Editing","text":"Expand to view <p>Coming soon!</p> <p>Go to top | Go to Introduction | Go to Getting Started Lab</p>"},{"location":"use-cases/onboarding-automation/#210-automate-end-to-end-workflows","title":"210 - Automate End-to-End Workflows","text":"Expand to view <p>Video: Automate End-to-End Workflows (4:40)</p> <p></p> <p>Automating workflows, including business processes and case work, can both increase productivity and improve customer experience. By using a single workflow automation platform, organizations can quickly scale their business.  This demonstration shows how IBM Business Automation Workflow enables collaboration, integrates with content management, and provides analytics to automate and optimize end-to-end workflows.  </p> <p>Go to top | Go to Introduction | Go to Getting Started Lab</p>"},{"location":"use-cases/refund-req/","title":"Refund Request","text":""},{"location":"use-cases/refund-req/#refund-request","title":"Refund Request","text":"<p>an IBM Cloud Pak for Business Automation use case</p>"},{"location":"use-cases/refund-req/#introduction","title":"Introduction","text":"<p>Use Case Overview: Imagine that a customer purchases your product, but is unfortunately unsatisfied and requests a refund. Today, your refund process has several manual steps. Managing high volumes of refunds that require manual investigation leads to slow average completion times, inconsistent status updates to customers, and higher costs for any errors made. This use case demonstrates the use of automation in the form of straight-through-processing to save time, money, and customer frustration.</p> <p>Choose an option:</p> <ul> <li>Cloud Pak for Business Automation as a Service demo environment (likely an IBMer or Business Partner): your environment is predeployed, continue to the Getting Started Lab section below.</li> <li>Install Yourself: To deploy Refund Request on your own environment, and technical architecture information, see the dba-refund-request git repository which includes the required deployment artifacts.</li> </ul> <p></p>"},{"location":"use-cases/refund-req/#getting-started-lab","title":"Getting Started Lab","text":"<p> Are you ready to see straight through processing in action? </p>"},{"location":"use-cases/refund-req/#1-scenario-introduction-refund-request","title":"1. Scenario Introduction - Refund Request","text":"Expand to view <p>Demo Video In this demo, you will learn how to handle spikes in demand with straight-through processing through business automation.  Note, this video uses an older version of the use case than the current Getting Started Lab.</p> <p></p> Demo Outline <p>Demo Outline</p> <p>Full Demo Narration</p> <ol> <li>Use Case Overview</li> <li>[Customer] Submit some refunds</li> <li>[Refund Investigator] Complete the investigation task</li> <li>[Business Analyst] Review the week 1 Operations Dashboard</li> <li>[Rule Manager] Improve the business rules</li> <li>[Customer] Test the new business rules</li> <li>[Business Analyst] Review the week 2 Operations Dashboard</li> <li>Use case review and value of straight-through processing</li> </ol> Discovery Map <p></p> Process Map <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/refund-req/#2-login-and-setup-your-environment","title":"2. Login and Setup Your Environment","text":"Expand to view <p>Select an option for your environment</p> Option 2A - Using a Cloud Pak for Business Automation as a Service environment (likely an IBMer or Business Partner) ? <p></p> <p>IBM maintains multiple SaaS tenants that can be reserved and accessed by both IBMers and Business Partners.</p> <p>Please login to IBM Technology Zone and navigate to here to reserve an environment.</p> <p>1. Once you have access to an environment, please continue here: \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Enablement Users, your environment setup should already be completed, please continue. \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Demo Users, once your account administrator completes the below setup and provides you access, please continue. \u00a0 \u00a0 \u00a0 \u2022 \u00a0 Administrator Users, please reference the Administration Guide (IBM only) for any additional setup information including onboarding users. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 &gt; Note: Navigate to the Demo tenants tab after login.</p> <p>2. Login to your Cloud Pak for Business Automation as a Service demo environment here to access your portal.</p> <p>3. Open the menu in the upper left.</p> <p>4. Select Production and then Run.</p> <p>5. Click the Business Automation Apps tile to launch the Business Automation Navigator desktop</p> <p></p> <p>6. Wait for the desktop to load in a new browser window/tab (it can take some time)</p> <p>7. Start by reviewing the available demos, they all represent a business automation use case so you can easily get started.</p> <p></p> <p>8. When ready, click the Refund Request tile to launch the use case.</p> <p>You have the following options to navigate the Refund Request use case:</p> <ul> <li> <p>Option 1: Technical deep dive: the default interface is configured for experienced users who explore using the launchpad that provides easy access to the product interfaces for each part of the use case. If you chose Technical deep dive, continue to the next section.</p> </li> <li> <p>Option 2: Guided or Live Walkthrough: you may wish to have the application provide you with a single page view of the use case across multiple steps so you do not have to go to multiple interfaces.</p> <ul> <li>There is both a Guided approach with instructions, step completion check boxes, and a summary of what you learned in each step, or a Live Walkthrough approach that includes the same single page app without instructions which is great for showing someone else the use case.</li> <li>To activate these options, scroll to the bottom of the app and locate the Control Panel section (if hidden, click just above the grey footer to show it), expand the Control Panel and select your choice.</li> <li>If you choose this option, please STOP reading this page here and continue in the application.</li> </ul> </li> </ul> <p></p> <p>Go to top | Go to Getting Started Lab</p> Option 2B - Are You Using Your Own Environment (not an on Cloud trial/demo) ? <p></p> <p>1. Standard Users, once your account administrator completes the below setup and provides you access, please continue.</p> <p>2. Administrator Users, expand the following section to access additional information to setup access for yourself and others in your environment:</p> Additional Administrator Setup For Your Own Environment <p>See the dba-refund-request git repository to deploy on your own platform.</p> <p>Standard Users, continue here...</p> <p>3. Ask your administrator for the URL to the desktop in Business Automation Navigator and your login credentials</p> <p>4. Wait for the desktop to load in a new browser window/tab (it can take some time) and log in</p> <p></p> <p>5. Start by reviewing the available demos, they all represent a business automation use case so you can easily get started.</p> <p>6. When ready, click the Refund Request tile to launch the demo and continue to the next section.</p> <p>Go to top (Option 2b) | Go to Getting Started Lab</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/refund-req/#3-submit-some-refunds","title":"3. Submit some refunds","text":"Expand to view <p>Be your customer!</p> <p>You are now a customer that just purchased a product, but unfortunately you are unsatisfied and request a refund.  What do you experience? As a customer, you want quick answers to your refund requests, especially when there are no special circumstances that should cause a delay.  Straight through processing would be ideal for you.</p> <p>Info</p> <p>You should only continue with these steps if you choose the Technical deep dive option listed above in section 2 !</p> <p>1. Check the tabs to learn more about the demo</p> <p>\u00a0 \u00a0\u00a0 \u00a0 a. Storyboard Outline</p> <p>\u00a0 \u00a0\u00a0 \u00a0 b. Demo Discovery Map</p> <p>\u00a0 \u00a0\u00a0 \u00a0 c. Demo Diagram</p> <p></p> <p>2. Navigate to the Run the demo! tab and then click on the picture of the Customer</p> <p></p> <p>3. Use the drop down to select the order ending in AP.</p> <p>Info</p> <ul> <li>AP = approved</li> <li>DE = denied</li> <li>MA = manually investigated</li> </ul> <p></p> <p>4. Click Submit to see the results</p> <p></p> <p>5. Choose to submit another refund request</p> <p>6. Select the order ending in DE</p> <p>7. At the bottom, expand the Demo Control Panel</p> <p>8. Turn on the decision labels to see which data is used to make the decision</p> <p></p> <p>9. Review the data used to make the decisions</p> <p></p> <p>10. Submit the refund if desired</p> <p>11. Choose to submit another refund request</p> <p>12. Select the order ending in MA</p> <p>13. Submit the refund and see that a manual investigation is required</p> <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/refund-req/#4-optional-complete-the-investigation-task","title":"4. (Optional) Complete the investigation task","text":"Expand to view <p>You are now a Focus Corp employee, first an Refund Investigator and later a Business Analyst and Rule Manager.</p> <p>Refund Investigators are busy people as they not only process refunds that cannot be automatically approved or denied, but they also look into trends and perform data analysis across the Focus Corp organization.  When they need to take time out from their high priority work to perform a refund request, they are not too happy.  Luckily, Cloud Pak for Business Automation provides them all the information they need in one place and streamlines the investigation process.</p> <p>1. Click Return to launchpad</p> <p>2. Click the picture of the Refund Investigator</p> <p>3. Depending on your platform you may see one or more of the new Workplace interface or Navigator's Work Dashboard or Process Portal</p> <p>Note the data is passed from the application to the process which then retrieved additional information and routed the work to the appropriate Refund Investigator</p>"},{"location":"use-cases/refund-req/#workplace","title":"Workplace","text":""},{"location":"use-cases/refund-req/#navigators-work-dashboard","title":"Navigator's Work Dashboard","text":""},{"location":"use-cases/refund-req/#process-portal","title":"Process Portal","text":"<p>4. Review the list of tasks and click on the most recent Investigation task to claim it</p> <p>5. Complete the task by approving or denying the refund</p>"},{"location":"use-cases/refund-req/#workplace-others-are-similar","title":"Workplace (others are similar)","text":"<p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/refund-req/#5-review-the-operations-dashboard","title":"5. Review the Operations Dashboard","text":"Expand to view <p>You are now a different Focus Corp employee, a Business Analyst.  How can you help to improve straight through processing?</p> <p>Business Analysts are able to easily monitor business automation performance and identify problems before they become large.</p> <p>1. Navigate back to the Refund Request application browser window/tab</p> <p>2. Click the picture of the Business Analyst</p> <p>3. Click to open the Refund Request (RR) - Week 1 dashboard</p> <p></p> <p>4. Review the week 1 dashboard metrics and note the following:</p> <p>\u00a0 \u00a0\u00a0 \u00a0 a. A large number of manual requests in the pie chart</p> <p>\u00a0 \u00a0\u00a0 \u00a0 b. The high time for average manual investigation processing (~120 minutes)</p> <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/refund-req/#6-improve-the-business-rules","title":"6. Improve the business rules","text":"Expand to view <p>Now, take on the role of a Focus Corp Rule Manager who can adjust rules, validate and simulate the decision service and deploy quickly and easily.</p> <p>1. Navigate back to the Refund Request application browser window/tab</p> <p>2. Click the picture of the Rule Manager</p> <p>3. Click Refund Processing followed by main to open the decision model</p> <p></p> <p>4. Review the decision model, green ovals are input data and blue boxes are decisions, each with their own business logic leading to the Final Refund Decision</p> <p>5. Click the blue Validate Time Window decision node</p> <p>6. On the left pane, scroll to the bottom and click to open the Validate Time Window logic</p> <p></p> <p>7. Review the decision table and note multiple rows that lead to manual review.</p> <p></p> <p>The Rule Manager already created a new branch to try some rule adjustments.</p> <p>8. At the upper left, click the down arrow next to main and select Reduce Manual Processing</p> <p></p> <p>9. Open the same Validate Time Window decision node and table</p> <p>10. Review the decision table and note there are less manual review rows.</p> <p>NOTE: In the full lifecycle, a Rule Manager runs test suites and simulations before deploying the rules.  If you wish and feel comfortable, you can explore this on your own.</p> <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/refund-req/#7-test-the-new-business-rules","title":"7. Test the new business rules","text":"Expand to view <p>Time to test the new business rules in the customer's interface and see if the results change based on the new business rules.</p> <p>1. Navigate back to the Refund Request application browser window/tab</p> <p>2. Click the picture of the Customer</p> <p>3. Scroll to the bottom and expand the Demo Control Panel</p> <p>4. Turn on the improved straight through processing scenario</p> <p></p> <p>5. Select the recent order ending with MA and submit</p> <p>The \"MA\" order was previously categorized as a manually processed refund request, now it is APPROVED!  The will improve straight-through processing and get answers to customers faster!</p> <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/refund-req/#8-review-the-updated-operations-dashboard","title":"8. Review the updated Operations Dashboard","text":"Expand to view <p>1. Navigate back to the Refund Request application browser window/tab</p> <p>2. Click the picture of the Business Analyst</p> <p>3. Click to open the Refund Request (RR) - Week 2 dashboard</p> <p>4. Review the dashboard metrics and note the following</p> <p>\u00a0 \u00a0\u00a0 \u00a0 a. a smaller number of manual requests in the pie chart</p> <p>\u00a0 \u00a0\u00a0 \u00a0 b. a lower time for average manual investigation processing (~60 minutes)</p> <p></p> <p>The improved straight-through processing is making an impact on Focus Corp's results!</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/refund-req/#9-summary","title":"9. Summary","text":"Expand to view <p>With combined workflow and decision automation, both capabilities within IBM Cloud Pak for Business Automation, and updated business rules, we reduce the manual processing queues significantly.</p> <p>Increased straight through processing results in lower average completion time, lower costs, and more consistent communication with the customer throughout the process.  Now our processes can truly increase customer satisfaction.</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/refund-req/#full-use-case-video-narration","title":"Full Use Case Video Narration","text":"Expand to view <p>The below uses an older version of the use case than the current Getting Started Lab.</p> <p>Focus Corp Demos Dashboard</p> <p>Reviewing the Focus Corp Demos Dashboard we see a text summary describing the demo, a discovery map diagram and a flow diagram for our reference.  OK, let's launch the demo...</p> <p>Customer refund requests</p> <p>We start by becoming the customer.  From the Focus Corp Your Returns and Refunds portal, we select one of our recent orders and submit a return.  The result is approved with an expected date to receive funds.</p> <p>We select and submit another order for refund but before we submit, let's take a peak behind the form at some of the business rules.  A few of the fields are used by a set of business rules to make the refund decision and we see those here.  This refund request results in a denial but it is still straight through processing without any manual work.</p> <p>Upon selecting a final, third order, the rules this time are not certain and send the refund request off to a human queue for manual investigation.  This bottom investigation branch of the straight through process can utilize significant resources.</p> <p>Operations week 1</p> <p>A few manual refunds are not a problem for the team, but too many will cause delays and our customer satisfaction will go down.  The Operations Specialist reviews the Refund Dashboard, powered by Business Automation Insights and see not only the percentage of manually processed requests is high at more than 67% but the average manual investigation time across all requests is more than 2 hours.  They pass their findings to the Rule Manager to take action.</p> <p>Rules manager</p> <p>As a rules manager, we have access to the decision model behind our refund process. The data is passed from the process system to the rules in the form of an order and refund request.  This moves through various decision tables including a time window validation that deals with the return reason, the days since delivery and the condition of the goods.  In combination, these lead to various processing decisions and we see a few that result in manual processing listed.  Another decision table deals with the type of goods and may also result in some manual processing.  Finally, this is then combined with a simple text rule to make the final decision.</p> <p>The rules manager drafted up a new branch of the decision model with some updates, let's take a look.  To reduce manual processing, the rules manager adjusted the tables to remove some of the manual processing decisions.  Comparing the two versions shows that row 3 column 3 moved from manual to denied, for example.  Other rows were removed which results in an default approved decision.  We could also test and simulate the rule changes and compare historical results but we won't cover that here in this demo.</p> <p>Customer manual to approved</p> <p>After the Rules Manager deploys the new rules, let go back to being a customer and try them out.  Submitting the same order as before which resulted in a manual result now is approved!</p> <p>Operations week</p> <p>After these new rules are in place for a week, the Operations Specialist sees some solid improvement with a reduction in half for manual requests and also lower manual processing time average from above 120, down to only 60 mins.</p> <p>Use Case Summary</p> <p>With combined workflow and decision automation, both capabilities within IBM Cloud Pak for Automation, and updated business rules, we reduce the manual processing queues significantly</p> <p>Increased straight through processing results in lower average completion time, lower costs, and more consistent communication with the customer throughout the process.  Now our processors can truly increase customer satisfaction.</p> <p>Go to top | Go to Getting Started Lab | Go to Introduction</p>"},{"location":"use-cases/shared-ser/","title":"Shared Services","text":""},{"location":"use-cases/shared-ser/#shared-services","title":"Shared Services","text":"<p>an IBM Cloud Pak for Business Automation use case</p>"},{"location":"use-cases/shared-ser/#introduction","title":"Introduction","text":"<p>Use Case Overview: Working in a remote environment adds complexity to most tasks. Imagine you lead a shared services team and receive requests from other departments such as Accounts Payable or HR. In person, it's simple to collect information and share status. However, in a remote environment, organizing work is more challenging. Relying on email and phone is just not enough. Work can be duplicated or executed inconsistently, resulting in lost data and delays. You need a lightweight workflow to manage incoming requests.</p> <p>Choose an option:</p> <ul> <li>Cloud Pak for Business Automation as a Service trial or demo environment: your environment is predeployed, continue to the Getting Started Lab section below.</li> <li>Install Yourself: To run Shared Services on your own environment you just need to install IBM Automation Workstream Services, there are no additional artifacts to deploy.</li> </ul> <p></p>"},{"location":"use-cases/shared-ser/#getting-started-lab","title":"Getting Started Lab","text":"<p>Are you ready to accelerate activities in your daily work ?</p>"},{"location":"use-cases/shared-ser/#1-scenario-introduction-shared-services","title":"1. Scenario Introduction - Shared Services","text":"Expand to view <p>Demo Video</p> <p>In this demo, you will learn how to use lightweight workflows to manage incoming requests.</p> <p></p> Demo Outline <p>Demo Outline Full Demo Narration</p> <ol> <li>Use Case Overview</li> <li>Focus Corp Demo Dashboard review</li> <li>Workstream Supervisor persona<ol> <li>Workplace Tour</li> <li>Confirm Workstreams access</li> <li>Create workstream</li> <li>Test workstream</li> <li>Publish workstream</li> </ol> </li> <li>Use case review and value of lightweight workflow applications</li> </ol> Discovery Map <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/shared-ser/#2-login-and-setup-your-environment","title":"2. Login and Setup Your Environment","text":"Expand to view <p>Select an option for your environment</p> Option 2A - Using a Cloud Pak for Business Automation as a Service trial or demo environment ? <p></p> <p>Any IBM maintained SaaS tenant with Automation Workstream Services will support this use case including the public trial for external users and multiple tenants that can be reserved and accessed by both IBMers and Business Partners.</p> <p>If you are using the trial, login here.</p> <p>For IBMers and Business Partners, please login to IBM Technology Zone and navigate to here to reserve an environment.</p> <p>For all users:</p> <p>1. Open the menu in the upper left.</p> <p>2. Select Production and then Run.</p> <p>3. Click the Workplace tile to launch the Workplace interface</p> <p>Go to top | Go to Getting Started Lab</p> Option 2B - Are You Using Your Own Environment (not a SaaS trial/demo) ? <p></p> <p>1. Standard Users, once your account administrator completes the Workstreams install and provides you access, please continue.</p> <p>Standard Users, continue here...</p> <p>2. Ask your administrator for the URL to Workplace and your login credentials</p> <p>Go to top (Option 2b) | Go to Getting Started Lab</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/shared-ser/#3-tour-workplace","title":"3. Tour Workplace","text":"Expand to view <p>Manage your work, don't let it manage you</p> <p>You are now a manager leading a Shared Services team that receives requests from other departments such as Accounts Payable or HR for assistance. As a Line of Business Supervisor, you decide to create a lightweight workflow application to manage incoming requests and avoid a disorganized email inbox.</p> <p>1. Allow Workplace to fully load and then look around.  If no one has used Workplace yet, it might be empty similar to below</p> <p></p> <p>Once it is used, tasks and workflows will appear and the counters at the top will populate based on due date (On track and Overdue) and expected completion based on past performance (At risk). All views can be easily configured using built-in filter and search.</p> <p></p> <p>2. Click on the blue Start workflow button highlighted in red above and review the activities and workflows available to launch, but do not launch any yet.</p> <p></p> <p>3. When done, close the Start workflow dialog.</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/shared-ser/#4-confirm-workstreams-access","title":"4. Confirm Workstreams access","text":"Expand to view <p>To start using Workplace, your administrator must first assign you, and any of your colleagues, a workstreams team which controls access in the Workplace environment.</p> <p>1. Click the menu grid icon in the upper right corner.</p> <p>2. If you see Create workstreams you have been given the required access to continue (Note, if you do not see this menu option, close the menu, wait 5-10 seconds and try again)</p> <p></p> <p>3. If you do not have Create workstreams in your menu, contact your administrator or respond to the email confirm for your trial and ask for the Configurators workstream team to be added and be sure to refresh your browser window to make it effective.</p> <p>4. Administrator Users, expand the following section to access additional steps to setup access for yourself and others in Workplace:</p> Additional Workstreams Administrator Setup <p></p> <p>1. Click the menu grid icon in the upper right corner and select Manage workstream teams.</p> <p>2. Click the radio button to the left of Administrators and wait for the right side list to load, which may be empty.</p> <p></p> <p>3. Click the plus sign in the right side to search for and add a new user to the team.  It is recommended to add yourself to Administrators and Configurators and optionally Team managers.</p> <p></p> <p>4. Optionally, create a new team named Shared Services and add yourself to represent assigning work to your team.</p> <p>5. IMPORTANT: now that you have the correct teams set, refresh your browser window/tab to make the roles effective.</p> <p>6. You will know once the role is effective when you click the menu grid icon in the upper right corner and see the following options.</p> <p></p> <p>Go to top | Go to Guided Tour</p> <p>Go to top | Go to Guided Tour</p>"},{"location":"use-cases/shared-ser/#5-create-a-workstream","title":"5. Create a workstream","text":"Expand to view <p>As a Supervisor, you are now ready to create a new workstream and publish it so the departments you work with, such as Accounts Payable and HR, can submit their requests in an organized manner.</p> <p>1. Click the menu grid icon in the upper right corner and select Create workstreams</p> <p></p> <p>2. Review the workstreams provided with the platform as examples that you can keep or edit or wait and review once you create your own workstream.</p> <p>3. Once done reviewing, click the blue New button and configure your definition as pictured below, then click Next when ready.</p> <p></p> <p>NOTE: selecting All users will allow any user in the system to submit a new request to your team; this could also be restricted to a team that you create, if desired.</p> <p> </p> <p>4. Review the workstream editor:</p> <p>a. Your workflow will show vertically on the left.</p> <p>b. The currently selected activity in your workflow is configurable on the right.</p> <p>c. A draft can be saved at any time.</p> <p>d. The Save button will place the workflow in trial mode for testers to try out.</p> <p> </p> <p>5. Name the first activity Skill profile and select Checklist as the type.</p> <p></p> <p>6. Scroll down on the right side to configure your workflow's first activity</p> <p>a. Add some checklist items.</p> <p>b. Select Allow users to update the checklist at start (it might make sense to use placeholders as depicted below).</p> <p></p> <p>c. Activate the data items slider, click Select data items, click the plus sign and add two data items as pictured below.</p> <p>d. Select each data item and Add selected items.</p> <p></p> <p>e. Click Who is responsible for this activity and choose either Shared Services, if you created that optional team and added yourself, or All users.  Note, if you leave this screen to go check the teams, be sure to save a draft first.</p> <p></p> <p></p> <p>7. When you are done and ready to create your second activity, you can either:</p> <ul> <li> <p>click the plus icon at the bottom, or</p> </li> <li> <p>scroll to the top and click the plus icon in the left side flow diagram.</p> </li> </ul> <p>8. Name the second activity Approve Resource and select Approval as the type.  This creates a two-way branch in the flow.</p> <p></p> <p>9. As above, click the plus sign next to If approved to add a third activity for that branch.</p> <p>a. Name the third activity Approval Notification</p> <p>b. Select Form as the type.</p> <p>c. Optionally click Select data items to add some data and select Read only.</p> <p></p> <p></p> <p>10. In the left diagram, click the Approve Resource activity box, then click the plus sign next to If rejected to add a fourth and final activity for the other branch.</p> <p>a. Name the fourth activity Denial Notification</p> <p>b.  Select Form as the type.</p> <p>c. Optionally add data as Read only.</p> <p></p> <p>11. You may change the Who is responsible for this activity selection for the two notification activities to The user who starts the workstream and/or the approval activity to Team managers (just make sure you are a member of the selected team).</p> <p>Success</p> <p>Congratulations, you created your first lightweight workflow!  Anyone can submit and route a workflow to the Shared Services team who respond with their skills and an approval with a notification sent to the submitter with the response.</p> <p>12. When complete, scroll to the top and click the blue Save button to put the workflow in trial mode</p> <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/shared-ser/#6-test-the-workstream","title":"6. Test the workstream","text":"Expand to view <p>It is time to test your new workstream. There is a built-in Testers group that allows you to delegate this role but for now, let us test ourselves.</p> <p>1. Click the menu grid icon in the upper right corner and select Create workstreams.</p> <p>2. Locate the new workstream you just created and click the three-dot menu on the right, then select Test.</p> <p></p> <p>3. The initialization screen is specific to this workflow; read the instructions and scroll down to customize the checklist available to you as the workflow starter.  An example is below</p> <p></p> <p>4. Once ready, scroll to the top right and click the blue Start workstream button.</p> <p>5. To test the first task as a Shared Services team member, click the menu grid icon in the upper right corner and select Workplace</p> <p>6. You should see a Skill profile task in the list, click the name and claim it.</p> <p>If you do not see the task in the list, you can either click the magnifying glass and search for <code>skill</code> or click the browser refresh button.  The task list may not refresh automatically in some browsers so searching should be faster than a full refresh.\"</p> <p></p> <p></p> <p>7. You are now testing as a Shared Services resource on the team you lead.</p> <p>a. Check off the items in the checklist</p> <p>b. Fill in the data fields</p> <p>c. Optionally upload a document or picture to securely include in the request.</p> <p>d. Optionally add a comment.</p> <p>e. Submit the task.</p> <p></p> <p></p> <p>8. Back at the task list, use the magnifying glass to search for Approve Resource and claim the task.</p> <p></p> <p>9. You are now the shared services team manager again, time to approve or decline the request.</p> <p>a. Review the approval screen.</p> <p>b. If you added data, it is read-only by default</p> <p>c. If you added comments in the previous task, they are present.</p> <p>d. Either Reject or Approve the request.</p> <p></p> <p></p> <p>10. Back at the task list, use the magnifying glass to search for Notification and claim the task.</p> <p></p> <p></p> <p>11. You are now the original submitter/requestor such as an Accounts Payable team member.</p> <p>a. Review the notification, including optional data and comments.</p> <p>b. When done, click Complete at the upper right.</p> <p></p> <p></p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/shared-ser/#7-publish-the-workstream","title":"7. Publish the workstream","text":"Expand to view <p>After testing, you are ready to publish your new workflow.</p> <p>1. Click the menu grid icon in the upper right corner and select Create workstreams</p> <p>2. Locate the new workstream you just created and click the three-dot menu on the right, then select Request publish.</p> <p></p> <p>The system has a built-in workflow for approving and publishing -- a workstream for publishing workstreams !</p> <p>3. Choose to Assign to any publisher and click Assign</p> <p></p> <p>4. Confirm the workflow is in the Published requested status as below.</p> <p></p> <p>5. Click the menu grid icon in the upper right corner and select Workplace.</p> <p>6. Locate or search for the task titled Workstream review... and click to claim it.</p> <p></p> <p>7. Review the workstream and note the Read-only notice next to the name since you are only reviewing, not editing.</p> <p>8. Click to Publish the workflow.</p> <p></p> <p>9. Click Start workflow and locate the just published workflow to launch.</p> <p></p> <p>Success</p> <p>In just a few minutes, you created, tested and published a new, lightweight workflow available to anyone in your organization !</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/shared-ser/#8-summary","title":"8. Summary","text":"Expand to view <p>As you saw, Workstream Services helps you manage remote work more effectively by allowing business users to create lightweight workflows that automate internal processes, no coding required. Get ready to improve your team's productivity by simplifying common tasks and decreasing the complexity and limitations of remote work.</p> <p>Go to top | Go to Getting Started Lab</p>"},{"location":"use-cases/shared-ser/#full-demo-narration","title":"Full Demo Narration","text":"Expand to view <p>Intro</p> <p>Working in a remote environment adds complexity to most tasks.  Imagine you lead a shared services team and receive requests from other departments such as Accounts Payable or HR. In person, it's simple to collect information and share status.</p> <p>However, in a remote environment, organizing work is more challenging.  Relying on email and phone is just not enough. Work can be duplicated or executed inconsistently, resulting in lost data and delays.  You need a lightweight workflow to manage incoming requests.</p> <p>Demo Start Page</p> <p>After reviewing the demo launch page including the outline and discovery map, we are ready to launch the Line of Business Supervisor's Workplace interface, the single place you manage all your work.</p> <p>Manage workstream teams</p> <p>You check out the start workflow list and see great starting activities and workflows such as approval, checklist and more.</p> <p>You start by managing the teams.  It's a good idea to add yourself to at least Administrators and Supervisors and maybe Team managers.  You can also create a Shared Services team so you can assign work to them later.</p> <p>Create a workstream</p> <p>Now it's time to create a workstream for incoming shared services requests.  We'll set it so everyone can start and give a description.  The simple, no-code editor allows you to assemble a workflow in just a few clicks.  Start with a checklist activity to represent the skill profile requested.  To make this reusable, we add checklist items with placeholders and allow the user that launches the flow to update and add more.  Let's also add a couple data items for the country and date available.  Once the request is submitted, the shared services team member will complete this form and confirm their skills.  Your first activity is complete!</p> <p>We add another step to get manager approval and setup a simple branch flow.  For the approval branch we add a form to notify of the result.  Then, head back to our approval and add the rejection branch.  So simple!  To complete the workstream, we save and we're ready to go.</p> <p>Test a workstream</p> <p>The new flow starts in trial mode for the author or a testing group to try it out safely before the end users.  Simple to test, we see the launch screen specific to this workstream where the user starting the flow, such as an Accounts Payable clerk, will configure available options including checklist items, and also any activity assignments allowed by the workstream, which we did not activate yet.</p> <p>Once started, we go back to the main Workplace.  Let's take on the role of a shared services team member and see how easy it is to complete the first step. Claim and open the task, check the checklist items, fill in the data, optionally attach a document for review, type a comment and submit.  This is much easier and more organized than email.  Now taking on my team manager role for approval, all looks good and I accept.  Finally, the original requestor receives an approval notification to complete the request. Back at the task list we can switch to workflows and without any customization, the tool provides a complete audit trail of all work by default.</p> <p>Publish a workstream</p> <p>Once ready, I can request a publish to share my workstream with other teams.  The platform has a built-in approval workstream for workstreams!  So we head to the same task list, click to review the workstream definition.  As a publisher, I may send it back with comments or publish directly.  And in just a few minutes, I created, tested and published a new, lightweight workflow available to anyone in my organization: ready to launch!</p> <p>Use Case Summary</p> <p>As you saw, Workstream Services helps you manage remote work more effectively by allowing business users to create lightweight workflows that automate internal processes, no coding required. Get ready to improve your team's productivity by simplifying common tasks and decreasing the complexity and limitations of remote work.</p> <p>Go to top | Go to Getting Started Lab | Go to Introduction</p>"}]}